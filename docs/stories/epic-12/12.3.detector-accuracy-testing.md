# Story 12.3: Detector Accuracy Testing

## Status
Draft

## Story
**As a** validation engineer,
**I want** to test pattern detectors against labeled dataset and measure precision/recall,
**so that** detector quality meets NFR2/NFR3/NFR4 targets.

## Acceptance Criteria
1. Test function: `test_detector_accuracy(detector, labeled_data) -> AccuracyMetrics`
2. Metrics: Precision, Recall, F1-score, Confusion matrix
3. Precision targets (NFRs): Range detection 90%+, Pattern detection 75%+, Phase identification 80%+
4. False positive analysis: identify bars where detector fired but shouldn't
5. False negative analysis: identify labeled patterns that detector missed
6. Threshold tuning: adjust confidence thresholds to optimize F1-score
7. Unit test: run accuracy test as part of CI pipeline
8. Reporting: generate HTML report with charts showing detector performance
9. Regression detection: fail test if accuracy drops >5% from baseline
10. Continuous monitoring: monthly regression test (NFR21)

## Tasks / Subtasks

- [ ] **Task 1: Create Accuracy Metrics Data Model** (AC: 1, 2)
  - [ ] Subtask 1.1: Create Pydantic model `AccuracyMetrics` in `backend/src/models/backtest.py` with fields:
    - `detector_name`: str (name of detector being tested, e.g., "SpringDetector", "RangeDetector")
    - `detector_version`: str (version identifier for tracking changes)
    - `test_timestamp`: datetime (UTC when test was run)
    - `dataset_version`: str (which labeled dataset was used, e.g., "v1")
    - `total_samples`: int (total number of test cases)
    - `true_positives`: int (correctly detected patterns)
    - `false_positives`: int (detected but not in ground truth)
    - `true_negatives`: int (correctly did not detect)
    - `false_negatives`: int (missed patterns that should have been detected)
    - `precision`: Decimal (TP / (TP + FP))
    - `recall`: Decimal (TP / (TP + FN))
    - `f1_score`: Decimal (2 × (precision × recall) / (precision + recall))
    - `confusion_matrix`: Dict[str, int] (full 2x2 matrix)
    - `threshold_used`: Decimal (confidence threshold applied during test)
    - `passes_nfr_target`: bool (whether precision meets NFR requirement)
    - `nfr_target`: Decimal (target precision for this detector type)
    - `metadata`: Dict[str, Any] (additional test details)
  - [ ] Subtask 1.2: Use Decimal type for all metrics [Source: architecture/15-coding-standards.md#Decimal-Precision]
  - [ ] Subtask 1.3: Add UTC timestamp validator matching OHLCVBar pattern [Source: architecture/4-data-models.md#OHLCVBar]
  - [ ] Subtask 1.4: Add computed property methods for precision, recall, F1-score calculations

- [ ] **Task 2: Create Detector Accuracy Test Framework** (AC: 1, 2, 3, 4, 5)
  - [ ] Subtask 2.1: Create `backend/src/backtesting/accuracy_tester.py` with class `DetectorAccuracyTester`:
    - Method `test_detector_accuracy(detector, labeled_data: pd.DataFrame) -> AccuracyMetrics`
    - Load labeled patterns from dataset (using Story 12.2 loader)
    - For each labeled pattern, run detector on corresponding OHLCV data
    - Compare detector output vs ground truth labels
    - Calculate TP, FP, TN, FN counts
    - Compute precision, recall, F1-score
    - Generate confusion matrix
  - [ ] Subtask 2.2: Implement detector adapter interface to standardize detector calls:
    - Abstract base class `PatternDetector` with method `detect(bars: List[OHLCVBar]) -> List[Pattern]`
    - Ensures all detectors (Spring, SOS, UTAD, LPS) have consistent interface
  - [ ] Subtask 2.3: Implement false positive analysis method `analyze_false_positives() -> List[FalsePositiveCase]`:
    - Returns list of cases where detector fired but ground truth = False
    - Include bar details, pattern metadata, why it was incorrect
  - [ ] Subtask 2.4: Implement false negative analysis method `analyze_false_negatives() -> List[FalseNegativeCase]`:
    - Returns list of labeled patterns that detector missed
    - Include bar details, why detector didn't fire, threshold issues
  - [ ] Subtask 2.5: Use type hints and docstrings following coding standards [Source: architecture/15-coding-standards.md]

- [ ] **Task 3: NFR Target Validation** (AC: 3)
  - [ ] Subtask 3.1: Create NFR threshold constants in `backend/src/backtesting/accuracy_tester.py`:
    ```python
    NFR2_RANGE_DETECTION_PRECISION_MIN = Decimal("0.90")  # 90%+
    NFR3_PATTERN_DETECTION_PRECISION_MIN = Decimal("0.75")  # 75%+
    NFR4_PHASE_IDENTIFICATION_ACCURACY_MIN = Decimal("0.80")  # 80%+
    ```
  - [ ] Subtask 3.2: Implement validation method `validate_nfr_compliance(metrics: AccuracyMetrics) -> bool`:
    - Check if metrics meet applicable NFR threshold based on detector_type
    - Return True if compliant, False otherwise
    - Log detailed pass/fail reasons
  - [ ] Subtask 3.3: Add assertion in unit tests to fail if NFR targets not met
  - [ ] Subtask 3.4: Reference NFR requirements in comments [Source: docs/prd/requirements.md#NFR2-NFR4]

- [ ] **Task 4: Threshold Tuning Utility** (AC: 6)
  - [ ] Subtask 4.1: Create method `tune_confidence_threshold(detector, labeled_data, threshold_range: range) -> Dict`:
    - Iterate through confidence thresholds (e.g., 70-95 in steps of 5)
    - Run accuracy test at each threshold
    - Calculate F1-score at each threshold
    - Return dict with threshold -> F1-score mapping
  - [ ] Subtask 4.2: Implement method `find_optimal_threshold(detector, labeled_data) -> int`:
    - Run threshold tuning
    - Return threshold that maximizes F1-score
    - Log recommended threshold and expected F1-score
  - [ ] Subtask 4.3: Generate threshold tuning report showing tradeoff curves (precision vs recall at different thresholds)

- [ ] **Task 5: Baseline Management for Regression Detection** (AC: 9)
  - [ ] Subtask 5.1: Create baseline storage in `backend/tests/datasets/baselines/`:
    - Directory structure: `baselines/{detector_name}_baseline.json`
    - Store baseline AccuracyMetrics as JSON
  - [ ] Subtask 5.2: Implement method `save_baseline(metrics: AccuracyMetrics, detector_name: str)`:
    - Serialize AccuracyMetrics to JSON
    - Write to baseline file
    - Include timestamp and dataset version
  - [ ] Subtask 5.3: Implement method `load_baseline(detector_name: str) -> AccuracyMetrics | None`:
    - Load baseline from file
    - Return None if no baseline exists (first run)
  - [ ] Subtask 5.4: Implement regression detection method `detect_regression(current: AccuracyMetrics, baseline: AccuracyMetrics) -> bool`:
    - Compare current F1-score vs baseline F1-score
    - Return True if current F1 < (baseline F1 - 0.05)  # 5% drop
    - Set `regression_detected` flag in AccuracyMetrics
  - [ ] Subtask 5.5: Add Git tracking for baseline files (not Git LFS, these are small JSON files)

- [ ] **Task 6: Unit Testing** (AC: 7)
  - [ ] Subtask 6.1: Create `backend/tests/unit/test_accuracy_tester.py`:
    - Test `test_detector_accuracy()` with synthetic detector and labeled data
    - Mock detector that returns predictable results (all TP, all FP, etc.)
    - Verify precision/recall/F1 calculations are correct
    - Test edge cases: zero TP, zero FP, perfect detector, useless detector
  - [ ] Subtask 6.2: Test NFR validation logic:
    - Create AccuracyMetrics with precision = 0.91 (above NFR2 threshold)
    - Verify `validate_nfr_compliance()` returns True
    - Create metrics with precision = 0.70 (below NFR3 threshold)
    - Verify validation returns False
  - [ ] Subtask 6.3: Test threshold tuning logic:
    - Mock detector with different behavior at different thresholds
    - Verify `find_optimal_threshold()` returns correct threshold
  - [ ] Subtask 6.4: Test regression detection:
    - Create baseline with F1 = 0.80
    - Test current with F1 = 0.74 (6% drop, should detect regression)
    - Test current with F1 = 0.78 (2% drop, should NOT detect regression)
  - [ ] Subtask 6.5: Follow pytest patterns [Source: architecture/12-testing-strategy.md#Backend-Testing]
  - [ ] Subtask 6.6: Use factory-boy for AccuracyMetrics test fixtures

- [ ] **Task 7: Integration Testing with Real Detectors** (AC: 1, 2, 3, 4, 5)
  - [ ] Subtask 7.1: Create `backend/tests/integration/test_detector_accuracy_integration.py`:
    - Load labeled dataset from Story 12.2 (`labeled_patterns_v1.parquet`)
    - Test SpringDetector accuracy against labeled Spring patterns
    - Test SOSDetector accuracy against labeled SOS patterns
    - Test UTADDetector accuracy against labeled UTAD patterns
    - Test LPSDetector accuracy against labeled LPS patterns
  - [ ] Subtask 7.2: Assert that each detector meets NFR targets (fail test if not)
  - [ ] Subtask 7.3: Generate detailed failure reports if detectors don't meet targets:
    - List false positives with bar details
    - List false negatives with bar details
    - Recommend threshold adjustments
  - [ ] Subtask 7.4: Mark test with `@pytest.mark.slow` since it processes 200+ patterns
  - [ ] Subtask 7.5: Skip test if labeled dataset doesn't exist (Story 12.2 not complete)

- [ ] **Task 8: HTML Report Generation** (AC: 8)
  - [ ] Subtask 8.1: Create `backend/src/backtesting/report_generator.py` with class `AccuracyReportGenerator`:
    - Method `generate_html_report(metrics: AccuracyMetrics, output_path: str)`
    - Use Jinja2 template engine for HTML generation
  - [ ] Subtask 8.2: Create HTML template `backend/src/backtesting/templates/accuracy_report.html`:
    - Display detector name, test date, dataset version
    - Show precision, recall, F1-score as large metrics
    - Render confusion matrix as 2x2 table with color coding
    - Show false positive examples (top 10)
    - Show false negative examples (top 10)
    - Include threshold tuning chart (if available)
    - Add pass/fail badge for NFR compliance
  - [ ] Subtask 8.3: Integrate matplotlib or plotly for charts:
    - Confusion matrix heatmap
    - Precision/Recall curve
    - Threshold tuning curve (F1 vs threshold)
  - [ ] Subtask 8.4: Save report to `backend/tests/reports/accuracy_{detector_name}_{timestamp}.html`
  - [ ] Subtask 8.5: Include CSS styling for professional appearance

- [ ] **Task 9: CI Pipeline Integration** (AC: 7)
  - [ ] Subtask 9.1: Update `.github/workflows/ci.yaml` to run accuracy tests:
    - Add step to run `pytest backend/tests/integration/test_detector_accuracy_integration.py`
    - Run after unit tests pass
    - Fail build if NFR targets not met
  - [ ] Subtask 9.2: Add CI step to upload HTML reports as artifacts:
    - Use `actions/upload-artifact@v3`
    - Upload all generated HTML reports for inspection
  - [ ] Subtask 9.3: Add CI step to check for regressions:
    - Load baselines from `backend/tests/datasets/baselines/`
    - Run regression detection
    - Fail build if regression detected (>5% F1 drop)
    - Post comment on PR with accuracy results summary
  - [ ] Subtask 9.4: Document CI integration in project README

- [ ] **Task 10: Monthly Regression Testing Automation** (AC: 10, NFR21)
  - [ ] Subtask 10.1: Create `.github/workflows/monthly-regression.yaml`:
    - Schedule: `cron: '0 2 1 * *'`  # 1st of month at 2 AM UTC
    - Run full accuracy test suite against all detectors
    - Compare results against baselines
    - Generate comprehensive regression report
  - [ ] Subtask 10.2: Add notification on regression detection:
    - Send email via GitHub Actions (using secrets for SMTP config)
    - Post to Slack if webhook configured
    - Create GitHub Issue automatically with regression details
  - [ ] Subtask 10.3: Implement manual trigger for on-demand regression testing:
    - Add `workflow_dispatch` trigger
    - Allow running from GitHub Actions UI
  - [ ] Subtask 10.4: Update baseline automatically if test passes:
    - If current test passes NFR targets and no regression detected
    - Update baseline files with new metrics
    - Commit updated baselines to Git
  - [ ] Subtask 10.5: Document monthly regression process in project wiki

- [ ] **Task 11: CLI Tool for Local Accuracy Testing** (AC: 1, 2, 8)
  - [ ] Subtask 11.1: Create `backend/scripts/test_detector_accuracy.py` CLI script:
    - Argument: `--detector` (spring, sos, utad, lps, all)
    - Argument: `--dataset-version` (default: v1)
    - Argument: `--threshold` (optional, for testing specific threshold)
    - Argument: `--tune-threshold` (flag to run threshold tuning)
    - Argument: `--save-baseline` (flag to update baseline)
    - Argument: `--report-output` (path for HTML report)
  - [ ] Subtask 11.2: Implement script logic:
    - Load detector classes dynamically
    - Load labeled dataset
    - Run accuracy test
    - Print metrics to console in table format
    - Optionally generate HTML report
    - Optionally save baseline
  - [ ] Subtask 11.3: Add colored console output (green for pass, red for fail)
  - [ ] Subtask 11.4: Document script usage in README

- [ ] **Task 12: Documentation** (AC: all)
  - [ ] Subtask 12.1: Create `backend/docs/accuracy-testing.md` guide:
    - Explain how accuracy testing works
    - Document AccuracyMetrics model
    - Explain NFR targets and validation
    - Provide examples of running tests locally
    - Explain regression detection and baselines
    - Document monthly regression automation
  - [ ] Subtask 12.2: Add docstrings to all classes and methods in accuracy_tester.py
  - [ ] Subtask 12.3: Update main project README with link to accuracy testing docs
  - [ ] Subtask 12.4: Add inline code comments explaining complex logic (precision/recall calculations, regression detection)

## Dev Notes

### Previous Story Context
- **Story 12.2 (Labeled Pattern Dataset Creation)** created the foundation for this story:
  - Parquet dataset: `backend/tests/datasets/labeled_patterns_v1.parquet`
  - Contains 200+ labeled patterns (Spring, SOS, UTAD, LPS, False Spring)
  - Dataset loader: `backend/src/backtesting/dataset_loader.py` with `load_labeled_patterns()`
  - Pydantic model `LabeledPattern` in `backend/src/models/backtest.py`
- **This story depends on Story 12.2 being complete** - the labeled dataset is required for accuracy testing
- Integration tests should gracefully skip if dataset doesn't exist yet

### Epic 12 Context
This story is part of Epic 12: Backtesting & Validation Framework. It delivers the **accuracy measurement system** that validates detector quality meets NFR2/NFR3/NFR4 targets. This is critical for:
- **NFR18**: Objective accuracy measurement using labeled historical dataset
- **NFR21**: Monthly regression testing to detect parameter drift
- **Continuous improvement**: Threshold tuning to optimize detector performance

### NFR Targets and Compliance

**NFR2: Range Detection Precision ≥ 90%** [Source: docs/prd/requirements.md#NFR2]
- Precision = TP / (TP + FP)
- "No false ranges" requirement - very high precision needed
- Applies to: TradingRangeDetector

**NFR3: Pattern Detection Precision ≥ 75%** [Source: docs/prd/requirements.md#NFR3]
- "High-quality signals only" - favors precision over recall
- Applies to: SpringDetector, SOSDetector, UTADDetector, LPSDetector
- Acceptable to miss some patterns (lower recall) if it reduces false positives

**NFR4: Phase Identification Accuracy ≥ 80%** [Source: docs/prd/requirements.md#NFR4]
- Note: Accuracy = (TP + TN) / (TP + TN + FP + FN), different from precision
- Applies to: PhaseIdentifier
- Must correctly classify phases (A, B, C, D, E)

**NFR8: Test Coverage ≥ 90%** [Source: docs/prd/requirements.md#NFR8]
- Comprehensive test coverage for all detection components
- This story adds integration tests for detector accuracy

**NFR18: Labeled Historical Dataset** [Source: docs/prd/requirements.md#NFR18]
- 200+ known patterns across multiple symbols and timeframes
- For validation testing and regression detection
- Created in Story 12.2

**NFR21: Monthly Regression Testing** [Source: docs/prd/requirements.md#NFR21]
- Execute monthly against labeled dataset
- Detect parameter drift, accuracy degradation, edge detection failures
- Automated alerting on >5% performance decline

### Data Models and Schemas

**AccuracyMetrics Model** [Source: architecture/4-data-models.md]:
- Create in `backend/src/models/backtest.py` (same file as LabeledPattern from Story 12.2)
- Use Decimal type for precision, recall, F1-score [Source: architecture/15-coding-standards.md#Decimal-Precision]
- Use UTC timestamps with validator [Source: architecture/4-data-models.md#OHLCVBar]
- Example validator pattern:
```python
@validator('test_date', pre=True)
def ensure_utc(cls, v):
    if v.tzinfo is None:
        return v.replace(tzinfo=timezone.utc)
    return v.astimezone(timezone.utc)
```

**Confusion Matrix Structure**:
- Standard 2x2 matrix for binary classification:
  - True Positive (TP): Detector fired, pattern was valid
  - False Positive (FP): Detector fired, pattern was invalid
  - True Negative (TN): Detector didn't fire, no pattern present
  - False Negative (FN): Detector didn't fire, but pattern was present
- Store as dict: `{"TP": int, "FP": int, "TN": int, "FN": int}`

**Metrics Calculations**:
- Precision = TP / (TP + FP) - "Of all patterns we detected, how many were correct?"
- Recall = TP / (TP + FN) - "Of all valid patterns, how many did we detect?"
- F1-Score = 2 × (Precision × Recall) / (Precision + Recall) - Harmonic mean
- Accuracy = (TP + TN) / (TP + TN + FP + FN) - Overall correctness

### File Locations

**Source Code** [Source: architecture/10-unified-project-structure.md]:
- Accuracy tester: `backend/src/backtesting/accuracy_tester.py`
- Report generator: `backend/src/backtesting/report_generator.py`
- Models: `backend/src/models/backtest.py` (add AccuracyMetrics)
- Templates: `backend/src/backtesting/templates/accuracy_report.html`

**Testing** [Source: architecture/10-unified-project-structure.md]:
- Unit tests: `backend/tests/unit/test_accuracy_tester.py`
- Integration tests: `backend/tests/integration/test_detector_accuracy_integration.py`
- Baseline storage: `backend/tests/datasets/baselines/{detector_name}_baseline.json`
- Report output: `backend/tests/reports/accuracy_{detector_name}_{timestamp}.html`

**Scripts**:
- CLI tool: `backend/scripts/test_detector_accuracy.py`

**CI/CD** [Source: architecture/10-unified-project-structure.md]:
- CI workflow: `.github/workflows/ci.yaml` (update existing)
- Monthly regression: `.github/workflows/monthly-regression.yaml` (new)

**Documentation**:
- Accuracy testing guide: `backend/docs/accuracy-testing.md` (new)

### Tech Stack

**Data Processing** [Source: architecture/3-tech-stack.md]:
- pandas 2.2+ for loading labeled dataset and processing results
- numpy 1.26+ for numerical operations in metrics calculations

**Data Validation** [Source: architecture/3-tech-stack.md]:
- Pydantic 2.5+ for AccuracyMetrics model with validation
- Decimal type for financial precision (NOT float)

**Testing** [Source: architecture/3-tech-stack.md]:
- pytest 8.0+ for unit and integration tests
- pytest-mock for mocking detectors
- factory-boy for test data generation (AccuracyMetrics fixtures)

**Reporting**:
- Jinja2 for HTML template rendering (already available in FastAPI)
- matplotlib or plotly for charts (need to add to dependencies)
- Consider using plotly for interactive HTML charts

**Logging** [Source: architecture/3-tech-stack.md]:
- structlog 24.1+ for structured logging of test results

### Coding Standards

**Naming Conventions** [Source: architecture/15-coding-standards.md#Naming-Conventions]:
- Classes: PascalCase (e.g., `DetectorAccuracyTester`, `AccuracyMetrics`)
- Functions: snake_case (e.g., `test_detector_accuracy`, `validate_nfr_compliance`)
- Files: snake_case (e.g., `accuracy_tester.py`, `report_generator.py`)

**Type Safety** [Source: architecture/15-coding-standards.md]:
- Use strict type hints on all functions
- Use Decimal (NOT float) for precision, recall, F1-score
- Validate all data against Pydantic models

**Decimal Precision** [Source: architecture/15-coding-standards.md#Decimal-Precision]:
- Use `Decimal` type for all metrics (precision, recall, F1-score)
- Never use `float` for these calculations
- Example: `precision: Decimal = Field(..., decimal_places=4, max_digits=6)`

### Detector Interface Abstraction

The accuracy tester needs a **consistent interface** to test different detectors (Spring, SOS, UTAD, LPS). Current detector implementations may have different method signatures.

**Recommendation**: Create an abstract base class `PatternDetector`:
```python
from abc import ABC, abstractmethod
from typing import List
from backend.src.models.ohlcv import OHLCVBar
from backend.src.models.pattern import Pattern

class PatternDetector(ABC):
    @abstractmethod
    def detect(self, bars: List[OHLCVBar]) -> List[Pattern]:
        """Detect patterns in OHLCV bars."""
        pass
```

All detector classes (SpringDetector, SOSDetector, etc.) should implement this interface. The accuracy tester can then test any detector uniformly:
```python
def test_detector_accuracy(detector: PatternDetector, labeled_data: pd.DataFrame) -> AccuracyMetrics:
    # Works with any detector that implements PatternDetector interface
    ...
```

This follows the **dependency inversion principle** and makes the accuracy tester extensible to new detector types.

### False Positive/Negative Analysis

**False Positive Analysis** (AC 4):
- Detector fired (detected pattern) but ground truth label says it shouldn't have
- Causes: threshold too low, validation too lenient, edge cases not handled
- Output: List of FP cases with bar details, pattern metadata, why it was wrong
- Action: Review FP cases to improve detector validation logic

**False Negative Analysis** (AC 5):
- Detector didn't fire but ground truth label says pattern was present
- Causes: threshold too high, validation too strict, pattern not recognized
- Output: List of FN cases with bar details, why detector missed it
- Action: Review FN cases to adjust thresholds or improve detection logic

**Use Cases**:
- Threshold tuning: Adjust confidence thresholds to balance FP vs FN
- Detector improvement: Identify systematic failures to fix in detector code
- Data quality: Identify mislabeled patterns in dataset (FP/FN might be labeling errors)

### Threshold Tuning Strategy

**Goal**: Find optimal confidence threshold that maximizes F1-score while meeting NFR targets.

**Approach**:
1. Run detector at multiple thresholds (70, 75, 80, 85, 90, 95)
2. Calculate F1-score at each threshold
3. Plot Precision-Recall curve
4. Select threshold that:
   - Maximizes F1-score
   - Meets NFR precision target (90% for ranges, 75% for patterns, 80% for phases)
5. Recommend threshold in report

**Trade-offs**:
- Higher threshold → Higher precision, lower recall (fewer but better signals)
- Lower threshold → Lower precision, higher recall (more signals but more FP)
- NFR3 prioritizes precision for patterns ("high-quality signals only")

### Baseline Management and Regression Detection

**Baseline Storage**:
- Store baseline AccuracyMetrics as JSON in `backend/tests/datasets/baselines/`
- One baseline file per detector: `spring_detector_baseline.json`
- Include: F1-score, precision, recall, test date, dataset version

**Regression Detection Logic** (AC 9, NFR21):
- Compare current F1-score vs baseline F1-score
- Regression = current F1 < (baseline F1 - 0.05)  # 5% drop
- Example: Baseline F1 = 0.80, current F1 = 0.74 → REGRESSION (6% drop)
- Example: Baseline F1 = 0.80, current F1 = 0.78 → OK (2.5% drop, within tolerance)

**Baseline Update Strategy**:
- Update baseline if:
  1. Current test passes NFR targets
  2. No regression detected (or intentional improvement)
  3. Manual approval (via `--save-baseline` flag or monthly automation)
- Don't auto-update baseline if regression detected (prevents hiding degradation)

**Monthly Regression Testing** (AC 10, NFR21):
- Automated GitHub Actions workflow runs 1st of each month
- Tests all detectors against current labeled dataset
- Compares against baselines
- Alerts on >5% performance decline
- Updates baselines if tests pass and improve

### CI/CD Integration

**CI Pipeline Integration** (AC 7):
- Run accuracy tests in GitHub Actions on every PR
- Fail build if NFR targets not met (prevents merging bad code)
- Upload HTML reports as artifacts for review
- Post accuracy summary as PR comment

**Monthly Regression Workflow** (AC 10):
- Scheduled GitHub Action: `cron: '0 2 1 * *'`  # 1st of month, 2 AM UTC
- Run full accuracy test suite
- Generate comprehensive report
- Alert on regressions (email, Slack, GitHub Issue)
- Auto-update baselines if tests pass

**Manual Triggers**:
- Allow manual execution via `workflow_dispatch` for on-demand testing
- Useful for testing before releases or after detector changes

### Testing Strategy

#### Unit Tests [Source: architecture/12-testing-strategy.md]
- Test metrics calculations with known inputs
- Mock detectors with predictable outputs (all TP, all FP, etc.)
- Test edge cases: zero TP, zero FP, perfect detector, useless detector
- Test NFR validation logic
- Test threshold tuning logic
- Test regression detection logic
- Use factory-boy for AccuracyMetrics fixtures
- Location: `backend/tests/unit/test_accuracy_tester.py`

#### Integration Tests
- Test real detectors (SpringDetector, SOSDetector, etc.) against labeled dataset
- Load labeled_patterns_v1.parquet from Story 12.2
- Assert NFR targets are met (fail test if not)
- Generate detailed failure reports
- Mark as `@pytest.mark.slow` (processes 200+ patterns)
- Skip if labeled dataset doesn't exist yet
- Location: `backend/tests/integration/test_detector_accuracy_integration.py`

#### Test Coverage
- Aim for 90%+ coverage (NFR8)
- Cover all branches in accuracy calculation logic
- Cover all NFR validation paths
- Cover regression detection logic

### Project Structure Notes

The accuracy testing infrastructure fits naturally into the existing project structure:
- `backend/src/backtesting/` directory already defined for backtesting components [Source: architecture/10-unified-project-structure.md#backtesting]
- `backend/tests/datasets/` already used for labeled dataset from Story 12.2
- `backend/tests/unit/` and `backend/tests/integration/` follow existing structure
- `.github/workflows/` already has ci.yaml for CI integration

### Performance Considerations

**Accuracy Testing Performance**:
- Testing 200+ labeled patterns against multiple detectors can be slow
- Each pattern requires loading OHLCV context (multiple bars)
- Mark integration tests as `@pytest.mark.slow`
- Consider parallel execution for multiple detectors (future optimization)

**CI Performance**:
- Full accuracy test suite should complete within CI timeout (10 minutes target)
- Cache labeled dataset to avoid re-downloading
- Use pytest-xdist for parallel test execution if needed

### Error Handling

**Graceful Degradation**:
- Skip integration tests if labeled dataset doesn't exist (Story 12.2 incomplete)
- Handle missing baseline gracefully (first run, no regression check)
- Handle detector failures during testing (log and continue)

**Error Logging**:
- Use structlog for structured logging [Source: architecture/3-tech-stack.md]
- Log detector name, pattern ID, bar timestamp on failures
- Include contextual information for debugging

### Monitoring and Observability

**Metrics to Track**:
- Detector accuracy over time (trend analysis)
- Regression detection frequency
- Threshold changes over time
- Dataset version used for testing

**Alerts**:
- Email/Slack on regression detection
- GitHub Issue creation for regressions
- Notification on monthly regression test completion

### Dependencies and Risks

**Dependencies**:
- Story 12.2 (Labeled Pattern Dataset) must be complete
- Detector implementations (Spring, SOS, UTAD, LPS) must exist
- pandas, numpy, pytest (already in tech stack)
- Jinja2 (already available via FastAPI)
- matplotlib or plotly (need to add for charts)

**Risks**:
1. **Detector interface inconsistency**: Different detectors may have different APIs
   - Mitigation: Create abstract PatternDetector interface (Task 2.2)
2. **NFR targets not met initially**: Detectors may not meet 75%/80%/90% targets
   - Mitigation: Use threshold tuning to optimize, iterate on detector logic
3. **Flaky tests**: Accuracy tests may be non-deterministic if detectors have randomness
   - Mitigation: Ensure detectors are deterministic, use fixed random seeds
4. **CI timeout**: Accuracy tests may take too long in CI
   - Mitigation: Optimize test execution, use pytest-xdist for parallelization

### Notes for Developer

1. **Start with AccuracyMetrics model** - defines the data structure for everything else
2. **Create abstract PatternDetector interface** - ensures consistent testing across detectors
3. **Implement core accuracy testing logic first** - TP/FP/TN/FN counting, metrics calculation
4. **Add unit tests as you go** - verify metrics calculations are correct
5. **Implement threshold tuning** - helps find optimal thresholds for each detector
6. **Build HTML report generator** - makes results human-readable and actionable
7. **Integrate with CI** - ensures tests run automatically on PRs
8. **Set up monthly regression** - implements NFR21 continuous monitoring
9. **Document everything** - accuracy testing guide, CLI tool usage, NFR targets

**Key Implementation Order**:
1. AccuracyMetrics model (foundation)
2. Core accuracy testing logic (DetectorAccuracyTester class)
3. NFR validation (ensure targets are checked)
4. Unit tests (verify calculations are correct)
5. Integration tests (test real detectors)
6. Threshold tuning (optimize detector performance)
7. Baseline management (regression detection)
8. HTML report generation (human-readable output)
9. CI integration (automate testing)
10. Monthly regression automation (NFR21)
11. CLI tool (local testing convenience)
12. Documentation (explain everything)

**Testing as You Go**:
- Write unit tests alongside implementation (TDD approach)
- Test metrics calculations with known inputs before testing real detectors
- Use mock detectors for unit tests (predictable outputs)
- Use real detectors for integration tests (actual accuracy measurement)

**Threshold Tuning Strategy**:
- Run threshold tuning on each detector
- Document recommended thresholds in test results
- Update detector confidence thresholds based on tuning results
- Re-run accuracy tests to verify improvement

**Regression Detection Strategy**:
- Save baseline after initial accuracy testing
- Compare all future tests against baseline
- Alert on >5% F1-score drop (NFR21)
- Investigate regressions: code changes? data changes? parameter drift?
- Update baseline intentionally after detector improvements

## Testing

### Testing Framework [Source: architecture/12-testing-strategy.md]
- Use pytest 8.0+ for all backend unit and integration tests
- Test files mirror source structure
- Unit tests: `backend/tests/unit/test_accuracy_tester.py`
- Integration tests: `backend/tests/integration/test_detector_accuracy_integration.py`

### Test Coverage Requirements [Source: architecture/12-testing-strategy.md, NFR8]
- Target 90%+ test coverage (NFR8)
- Cover all branches in accuracy calculation logic:
  - Precision calculation (TP = 0 edge case, FP = 0 edge case)
  - Recall calculation (TP = 0 edge case, FN = 0 edge case)
  - F1-score calculation (precision = 0 or recall = 0 edge case)
  - Confusion matrix generation
- Cover NFR validation logic (pass/fail for each NFR type)
- Cover regression detection logic (5% threshold boundary conditions)
- Cover threshold tuning logic (optimal threshold selection)

### Unit Test Requirements
**Test AccuracyMetrics Model**:
- Validate Decimal precision for metrics
- Validate UTC timestamps
- Test computed properties (accuracy, specificity, NPV)
- Test JSON serialization/deserialization

**Test Accuracy Calculations**:
- Create mock detector with known outputs (all TP, all FP, all TN, all FN)
- Verify precision = TP / (TP + FP) calculated correctly
- Verify recall = TP / (TP + FN) calculated correctly
- Verify F1-score = 2 * (P * R) / (P + R) calculated correctly
- Test edge cases:
  - Zero TP: precision = 0, recall = 0, F1 = 0 (or undefined)
  - Zero FP: precision = 1.0 (perfect)
  - Zero FN: recall = 1.0 (perfect)
  - Perfect detector: precision = recall = F1 = 1.0
  - Useless detector: precision = recall = F1 = 0.0

**Test NFR Validation**:
- Create AccuracyMetrics with precision = 0.91, detector_type = "RANGE"
  - Assert `validate_nfr_compliance()` returns True (meets NFR2)
- Create AccuracyMetrics with precision = 0.89, detector_type = "RANGE"
  - Assert validation returns False (fails NFR2)
- Create AccuracyMetrics with precision = 0.76, detector_type = "PATTERN"
  - Assert validation returns True (meets NFR3)
- Create AccuracyMetrics with precision = 0.74, detector_type = "PATTERN"
  - Assert validation returns False (fails NFR3)

**Test Threshold Tuning**:
- Mock detector with different precision/recall at different thresholds
- Verify `tune_confidence_threshold()` returns dict with correct F1 scores
- Verify `find_optimal_threshold()` selects threshold with max F1

**Test Regression Detection**:
- Baseline F1 = 0.80, current F1 = 0.76 (5% drop exactly)
  - Should detect regression (edge case)
- Baseline F1 = 0.80, current F1 = 0.74 (6% drop)
  - Should detect regression
- Baseline F1 = 0.80, current F1 = 0.78 (2.5% drop)
  - Should NOT detect regression (within tolerance)
- No baseline exists (first run)
  - Should return False (no regression, no baseline to compare)

**Test Baseline Management**:
- Test `save_baseline()` creates JSON file correctly
- Test `load_baseline()` reads JSON file correctly
- Test `load_baseline()` returns None if file doesn't exist

### Integration Test Requirements
**Test Real Detectors Against Labeled Dataset**:
- Load `labeled_patterns_v1.parquet` from Story 12.2
- Filter labeled patterns by type (Spring, SOS, UTAD, LPS)
- Test SpringDetector:
  - Run detector on Spring labeled patterns
  - Calculate accuracy metrics
  - Assert precision ≥ 75% (NFR3)
  - Generate detailed report if fails
- Test SOSDetector (same approach)
- Test UTADDetector (same approach)
- Test LPSDetector (same approach)

**Graceful Degradation**:
- Use pytest skip decorator if dataset doesn't exist:
```python
@pytest.mark.skipif(not os.path.exists('backend/tests/datasets/labeled_patterns_v1.parquet'),
                    reason="Labeled dataset not available (Story 12.2 incomplete)")
def test_spring_detector_accuracy():
    ...
```

**Performance Marking**:
- Mark integration tests as slow:
```python
@pytest.mark.slow
def test_spring_detector_accuracy():
    ...
```
- Run with `pytest -m "not slow"` for fast tests only
- Run with `pytest -m slow` for slow tests only

### Test Data and Fixtures [Source: architecture/3-tech-stack.md]
- Use factory-boy for AccuracyMetrics test fixtures
- Example factory:
```python
import factory
from backend.src.models.backtest import AccuracyMetrics
from decimal import Decimal

class AccuracyMetricsFactory(factory.Factory):
    class Meta:
        model = AccuracyMetrics

    detector_name = "MockDetector"
    detector_type = "PATTERN"
    test_date = factory.Faker('date_time', tzinfo=timezone.utc)
    dataset_version = "v1"
    total_samples = 100
    true_positives = 75
    false_positives = 10
    true_negatives = 5
    false_negatives = 10
    precision = Decimal("0.8824")  # 75 / (75 + 10)
    recall = Decimal("0.8824")  # 75 / (75 + 10)
    f1_score = Decimal("0.8824")
    # ... etc
```

### CI Integration Testing
- Run accuracy tests in GitHub Actions on every PR
- Use GitHub Actions matrix strategy to test multiple detectors in parallel:
```yaml
strategy:
  matrix:
    detector: [spring, sos, utad, lps]
steps:
  - name: Test ${{ matrix.detector }} accuracy
    run: pytest backend/tests/integration/test_detector_accuracy_integration.py -k ${{ matrix.detector }}
```

### Test Reporting
- Generate HTML reports for all accuracy tests
- Upload as GitHub Actions artifacts
- Post summary comment on PR with accuracy results

### Mocking Strategy [Source: architecture/3-tech-stack.md]
- Use pytest-mock for mocking in unit tests
- Mock PatternDetector interface for predictable test outputs
- Example mock detector:
```python
class MockDetector(PatternDetector):
    def __init__(self, outputs: List[Pattern]):
        self.outputs = outputs

    def detect(self, bars: List[OHLCVBar]) -> List[Pattern]:
        return self.outputs
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-20 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

_This section will be populated by the development agent during implementation._

### Agent Model Used

_To be filled by dev agent_

### Debug Log References

_To be filled by dev agent_

### Completion Notes

_To be filled by dev agent_

### File List

_To be filled by dev agent_

## QA Results

_This section will be populated by the QA agent after story completion._
