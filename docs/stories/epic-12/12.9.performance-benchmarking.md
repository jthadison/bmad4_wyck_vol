# Story 12.9: Performance Benchmarking

## Status
Ready for Development

## Sprint Assignment

**Sprint 3** - Production Readiness

- Dependencies: Story 12.1 (Backtesting Engine)
- Parallel with: Stories 12.6, 12.7, 12.8
- Required by: Story 12.10 (Final Integration)

## Story
**As a** developer,
**I want** performance benchmarks to ensure NFR1/NFR7 latency targets are met,
**so that** the system runs efficiently in production.

## Acceptance Criteria
1. Benchmarks: signal generation latency, backtest speed, database query time
2. NFR1 target: <1 second per symbol per bar analyzed
3. NFR7 target: 100+ bars/second for backtesting
4. Profiling: identify bottlenecks in hot paths (volume analysis, pattern detection)
5. Optimization: vectorize NumPy operations, cache intermediate results
6. CI integration: run benchmarks on every PR, fail if regression >10%
7. Reporting: flame graphs showing function call times
8. Load testing: simulate 100 concurrent symbols analyzed
9. Database optimization: index tuning for audit log queries
10. Monitoring: Prometheus metrics track production latency

## Tasks / Subtasks

- [ ] **Task 1: Create Performance Benchmark Framework** (AC: 1, 2, 3)
  - [ ] Subtask 1.1: Create `backend/benchmarks/` directory for benchmark suite
  - [ ] Subtask 1.2: Install pytest-benchmark plugin: add `pytest-benchmark = "^4.0.0"` to pyproject.toml
  - [ ] Subtask 1.3: Create `backend/benchmarks/benchmark_config.py` configuration:
    - Warmup rounds: 3 (discard first 3 runs to account for caching)
    - Benchmark rounds: 10 (run each benchmark 10 times for statistical significance)
    - Timeout: 60 seconds per benchmark
    - Target thresholds: NFR1 <1.0 seconds, NFR7 >100 bars/second
  - [ ] Subtask 1.4: Create `backend/benchmarks/conftest.py` with shared fixtures:
    - Fixture: `sample_ohlcv_bars` - Generate 1000 synthetic OHLCV bars for consistent benchmarking
    - Fixture: `backtest_engine` - Pre-configured BacktestEngine instance
    - Fixture: `pattern_detectors` - All pattern detector instances (SpringDetector, SOSDetector, UTADDetector)
    - Fixture: `test_symbol_data` - Historical data for AAPL 2020-2024 (for realistic benchmarks)
  - [ ] Subtask 1.5: Configure pytest.ini for benchmark section:
    - `benchmark_warmup = true`
    - `benchmark_autosave = true`
    - `benchmark_save_data = true`
    - Save results to `backend/benchmarks/.benchmarks/` for historical comparison

- [ ] **Task 2: Signal Generation Latency Benchmarks** (AC: 2)
  - [ ] Subtask 2.1: Create `backend/benchmarks/test_signal_generation_latency.py`
  - [ ] Subtask 2.2: Benchmark: `test_volume_analysis_latency(benchmark, sample_ohlcv_bars)`:
    - Measure time to calculate volume_ratio and spread_ratio for 100 bars
    - Target: <50ms per bar (part of NFR1 <1 second total budget)
    - Use pytest-benchmark: `result = benchmark(volume_analyzer.analyze, bars)`
  - [ ] Subtask 2.3: Benchmark: `test_spring_detection_latency(benchmark, sample_ohlcv_bars)`:
    - Measure time to run SpringDetector on 100 bars
    - Target: <200ms per bar (pattern detection is most expensive operation)
    - Include setup time: trading range identification, volume validation
  - [ ] Subtask 2.4: Benchmark: `test_sos_detection_latency(benchmark, sample_ohlcv_bars)`:
    - Measure time to run SOSDetector on 100 bars
    - Target: <150ms per bar
  - [ ] Subtask 2.5: Benchmark: `test_utad_detection_latency(benchmark, sample_ohlcv_bars)`:
    - Measure time to run UTADDetector on 100 bars
    - Target: <150ms per bar
  - [ ] Subtask 2.6: Benchmark: `test_full_pipeline_latency(benchmark, sample_ohlcv_bars)`:
    - Measure end-to-end time: OHLCV ingestion → volume analysis → pattern detection → signal generation
    - Target: NFR1 <1 second per symbol per bar analyzed
    - Verify all detectors run in series without parallelization overhead
  - [ ] Subtask 2.7: Benchmark: `test_pattern_engine_coordinator(benchmark, sample_ohlcv_bars)`:
    - Measure PatternEngine.detect_patterns() with all detectors enabled
    - Simulate realistic workload: 100 bars, 3 pattern types
  - [ ] Subtask 2.8: Assert all benchmarks meet NFR1 target: <1 second

- [ ] **Task 3: Backtest Speed Benchmarks** (AC: 3)
  - [ ] Subtask 3.1: Create `backend/benchmarks/test_backtest_speed.py`
  - [ ] Subtask 3.2: Benchmark: `test_backtest_engine_speed(benchmark, test_symbol_data)`:
    - Run BacktestEngine.run_backtest() on 1000 bars (AAPL 2020-2024)
    - Measure bars processed per second
    - Target: NFR7 >100 bars/second
    - Calculate: total_bars / execution_time_seconds
  - [ ] Subtask 3.3: Benchmark: `test_order_simulation_speed(benchmark)`:
    - Measure time to simulate 100 market orders (fill price calculation, slippage, commission)
    - Target: <10ms per order (should not bottleneck backtest speed)
  - [ ] Subtask 3.4: Benchmark: `test_position_tracking_speed(benchmark)`:
    - Measure time to update position state for 100 bars with 10 concurrent positions
    - Target: <5ms per bar
  - [ ] Subtask 3.5: Benchmark: `test_metrics_calculation_speed(benchmark)`:
    - Measure time to calculate BacktestMetrics (win rate, avg R, Sharpe, max drawdown) from 100 trades
    - Target: <50ms total (happens once at end of backtest)
  - [ ] Subtask 3.6: Benchmark: `test_walk_forward_speed(benchmark, test_symbol_data)`:
    - Measure time to run walk-forward backtest (6-month train, 3-month validate, 4 windows)
    - Target: Complete in <30 seconds for single symbol
  - [ ] Subtask 3.7: Assert all benchmarks meet NFR7 target: >100 bars/second

- [ ] **Task 4: Database Query Benchmarks** (AC: 1, 9)
  - [ ] Subtask 4.1: Create `backend/benchmarks/test_database_queries.py`
  - [ ] Subtask 4.2: Setup: Populate test database with realistic data volume:
    - 100,000 OHLCV bars (10 symbols × 10,000 bars each)
    - 1,000 patterns detected
    - 500 signals generated
    - 200 audit log entries
  - [ ] Subtask 4.3: Benchmark: `test_ohlcv_range_query(benchmark)`:
    - Query: Fetch 100 bars for symbol="AAPL", timeframe="1d", date range 2020-2024
    - Target: <50ms
    - Use SQLAlchemy with indexes on (symbol, timeframe, timestamp)
  - [ ] Subtask 4.4: Benchmark: `test_pattern_lookup_query(benchmark)`:
    - Query: Fetch all patterns for symbol="AAPL", pattern_type="SPRING", last 30 days
    - Target: <30ms
    - Use indexes on (symbol, pattern_type, detection_time)
  - [ ] Subtask 4.5: Benchmark: `test_signal_history_query(benchmark)`:
    - Query: Fetch all signals for symbol="AAPL", status="ACTIVE", ordered by created_at DESC
    - Target: <40ms
    - Use indexes on (symbol, status, created_at)
  - [ ] Subtask 4.6: Benchmark: `test_audit_log_query(benchmark)`:
    - Query: Fetch audit logs for user_id, action_type, date range (last 7 days)
    - Target: <100ms (audit logs can grow large)
    - Recommended indexes: (user_id, action_type, timestamp)
  - [ ] Subtask 4.7: Benchmark: `test_aggregate_metrics_query(benchmark)`:
    - Query: Calculate portfolio metrics (total P&L, win rate, active positions) via SQL aggregation
    - Target: <150ms
    - Use materialized views (Phase 2 optimization)
  - [ ] Subtask 4.8: Identify slow queries (>100ms) using EXPLAIN ANALYZE
  - [ ] Subtask 4.9: Document recommended indexes in migration file

- [ ] **Task 5: Profiling and Bottleneck Identification** (AC: 4)
  - [ ] Subtask 5.1: Install profiling tools: `py-spy = "^0.3.14"`, `memory-profiler = "^0.61.0"`
  - [ ] Subtask 5.2: Create `backend/benchmarks/profile_hot_paths.py` script:
    - Profile signal generation pipeline with py-spy
    - Profile backtest execution with py-spy
    - Generate flame graphs showing function call times
  - [ ] Subtask 5.3: Run py-spy profiler on signal generation:
    - Command: `py-spy record -o signal_generation.svg --native -- python -m backend.src.signal_generator.generator`
    - Analyze flame graph for bottlenecks (>10% of total time)
  - [ ] Subtask 5.4: Run py-spy profiler on backtest engine:
    - Command: `py-spy record -o backtest_engine.svg --native -- python -m backend.src.backtesting.engine`
    - Identify slow functions in hot path
  - [ ] Subtask 5.5: Profile memory usage with memory-profiler:
    - Decorator: `@profile` on BacktestEngine.run_backtest()
    - Identify memory allocations in loops (OHLCV bar processing)
  - [ ] Subtask 5.6: Analyze profiling results and document bottlenecks:
    - List functions consuming >5% of execution time
    - Flag candidates for vectorization (NumPy operations)
    - Flag candidates for caching (repeated calculations)
  - [ ] Subtask 5.7: Save flame graphs to `backend/benchmarks/profiles/` for review

- [ ] **Task 6: Optimization Implementation** (AC: 5)
  - [ ] Subtask 6.1: Vectorize volume analysis calculations:
    - Use NumPy array operations instead of Python loops
    - Calculate volume_ratio for 100 bars in single operation: `bars['volume'] / bars['volume'].rolling(20).mean()`
    - Calculate spread_ratio similarly
    - Benchmark improvement: expect 10x speedup
  - [ ] Subtask 6.2: Cache intermediate results for pattern detection:
    - Cache trading range identification (ranges don't change per bar)
    - Cache 20-bar rolling averages (volume, spread)
    - Use LRU cache with ttl (time-to-live) of 60 seconds
  - [ ] Subtask 6.3: Optimize pattern detector loops:
    - Replace Python for-loops with pandas vectorized operations
    - Use .loc[] indexing instead of iterrows() (10x faster)
  - [ ] Subtask 6.4: Optimize backtest engine:
    - Pre-allocate arrays for trade list (avoid list.append() in loops)
    - Use NumPy structured arrays for OHLCV data (faster than pandas DataFrame for hot loops)
  - [ ] Subtask 6.5: Database query optimization:
    - Add composite indexes identified in Task 4
    - Use SQLAlchemy query.options(joinedload()) to avoid N+1 queries
    - Enable connection pooling: pool_size=10, max_overflow=20
  - [ ] Subtask 6.6: Re-run benchmarks after optimizations
  - [ ] Subtask 6.7: Document performance improvements in commit message

- [ ] **Task 7: CI Integration for Benchmark Regression Detection** (AC: 6)
  - [ ] Subtask 7.1: Create `.github/workflows/benchmarks.yaml` workflow
  - [ ] Subtask 7.2: Configure triggers:
    - On pull_request: Run benchmarks on every PR
    - On push to main: Run benchmarks and save baseline
    - On workflow_dispatch: Manual trigger for on-demand benchmarking
  - [ ] Subtask 7.3: Workflow steps:
    - Checkout code
    - Setup Python 3.11+
    - Install Poetry dependencies
    - Setup PostgreSQL service (GitHub Actions service container)
    - Run Alembic migrations
    - Populate test database with benchmark data
    - Run pytest benchmarks: `poetry run pytest backend/benchmarks/ --benchmark-only --benchmark-json=output.json`
    - Compare results to baseline (from main branch)
  - [ ] Subtask 7.4: Implement regression detection:
    - Fetch baseline benchmark results from main branch: `git show main:backend/benchmarks/.benchmarks/latest.json`
    - Compare current results to baseline
    - Fail workflow if any benchmark regresses >10%
    - Formula: `((current_time - baseline_time) / baseline_time) * 100 > 10`
  - [ ] Subtask 7.5: Add workflow comment to PR with benchmark results:
    - Table showing: Benchmark name, Baseline time, Current time, Change %
    - Highlight regressions in red
    - Use GitHub Actions `actions/github-script` to post comment
  - [ ] Subtask 7.6: Save benchmark results as workflow artifact:
    - Upload `output.json` for historical analysis
    - Upload flame graphs if generated
  - [ ] Subtask 7.7: Set workflow permissions: contents: read, pull-requests: write (for posting comments)

- [ ] **Task 8: Benchmark Reporting and Visualization** (AC: 7)
  - [ ] Subtask 8.1: Install pytest-benchmark plugin with storage:
    - Configure: `--benchmark-autosave` to save results automatically
    - Configure: `--benchmark-compare` to compare against previous runs
  - [ ] Subtask 8.2: Generate HTML benchmark report:
    - Command: `pytest --benchmark-only --benchmark-histogram=histogram.html`
    - Report shows distribution of execution times across runs
    - Save to `backend/benchmarks/reports/histogram.html`
  - [ ] Subtask 8.3: Generate flame graphs with py-spy:
    - Signal generation flame graph: `backend/benchmarks/profiles/signal_generation_flame.svg`
    - Backtest flame graph: `backend/benchmarks/profiles/backtest_flame.svg`
    - View with: `open signal_generation_flame.svg` (interactive in browser)
  - [ ] Subtask 8.4: Create `backend/benchmarks/compare_benchmarks.py` script:
    - Load current and baseline JSON results
    - Generate markdown table comparing benchmarks
    - Highlight regressions (>10% slower)
    - Highlight improvements (>10% faster)
  - [ ] Subtask 8.5: Add npm script to package.json:
    - `"benchmark": "cd backend && poetry run pytest benchmarks/ --benchmark-only"`
    - `"benchmark:compare": "cd backend && poetry run python benchmarks/compare_benchmarks.py"`
    - `"benchmark:profile": "cd backend && py-spy record -o benchmarks/profiles/profile.svg --native -- python -m pytest benchmarks/"`
  - [ ] Subtask 8.6: Document benchmark reports in README

- [ ] **Task 9: Load Testing for Concurrent Symbol Analysis** (AC: 8)
  - [ ] Subtask 9.1: Create `backend/benchmarks/test_load.py` for load testing
  - [ ] Subtask 9.2: Install locust for load testing: `locust = "^2.20.0"`
  - [ ] Subtask 9.3: Create Locust task for pattern detection:
    - Simulate 100 concurrent users
    - Each user: POST /api/v1/patterns/detect with symbol and OHLCV data
    - Measure response time at load
  - [ ] Subtask 9.4: Load test scenario: 100 concurrent symbols analyzed:
    - Spawn 100 Locust users (1 per symbol)
    - Each user requests pattern detection every 5 seconds (simulating live bar updates)
    - Run for 5 minutes
    - Measure: Average response time, p95 response time, error rate
  - [ ] Subtask 9.5: Benchmark: `test_concurrent_signal_generation(benchmark)`:
    - Use asyncio to simulate 100 concurrent signal generation requests
    - Measure total time and individual request times
    - Target: <2 seconds for 100 concurrent requests (parallelization benefit)
  - [ ] Subtask 9.6: Analyze load test results:
    - Identify resource bottlenecks (CPU, memory, database connections)
    - Check for race conditions or deadlocks
    - Verify thread-safe data structures (NFR16)
  - [ ] Subtask 9.7: Create Locust configuration file: `backend/benchmarks/locustfile.py`
  - [ ] Subtask 9.8: Add npm script: `"load-test": "cd backend && locust -f benchmarks/locustfile.py --headless -u 100 -r 10 --run-time 5m"`

- [ ] **Task 10: Database Index Optimization** (AC: 9)
  - [ ] Subtask 10.1: Create Alembic migration: `{timestamp}_optimize_indexes_for_performance.py`
  - [ ] Subtask 10.2: Add composite index on ohlcv_bars table:
    - Index: `idx_ohlcv_symbol_timeframe_timestamp` on (symbol, timeframe, timestamp)
    - Supports queries: "Fetch bars for symbol X, timeframe Y, date range Z"
  - [ ] Subtask 10.3: Add composite index on patterns table:
    - Index: `idx_patterns_symbol_type_time` on (symbol, pattern_type, detection_time DESC)
    - Supports queries: "Fetch patterns for symbol X, pattern type Y, recent first"
  - [ ] Subtask 10.4: Add composite index on signals table:
    - Index: `idx_signals_symbol_status_created` on (symbol, status, created_at DESC)
    - Supports queries: "Fetch active signals for symbol X, recent first"
  - [ ] Subtask 10.5: Add composite index on audit_logs table:
    - Index: `idx_audit_user_action_time` on (user_id, action_type, timestamp DESC)
    - Supports queries: "Fetch audit logs for user X, action type Y, date range Z"
  - [ ] Subtask 10.6: Add partial index on backtest_results table:
    - Index: `idx_backtest_status_pending` on (status) WHERE status = 'RUNNING'
    - Optimizes queries for active backtests only
  - [ ] Subtask 10.7: Run EXPLAIN ANALYZE on slow queries before and after indexes:
    - Document query plan improvements (e.g., Index Scan vs Seq Scan)
    - Verify index usage in query planner
  - [ ] Subtask 10.8: Run migration: `alembic upgrade head`
  - [ ] Subtask 10.9: Re-run database benchmarks (Task 4) to verify improvement

- [ ] **Task 11: Prometheus Metrics for Production Monitoring** (AC: 10)
  - [ ] Subtask 11.1: Install Prometheus client library: `prometheus-client = "^0.19.0"`
  - [ ] Subtask 11.2: Create `backend/src/observability/metrics.py` for metric definitions
  - [ ] Subtask 11.3: Define Prometheus metrics:
    - Counter: `signal_generation_requests_total` (labels: symbol, pattern_type)
    - Histogram: `signal_generation_latency_seconds` (buckets: 0.1, 0.5, 1.0, 2.0, 5.0)
    - Counter: `backtest_executions_total` (labels: symbol, status)
    - Histogram: `backtest_duration_seconds` (buckets: 1, 5, 10, 30, 60, 120)
    - Gauge: `active_signals_count` (current number of active signals)
    - Histogram: `database_query_duration_seconds` (labels: query_type)
    - Counter: `pattern_detections_total` (labels: pattern_type, symbol)
  - [ ] Subtask 11.4: Instrument signal generation with metrics:
    - Increment counter on each signal generation request
    - Record latency histogram with execution time
    - Example: `signal_generation_latency_seconds.observe(elapsed_time)`
  - [ ] Subtask 11.5: Instrument backtest engine with metrics:
    - Increment counter on each backtest execution
    - Record duration histogram
    - Update active_signals_count gauge
  - [ ] Subtask 11.6: Instrument database queries with metrics:
    - Decorator: `@instrument_query` on repository methods
    - Record query duration and query type (SELECT, INSERT, UPDATE)
  - [ ] Subtask 11.7: Create `/api/v1/metrics` endpoint in FastAPI:
    - Route: GET /metrics
    - Response: Prometheus text format (application/openmetrics-text)
    - Use `prometheus_client.generate_latest()` to export metrics
  - [ ] Subtask 11.8: Configure Prometheus scrape target (Phase 2):
    - Add backend to prometheus.yml: `targets: ["backend:8000"]`
    - Scrape interval: 15 seconds
  - [ ] Subtask 11.9: Document metrics in `backend/docs/observability.md`

- [ ] **Task 12: Grafana Dashboard for Performance Monitoring** (AC: 10)
  - [ ] Subtask 12.1: Create Grafana dashboard JSON: `infrastructure/grafana/performance_dashboard.json`
  - [ ] Subtask 12.2: Dashboard panels:
    - Panel 1: Signal generation latency (p50, p95, p99) - Line chart over time
    - Panel 2: Backtest execution duration (p95) - Line chart
    - Panel 3: Database query duration by type (average) - Stacked area chart
    - Panel 4: Pattern detections per minute (rate) - Bar chart by pattern type
    - Panel 5: Active signals count - Gauge
    - Panel 6: Error rate (errors/total requests) - Single stat with threshold
  - [ ] Subtask 12.3: Add alerts to dashboard:
    - Alert: Signal generation latency p95 > 1 second (NFR1 violation)
    - Alert: Backtest speed < 100 bars/second (NFR7 violation)
    - Alert: Database query duration p95 > 200ms
    - Alert: Error rate > 1%
  - [ ] Subtask 12.4: Configure alert notifications:
    - Slack channel: #performance-alerts
    - Email: dev-team@example.com
  - [ ] Subtask 12.5: Import dashboard to Grafana:
    - API call or manual import via UI
    - Set data source to Prometheus
  - [ ] Subtask 12.6: Document dashboard usage in README

- [ ] **Task 13: Unit Tests for Benchmark Infrastructure** (AC: 6)
  - [ ] Subtask 13.1: Create `backend/tests/unit/test_benchmarks.py`
  - [ ] Subtask 13.2: Test: `test_benchmark_config_loads()`:
    - Verify benchmark configuration loads correctly
    - Assert warmup_rounds, benchmark_rounds, timeout
  - [ ] Subtask 13.3: Test: `test_fixtures_generate_valid_data()`:
    - Verify `sample_ohlcv_bars` fixture generates 1000 bars
    - Verify bars have valid OHLCV values (high >= low, volume >= 0)
  - [ ] Subtask 13.4: Test: `test_benchmark_regression_detection()`:
    - Mock baseline JSON with known values
    - Mock current benchmark results with 15% slower execution
    - Assert regression detected (>10% threshold)
  - [ ] Subtask 13.5: Test: `test_prometheus_metrics_export()`:
    - Call GET /api/v1/metrics
    - Assert response format is Prometheus text format
    - Assert metrics include signal_generation_latency_seconds
  - [ ] Subtask 13.6: Test: `test_flame_graph_generation()`:
    - Mock py-spy execution
    - Verify flame graph SVG file created
  - [ ] Subtask 13.7: Follow pytest patterns [Source: architecture/12-testing-strategy.md]

- [ ] **Task 14: Documentation** (AC: all)
  - [ ] Subtask 14.1: Create `backend/docs/performance-benchmarking.md` guide:
    - Explain purpose of performance benchmarking
    - NFR1 and NFR7 requirements and rationale
    - How to run benchmarks locally
    - How to interpret benchmark results
    - Optimization strategies and best practices
  - [ ] Subtask 14.2: Document benchmark suite structure:
    - List all benchmark files and what they test
    - Explain fixtures and shared configuration
  - [ ] Subtask 14.3: Document CI benchmark workflow:
    - How regression detection works (>10% threshold)
    - How to view benchmark results in PR comments
    - How to investigate performance regressions
  - [ ] Subtask 14.4: Document profiling tools:
    - How to use py-spy for flame graphs
    - How to use memory-profiler for memory analysis
    - How to interpret flame graphs
  - [ ] Subtask 14.5: Document database index optimization:
    - Recommended indexes for common queries
    - How to use EXPLAIN ANALYZE to verify index usage
  - [ ] Subtask 14.6: Document Prometheus metrics:
    - List all exported metrics and their purpose
    - Example PromQL queries for common analysis
    - Link to Grafana dashboard
  - [ ] Subtask 14.7: Add inline code comments explaining performance-critical sections
  - [ ] Subtask 14.8: Update main project README with link to performance docs

- [ ] **Task 15: Performance Testing Best Practices** (AC: 4, 5)
  - [ ] Subtask 15.1: Create `backend/benchmarks/README.md` with best practices:
    - Always run benchmarks on consistent hardware (same CPU, memory)
    - Disable CPU frequency scaling during benchmarks
    - Close background applications that may interfere
    - Run benchmarks multiple times and compare distributions
  - [ ] Subtask 15.2: Add pre-benchmark setup script: `backend/benchmarks/setup.sh`:
    - Check CPU governor: should be "performance" not "powersave"
    - Check available memory: warn if <4GB free
    - Populate test database with benchmark data
  - [ ] Subtask 15.3: Document optimization workflow:
    - 1. Run benchmarks to establish baseline
    - 2. Profile hot paths with py-spy
    - 3. Implement optimizations (vectorization, caching)
    - 4. Re-run benchmarks to verify improvement
    - 5. Commit optimizations with benchmark comparison
  - [ ] Subtask 15.4: Add code review checklist for performance changes:
    - Does this change affect hot path (signal generation, backtesting)?
    - Have benchmarks been run to verify no regression?
    - Have flame graphs been analyzed for bottlenecks?

## Dev Notes

### Previous Story Context

**Story 12.1 (Custom Backtesting Engine Architecture)** created the BacktestEngine that needs performance benchmarking:
- `BacktestEngine` class with target: process 10,000 bars in <5 seconds [Source: docs/stories/epic-12/12.1.custom-backtesting-engine-architecture.md]
- This story validates that target is met and enforces it in CI

**Story 12.7 (Regression Testing Automation)** created automated testing infrastructure:
- GitHub Actions workflows for scheduled testing
- Benchmark regression detection builds on this pattern
- This story extends CI to include performance regression detection

**This story is critical for NFR compliance** - without automated benchmarking, performance can degrade silently over time (performance drift similar to parameter drift).

### Epic 12 Context

This story is part of Epic 12: Backtesting & Validation Framework. It delivers the **performance benchmarking infrastructure** that ensures the system meets NFR1 and NFR7 requirements. This is critical for:
- **NFR1 Compliance**: <1 second per symbol per bar analyzed for signal generation
- **NFR7 Compliance**: 100+ bars/second for backtesting
- **Performance Regression Detection**: Prevent performance degradation as code evolves
- **Production Readiness**: Ensure system can handle production load before deployment

### Performance Requirements Deep Dive

**NFR1: Signal Generation Latency <1 second per symbol per bar** [Source: docs/prd/requirements.md#NFR1]

This budget breaks down as:
- Volume analysis (volume_ratio, spread_ratio): <50ms
- Pattern detection (SpringDetector, SOSDetector, UTADDetector): <500ms (largest component)
- Signal generation and validation: <200ms
- Database operations (save pattern, save signal): <150ms
- Notification delivery: <100ms
- **Total: <1000ms (1 second)**

Why this matters:
- Live trading: Signals must be generated before next bar closes (e.g., 5-min bar means <5 minutes total, but <1 second ensures responsiveness)
- Multi-symbol analysis: Analyzing 50 symbols must complete in <50 seconds for timely signals
- User experience: Dashboard updates feel instant when latency <1 second

**NFR7: Backtest Speed 100+ bars/second** [Source: docs/prd/requirements.md#NFR7]

Why 100 bars/second:
- 4 years of daily data = ~1000 bars
- At 100 bars/second: 1000 bars / 100 = 10 seconds per backtest
- Walk-forward testing (4 windows) = 40 seconds
- Regression testing (10 symbols) = 100 seconds (~2 minutes)
- Acceptable for automated monthly regression tests

Below 100 bars/second:
- 10 symbols × 1000 bars = 10,000 bars total
- At 50 bars/second: 10,000 / 50 = 200 seconds (>3 minutes) - too slow for CI

**Why 10% Regression Threshold for CI?**

- 10% is statistically significant (not just noise from CPU variance)
- Small enough to catch meaningful regressions early
- Large enough to avoid false positives from test environment variability
- Example: 0.8 seconds → 0.88 seconds = 10% regression → FAIL CI

### Benchmarking Strategy

**Types of Benchmarks:**

1. **Microbenchmarks**: Test individual functions (volume_ratio calculation, pattern detection logic)
   - Fast to run (<1 second each)
   - Useful for identifying specific bottlenecks
   - Example: Benchmark `SpringDetector.detect()` in isolation

2. **Component Benchmarks**: Test complete modules (PatternEngine, BacktestEngine)
   - Moderate runtime (1-10 seconds)
   - Reflect realistic usage patterns
   - Example: Benchmark full signal generation pipeline

3. **Integration Benchmarks**: Test end-to-end workflows with database
   - Slower to run (10-60 seconds)
   - Most realistic, includes I/O overhead
   - Example: Benchmark backtest with real database writes

4. **Load Tests**: Test concurrent usage (100 concurrent symbols)
   - Identify scalability limits
   - Detect race conditions and deadlocks
   - Example: 100 users requesting pattern detection simultaneously

**Benchmark Workflow:**

1. **Baseline Establishment**: Run benchmarks on main branch, save results
2. **PR Benchmarks**: Run benchmarks on feature branch
3. **Comparison**: Compare feature branch results to baseline
4. **Regression Detection**: Fail CI if any benchmark >10% slower
5. **Reporting**: Post benchmark comparison as PR comment

### Profiling and Optimization

**Hot Path Identification with Flame Graphs:**

Flame graphs visualize function call stacks and time spent in each function:
- X-axis: Percentage of total time (wider = more time)
- Y-axis: Call stack depth (higher = deeper nesting)
- Color: Different colors distinguish functions (no semantic meaning)

How to read:
- Wide blocks at top = hot functions (spend most time there)
- Narrow blocks = infrequent or fast functions
- Deep stacks = complex call chains (potential for optimization)

Example flame graph analysis:
```
SpringDetector.detect() ████████████████████ 60%
  ├─ volume_validator() ████ 10%
  ├─ find_trading_range() ████████ 20%
  └─ calculate_spring_confidence() ████████████ 30%
```

Optimization targets: `calculate_spring_confidence()` (30% of time), `find_trading_range()` (20% of time)

**Vectorization Strategy:**

Replace Python loops with NumPy vectorized operations for 10-100x speedup.

Before (slow):
```python
volume_ratios = []
for bar in bars:
    avg_volume = calculate_avg_volume(bars[:bar.index], window=20)
    volume_ratios.append(bar.volume / avg_volume)
```

After (fast):
```python
import numpy as np
volumes = np.array([bar.volume for bar in bars])
avg_volumes = np.convolve(volumes, np.ones(20)/20, mode='valid')
volume_ratios = volumes[20:] / avg_volumes  # Vectorized division
```

Speedup: 10-50x for 1000 bars

**Caching Strategy:**

Cache expensive calculations that don't change frequently.

Example: Trading range identification
- Trading ranges don't change per bar (only when new range forms)
- Cache range boundaries for 60 seconds
- Invalidate cache when new range detected

```python
from functools import lru_cache
import time

@lru_cache(maxsize=100)
def get_trading_range(symbol: str, timestamp: int):
    # Expensive calculation
    return TradingRange(...)

# Call with timestamp rounded to minute
range = get_trading_range("AAPL", int(time.time() / 60))
```

### Database Optimization

**Index Strategy:**

Composite indexes optimize multi-column queries:

Example query:
```sql
SELECT * FROM ohlcv_bars
WHERE symbol = 'AAPL' AND timeframe = '1d' AND timestamp BETWEEN '2020-01-01' AND '2024-12-31'
ORDER BY timestamp DESC;
```

Index needed: `(symbol, timeframe, timestamp)` in that order

Why column order matters:
- Index is like a phone book sorted by (Last Name, First Name)
- Can search by Last Name alone, or Last Name + First Name
- Cannot search efficiently by First Name alone
- Similarly: (symbol, timeframe, timestamp) supports queries filtering by symbol, or symbol+timeframe, or symbol+timeframe+timestamp
- Does NOT support filtering by timeframe alone or timestamp alone

**EXPLAIN ANALYZE for Query Optimization:**

```sql
EXPLAIN ANALYZE SELECT * FROM patterns
WHERE symbol = 'AAPL' AND pattern_type = 'SPRING' AND detection_time > '2024-01-01';
```

Output (before index):
```
Seq Scan on patterns  (cost=0.00..1234.56 rows=100 width=200) (actual time=0.123..45.678 rows=98 loops=1)
  Filter: (symbol = 'AAPL' AND pattern_type = 'SPRING' AND detection_time > '2024-01-01')
Planning Time: 0.5 ms
Execution Time: 45.7 ms
```

Output (after index):
```
Index Scan using idx_patterns_symbol_type_time on patterns  (cost=0.29..12.34 rows=100 width=200) (actual time=0.012..1.234 rows=98 loops=1)
  Index Cond: (symbol = 'AAPL' AND pattern_type = 'SPRING' AND detection_time > '2024-01-01')
Planning Time: 0.3 ms
Execution Time: 1.3 ms
```

Improvement: 45.7ms → 1.3ms (35x faster)

**Partial Indexes for Status Queries:**

Many queries filter by status (e.g., "Find RUNNING backtests"):

```sql
CREATE INDEX idx_backtest_status_running ON backtest_results (id) WHERE status = 'RUNNING';
```

Advantages:
- Smaller index (only includes RUNNING rows)
- Faster queries (less data to scan)
- Automatically maintained (PostgreSQL updates on INSERT/UPDATE/DELETE)

Use case: Dashboard showing active backtests only (status = 'RUNNING')

### Prometheus Metrics and Monitoring

**Metric Types:**

1. **Counter**: Monotonically increasing value (never decreases)
   - Example: `signal_generation_requests_total`
   - Use: Track total number of events over time
   - PromQL: `rate(signal_generation_requests_total[5m])` for requests per second

2. **Gauge**: Value that can go up or down
   - Example: `active_signals_count`
   - Use: Track current state (memory usage, active connections)
   - PromQL: `active_signals_count` for current value

3. **Histogram**: Distribution of values in configurable buckets
   - Example: `signal_generation_latency_seconds` with buckets [0.1, 0.5, 1.0, 2.0, 5.0]
   - Use: Track latency distributions, calculate percentiles
   - PromQL: `histogram_quantile(0.95, signal_generation_latency_seconds)` for p95 latency

**Key Metrics for Performance Monitoring:**

- Signal generation latency p95: Should be <1 second (NFR1)
- Backtest duration p95: Should align with NFR7 (1000 bars in <10 seconds)
- Database query duration by type: Identify slow query types
- Pattern detections per minute: Track system throughput
- Error rate: Errors / total requests (should be <1%)

**Alerting Strategy:**

- **Critical alerts** (page on-call): NFR1/NFR7 violations, error rate >5%
- **Warning alerts** (Slack notification): Latency approaching thresholds (p95 >800ms)
- **Info alerts** (dashboard only): Performance improvements, unusual traffic patterns

### CI Integration Strategy

**Benchmark Regression Detection Workflow:**

1. **Feature branch**: Developer makes changes, opens PR
2. **GitHub Actions trigger**: Workflow runs on pull_request event
3. **Setup environment**: Checkout code, install dependencies, setup database
4. **Run benchmarks**: Execute pytest benchmarks, save results to JSON
5. **Fetch baseline**: Download baseline results from main branch artifact or Git LFS
6. **Compare results**: For each benchmark, calculate percent change
7. **Detect regressions**: Flag benchmarks with >10% slowdown
8. **Post PR comment**: Table showing all benchmarks with change percentages
9. **Fail workflow**: If any benchmark regressed >10%, fail CI check
10. **Block merge**: PR cannot merge until performance regression fixed

**Example PR Comment:**

```markdown
## Benchmark Results

| Benchmark | Baseline | Current | Change | Status |
|-----------|----------|---------|--------|--------|
| signal_generation_latency | 0.85s | 0.82s | -3.5% | ✅ IMPROVED |
| backtest_speed | 125 bars/s | 110 bars/s | -12.0% | ❌ REGRESSION |
| database_query_ohlcv | 45ms | 48ms | +6.7% | ⚠️ SLOWER |

**FAIL**: 1 benchmark regressed >10%

Please investigate `backtest_speed` regression before merging.
```

### Load Testing Strategy

**Why 100 Concurrent Symbols?**

Production usage estimate:
- Personal trading: Monitor 20-50 symbols
- Professional trading: Monitor 100-200 symbols
- System should handle 100 concurrent symbols comfortably

Load test simulates:
- 100 symbols analyzed simultaneously
- Each symbol receives new bar every 5 minutes (live trading simulation)
- Pattern detection runs for each symbol
- Database writes for patterns and signals

**Locust Load Test Configuration:**

```python
from locust import HttpUser, task, between

class TradingSystemUser(HttpUser):
    wait_time = between(1, 5)  # Wait 1-5 seconds between tasks

    @task
    def detect_patterns(self):
        self.client.post("/api/v1/patterns/detect", json={
            "symbol": self.symbol,
            "bars": self.generate_ohlcv_bars(100)
        })

    def on_start(self):
        # Each user assigned a unique symbol
        self.symbol = f"SYMBOL_{self.user_id}"
```

Run:
```bash
locust -f locustfile.py --headless -u 100 -r 10 --run-time 5m
```

- `-u 100`: 100 concurrent users (symbols)
- `-r 10`: Spawn 10 users per second (ramp-up)
- `--run-time 5m`: Run for 5 minutes

**Load Test Metrics:**

- Average response time: Should be <2 seconds under load
- p95 response time: Should be <5 seconds
- Error rate: Should be <1% (occasional timeouts acceptable)
- Requests per second: System should sustain 20+ RPS (100 symbols / 5 seconds)

### File Locations

**Benchmark Suite** [Source: architecture/10-unified-project-structure.md]:
- Benchmark directory: `backend/benchmarks/`
- Benchmark config: `backend/benchmarks/benchmark_config.py`
- Fixtures: `backend/benchmarks/conftest.py`
- Signal generation benchmarks: `backend/benchmarks/test_signal_generation_latency.py`
- Backtest benchmarks: `backend/benchmarks/test_backtest_speed.py`
- Database benchmarks: `backend/benchmarks/test_database_queries.py`
- Load tests: `backend/benchmarks/test_load.py`
- Locust config: `backend/benchmarks/locustfile.py`
- Profiling script: `backend/benchmarks/profile_hot_paths.py`
- Comparison script: `backend/benchmarks/compare_benchmarks.py`

**Profiling Results**:
- Flame graphs: `backend/benchmarks/profiles/` (signal_generation_flame.svg, backtest_flame.svg)
- Benchmark reports: `backend/benchmarks/reports/` (histogram.html)
- Benchmark history: `backend/benchmarks/.benchmarks/` (JSON results from each run)

**CI/CD**:
- Benchmark workflow: `.github/workflows/benchmarks.yaml`

**Monitoring**:
- Prometheus metrics: `backend/src/observability/metrics.py`
- Grafana dashboard: `infrastructure/grafana/performance_dashboard.json`

**Documentation**:
- Performance guide: `backend/docs/performance-benchmarking.md`
- Observability guide: `backend/docs/observability.md`
- Benchmark suite README: `backend/benchmarks/README.md`

**Database Migrations**:
- Index optimization migration: `backend/alembic/versions/{timestamp}_optimize_indexes_for_performance.py`

### Tech Stack

**Benchmarking Tools** [Source: architecture/3-tech-stack.md]:
- pytest-benchmark 4.0+ for automated benchmarking
- py-spy 0.3+ for flame graph profiling
- memory-profiler 0.61+ for memory analysis
- locust 2.20+ for load testing

**Monitoring Tools** [Source: architecture/3-tech-stack.md]:
- prometheus-client 0.19+ for metrics export
- Prometheus (Phase 2) for metrics storage
- Grafana (Phase 2) for dashboards and alerts

**Database** [Source: architecture/3-tech-stack.md]:
- PostgreSQL 15+ with EXPLAIN ANALYZE for query optimization
- SQLAlchemy 2.0+ for query instrumentation

**CI/CD** [Source: architecture/3-tech-stack.md]:
- GitHub Actions for automated benchmark runs on PRs

### Coding Standards

**Naming Conventions** [Source: architecture/15-coding-standards.md]:
- Benchmark functions: `test_<component>_<metric>(benchmark, fixtures)` (e.g., `test_signal_generation_latency`)
- Profiling scripts: `profile_<component>.py` (e.g., `profile_hot_paths.py`)
- Prometheus metrics: `<component>_<metric>_<unit>` (e.g., `signal_generation_latency_seconds`)

**Decimal Precision** [Source: architecture/15-coding-standards.md#Decimal-Precision]:
- Use Decimal (NOT float) for financial calculations in benchmarks
- Ensures benchmark results reflect production performance accurately

**Type Safety**:
- Use strict type hints on all benchmark functions
- Annotate pytest fixtures with return types

### Testing Strategy

#### Unit Tests [Source: architecture/12-testing-strategy.md]
- Test benchmark infrastructure (config loading, regression detection logic)
- Test Prometheus metrics export
- Mock external dependencies (database, file I/O)
- Location: `backend/tests/unit/test_benchmarks.py`

#### Benchmark Tests
- Test individual components (volume analysis, pattern detection)
- Test complete workflows (signal generation, backtesting)
- Test database queries with realistic data
- Location: `backend/benchmarks/test_*.py`

#### Load Tests
- Test concurrent usage (100 symbols)
- Test scalability limits
- Detect race conditions
- Location: `backend/benchmarks/test_load.py` and `backend/benchmarks/locustfile.py`

#### Integration Tests
- Covered by benchmark suite (benchmarks use real database, real components)

### Performance Optimization Guidelines

**When to Optimize:**
1. Benchmark shows >1 second latency (NFR1 violation)
2. Benchmark shows <100 bars/second (NFR7 violation)
3. Flame graph shows function consuming >10% of total time
4. Database query taking >100ms

**Optimization Techniques:**

1. **Vectorization**: Replace Python loops with NumPy operations
   - Use: Volume ratio calculations, spread ratio calculations
   - Speedup: 10-100x

2. **Caching**: Store expensive calculations
   - Use: Trading range identification, rolling averages
   - Speedup: 2-10x (depends on cache hit rate)

3. **Database Indexing**: Add composite indexes
   - Use: All queries filtering by multiple columns
   - Speedup: 10-100x (depends on table size)

4. **Query Optimization**: Reduce N+1 queries, use JOINs
   - Use: Loading patterns with trading ranges
   - Speedup: 5-20x

5. **Algorithmic Improvements**: Better algorithms (e.g., binary search vs linear search)
   - Use: Pattern matching, level identification
   - Speedup: 10-1000x (depends on algorithm)

**Optimization Workflow:**
1. Profile to identify bottleneck
2. Implement optimization
3. Benchmark to verify improvement
4. Commit with benchmark comparison in commit message

### Dependencies and Risks

**Dependencies:**
- Story 12.1 (Custom Backtesting Engine) must be complete - need BacktestEngine to benchmark
- Signal generation infrastructure (Epic 8) must be complete - need signal pipeline to benchmark
- Pattern detectors (Epic 6) must be complete - need detectors to benchmark

**Risks:**
1. **Benchmark environment variability**: GitHub Actions runners have variable CPU performance
   - Mitigation: Run benchmarks multiple times, use statistical analysis (mean, std dev)
2. **False positives**: Small performance changes flagged as regressions
   - Mitigation: 10% threshold (not 5%), compare distributions not single runs
3. **Optimization rabbit holes**: Spending too much time optimizing non-critical paths
   - Mitigation: Use flame graphs to identify hot paths, optimize only functions >10% of time
4. **Database size affects benchmarks**: Queries slower on large databases
   - Mitigation: Use consistent test database size, document data volume

### Notes for Developer

1. **Start with benchmark infrastructure** - pytest-benchmark, fixtures, CI workflow
2. **Establish baselines early** - run benchmarks on current code before optimizations
3. **Profile before optimizing** - flame graphs reveal true bottlenecks, not assumptions
4. **Optimize hot paths only** - functions consuming >10% of time
5. **Vectorize with NumPy** - 10-100x speedup for numerical operations
6. **Add database indexes** - composite indexes for multi-column queries
7. **Instrument with Prometheus** - metrics enable production monitoring
8. **Integrate with CI** - automated regression detection prevents performance drift
9. **Document optimizations** - explain why optimization works, include benchmark comparison
10. **Test under load** - 100 concurrent symbols reveals scalability issues

**Key Implementation Order:**
1. Benchmark infrastructure (pytest-benchmark, fixtures, config)
2. Signal generation benchmarks (NFR1 validation)
3. Backtest benchmarks (NFR7 validation)
4. Database benchmarks (query optimization)
5. Profiling (flame graphs, bottleneck identification)
6. Optimizations (vectorization, caching, indexing)
7. CI integration (regression detection, PR comments)
8. Load testing (concurrent symbols)
9. Prometheus metrics (production monitoring)
10. Grafana dashboards (visualization)
11. Documentation

**Testing as You Go:**
- Run benchmarks after each optimization to verify improvement
- Use pytest-benchmark --compare to see before/after
- Generate flame graphs before and after optimization
- Verify no regressions in other benchmarks (ensure optimization didn't break something else)

**Optimization Priorities:**
1. **High impact, low effort**: Database indexing (add index = 10x speedup)
2. **High impact, medium effort**: NumPy vectorization (rewrite loops = 10-50x speedup)
3. **Medium impact, low effort**: Caching (add decorator = 2-5x speedup)
4. **Low impact, high effort**: Algorithmic improvements (complex, varies)

## Testing

### Testing Framework [Source: architecture/12-testing-strategy.md]
- Use pytest 8.0+ for all backend testing
- Use pytest-benchmark 4.0+ for performance benchmarking
- Benchmark files: `backend/benchmarks/test_*.py`
- Unit test files: `backend/tests/unit/test_benchmarks.py`

### Test Coverage Requirements [Source: architecture/12-testing-strategy.md, NFR8]
- Target 90%+ test coverage (NFR8) for benchmark infrastructure code
- Cover benchmark configuration loading
- Cover regression detection logic
- Cover Prometheus metrics export
- Cover profiling script execution

### Benchmark Test Requirements

**Signal Generation Latency Benchmarks**:
- Test volume analysis: <50ms per bar
- Test Spring detection: <200ms per bar
- Test SOS detection: <150ms per bar
- Test UTAD detection: <150ms per bar
- Test full pipeline: NFR1 <1 second per symbol per bar

**Backtest Speed Benchmarks**:
- Test BacktestEngine: NFR7 >100 bars/second
- Test order simulation: <10ms per order
- Test position tracking: <5ms per bar
- Test metrics calculation: <50ms for 100 trades

**Database Query Benchmarks**:
- Test OHLCV range query: <50ms
- Test pattern lookup: <30ms
- Test signal history query: <40ms
- Test audit log query: <100ms

**Load Test Requirements**:
- Test 100 concurrent symbols
- Average response time: <2 seconds
- p95 response time: <5 seconds
- Error rate: <1%

### Unit Test Requirements

**Test Benchmark Infrastructure**:
- Test benchmark config loads correctly
- Test fixtures generate valid data (1000 OHLCV bars)
- Test regression detection logic (>10% threshold)
- Test Prometheus metrics export format

**Mock Strategy**:
- Mock database for fast benchmark setup
- Mock BacktestEngine for unit tests (use real engine for benchmarks)
- Mock py-spy for flame graph generation tests

### CI/CD Testing

**GitHub Actions Workflow**:
- Run benchmarks on every PR
- Compare to baseline from main branch
- Fail if any benchmark regresses >10%
- Post PR comment with benchmark comparison table

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-20 | 1.0 | Initial story creation with comprehensive performance benchmarking details | Bob (Scrum Master) |

## Dev Agent Record

_This section will be populated by the development agent during implementation._

### Agent Model Used

_To be filled by dev agent_

### Debug Log References

_To be filled by dev agent_

### Completion Notes

_To be filled by dev agent_

### File List

_To be filled by dev agent_

## QA Results

_This section will be populated by the QA agent after story completion._
