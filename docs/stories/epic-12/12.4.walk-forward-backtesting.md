# Story 12.4: Walk-Forward Backtesting

## Status
Draft

## Story
**As a** validation engineer,
**I want** walk-forward testing to validate out-of-sample performance,
**so that** system isn't overfit to specific historical periods.

## Acceptance Criteria
1. Walk-forward periods: Train on 2020-2022, validate on 2023-2024
2. Rolling windows: 6-month train, 3-month validate, roll forward
3. Stability check: performance consistent across all validation windows
4. Metric tracking: win rate, avg R, profit factor, max drawdown per window
5. Degradation detection: alert if validation performance <80% of training
6. Function: `walk_forward_test(symbols, config) -> List[ValidationWindow]`
7. Visualization: chart showing train vs validation performance over time
8. Statistical significance: p-value calculation for performance differences
9. Unit test: synthetic data validates walk-forward logic
10. Integration test: real data produces expected validation windows

## Tasks / Subtasks

- [ ] **Task 1: Create Walk-Forward Data Models** (AC: 4, 6)
  - [ ] Subtask 1.1: Create Pydantic model `ValidationWindow` in `backend/src/models/backtest.py`:
    - `window_id`: UUID (unique identifier for this validation window)
    - `window_number`: int (sequential window number, e.g., 1, 2, 3...)
    - `train_start_date`: date (start of training period)
    - `train_end_date`: date (end of training period)
    - `validate_start_date`: date (start of validation period)
    - `validate_end_date`: date (end of validation period)
    - `train_metrics`: BacktestMetrics (performance during training period)
    - `validate_metrics`: BacktestMetrics (out-of-sample performance during validation)
    - `train_backtest_id`: UUID (reference to training backtest run)
    - `validate_backtest_id`: UUID (reference to validation backtest run)
    - `performance_ratio`: Decimal (validate_metric / train_metric, e.g., 0.85 = 85% of training)
    - `degradation_detected`: bool (True if validation <80% of training)
    - `created_at`: datetime (UTC)
  - [ ] Subtask 1.2: Create Pydantic model `WalkForwardConfig` in `backend/src/models/backtest.py`:
    - `symbols`: List[str] (symbols to test, e.g., ["AAPL", "MSFT", "GOOGL", "TSLA"])
    - `overall_start_date`: date (e.g., 2020-01-01)
    - `overall_end_date`: date (e.g., 2024-12-31)
    - `train_period_months`: int (default 6 months)
    - `validate_period_months`: int (default 3 months)
    - `backtest_config`: BacktestConfig (base config for running backtests)
    - `primary_metric`: Literal["win_rate", "avg_r_multiple", "profit_factor", "sharpe_ratio"] (default "win_rate")
    - `degradation_threshold`: Decimal (default 0.80, meaning 80% of training performance)
  - [ ] Subtask 1.3: Create Pydantic model `WalkForwardResult` in `backend/src/models/backtest.py`:
    - `walk_forward_id`: UUID (unique identifier for this walk-forward test)
    - `config`: WalkForwardConfig (configuration used)
    - `windows`: List[ValidationWindow] (all validation windows tested)
    - `summary_statistics`: Dict (aggregate stats across all windows)
    - `stability_score`: Decimal (coefficient of variation of validation performance)
    - `degradation_windows`: List[int] (window numbers where degradation detected)
    - `statistical_significance`: Dict (p-values for train vs validate differences)
    - `created_at`: datetime (UTC)
  - [ ] Subtask 1.4: Add Decimal precision validators [Source: architecture/15-coding-standards.md#Decimal-Precision]
  - [ ] Subtask 1.5: Add UTC timestamp validators [Source: architecture/4-data-models.md#OHLCVBar]
  - [ ] Subtask 1.6: Unit test all models with Pydantic validation (pytest)
  - [ ] Subtask 1.7: Run `pydantic-to-typescript` to generate TypeScript types

- [ ] **Task 2: Create Walk-Forward Engine** (AC: 1, 2, 6, 9, 10)
  - [ ] Subtask 2.1: Create class `WalkForwardEngine` in `backend/src/backtesting/walk_forward_engine.py`
  - [ ] Subtask 2.2: Dependency injection: BacktestEngine (from Story 12.1)
  - [ ] Subtask 2.3: Implement `walk_forward_test(symbols: List[str], config: WalkForwardConfig) -> WalkForwardResult`:
    - Calculate total date range from config
    - Generate rolling window periods using train_period_months and validate_period_months
    - For each window:
      - Run training backtest using BacktestEngine
      - Run validation backtest using BacktestEngine
      - Calculate performance ratio
      - Detect degradation
      - Store ValidationWindow
    - Calculate summary statistics
    - Calculate stability score
    - Perform statistical significance tests
    - Return WalkForwardResult
  - [ ] Subtask 2.4: Implement `_generate_windows(config: WalkForwardConfig) -> List[Tuple[date, date, date, date]]`:
    - Input: overall date range, train_period_months, validate_period_months
    - Output: List of (train_start, train_end, validate_start, validate_end) tuples
    - Logic: Rolling window with continuous progression
    - Example for 6-month train, 3-month validate:
      - Window 1: Train 2020-01 to 2020-06, Validate 2020-07 to 2020-09
      - Window 2: Train 2020-04 to 2020-09, Validate 2020-10 to 2020-12
      - Window 3: Train 2020-07 to 2020-12, Validate 2021-01 to 2021-03
      - Continue until overall_end_date reached
  - [ ] Subtask 2.5: Implement `_run_backtest_for_window(symbol, start_date, end_date, config) -> BacktestResult`:
    - Create BacktestConfig from WalkForwardConfig.backtest_config
    - Set date range to window period
    - Call BacktestEngine.run_backtest()
    - Return BacktestResult
  - [ ] Subtask 2.6: Implement `_calculate_performance_ratio(train_metrics, validate_metrics, primary_metric) -> Decimal`:
    - Extract primary_metric value from train_metrics (e.g., win_rate)
    - Extract primary_metric value from validate_metrics
    - Calculate ratio: validate_value / train_value
    - Handle edge case: train_value = 0 (return Decimal("0"))
    - Return performance ratio
  - [ ] Subtask 2.7: Implement `_detect_degradation(performance_ratio, threshold) -> bool`:
    - Compare performance_ratio vs degradation_threshold
    - Return True if performance_ratio < threshold (e.g., <0.80)
    - Return False otherwise
  - [ ] Subtask 2.8: Use type hints and docstrings following coding standards [Source: architecture/15-coding-standards.md]
  - [ ] Subtask 2.9: Add structlog logging for each window execution [Source: architecture/3-tech-stack.md#structlog]

- [ ] **Task 3: Calculate Summary Statistics** (AC: 3, 4)
  - [ ] Subtask 3.1: Implement `_calculate_summary_statistics(windows: List[ValidationWindow]) -> Dict`:
    - Calculate aggregate metrics across all validation windows:
      - `avg_validate_win_rate`: average win rate across all validate periods
      - `avg_validate_avg_r`: average R-multiple across all validate periods
      - `avg_validate_profit_factor`: average profit factor across all validate periods
      - `avg_validate_sharpe`: average Sharpe ratio across all validate periods
      - `total_windows`: count of windows
      - `degradation_count`: count of windows with degradation
      - `degradation_percentage`: (degradation_count / total_windows) * 100
    - Return dict with these statistics
  - [ ] Subtask 3.2: Implement `_calculate_stability_score(windows: List[ValidationWindow], primary_metric: str) -> Decimal`:
    - Extract primary_metric values from all validation windows
    - Calculate coefficient of variation (CV) = std_dev / mean
    - Lower CV = more stable performance across windows
    - CV < 0.20 = highly stable, CV 0.20-0.40 = moderately stable, CV > 0.40 = unstable
    - Return stability_score as Decimal
  - [ ] Subtask 3.3: Implement consistency check logic:
    - Performance is "consistent" if CV < 0.30 (configurable threshold)
    - No degradation in >70% of windows
    - Log warning if consistency checks fail

- [ ] **Task 4: Statistical Significance Testing** (AC: 8)
  - [ ] Subtask 4.1: Install scipy for statistical tests (add to `backend/pyproject.toml`)
  - [ ] Subtask 4.2: Implement `_calculate_statistical_significance(windows: List[ValidationWindow]) -> Dict`:
    - Use paired t-test to compare train vs validate metrics
    - Extract train_metrics.win_rate and validate_metrics.win_rate from all windows
    - Perform scipy.stats.ttest_rel(train_values, validate_values)
    - Calculate p-value for win_rate
    - Repeat for avg_r_multiple, profit_factor, sharpe_ratio
    - Return dict: {"win_rate_pvalue": float, "avg_r_pvalue": float, ...}
  - [ ] Subtask 4.3: Interpret p-values:
    - p < 0.05: statistically significant difference (warning: potential overfitting)
    - p >= 0.05: no significant difference (good: train and validate similar)
  - [ ] Subtask 4.4: Add p-value interpretation to WalkForwardResult
  - [ ] Subtask 4.5: Log statistical significance results using structlog

- [ ] **Task 5: Degradation Detection and Alerting** (AC: 5)
  - [ ] Subtask 5.1: Implement degradation detection per window (already in Task 2.7)
  - [ ] Subtask 5.2: Collect degradation_windows (window numbers with degradation)
  - [ ] Subtask 5.3: Implement `_generate_degradation_report(degradation_windows, windows) -> str`:
    - Generate human-readable report listing degraded windows
    - Include: window number, train performance, validate performance, ratio
    - Example: "Window 3: Train Win Rate 65%, Validate Win Rate 48%, Ratio 74% (DEGRADED)"
  - [ ] Subtask 5.4: Log degradation warnings using structlog with context:
    - Log level: WARNING
    - Include: walk_forward_id, window_number, performance_ratio, threshold
  - [ ] Subtask 5.5: Store degradation_windows list in WalkForwardResult
  - [ ] Subtask 5.6: Add degradation summary to summary_statistics (count, percentage)

- [ ] **Task 6: Unit Testing** (AC: 9)
  - [ ] Subtask 6.1: Create `backend/tests/unit/test_walk_forward_engine.py`
  - [ ] Subtask 6.2: Test `_generate_windows()` with synthetic config:
    - Config: 2020-01-01 to 2021-12-31, 6-month train, 3-month validate
    - Verify correct number of windows generated
    - Verify window dates are sequential and non-overlapping (validate periods)
    - Verify train periods are rolling (overlap allowed)
  - [ ] Subtask 6.3: Test `_calculate_performance_ratio()`:
    - Train win rate 60%, validate win rate 54% → ratio 0.90 (90%)
    - Train win rate 60%, validate win rate 45% → ratio 0.75 (75%)
    - Train win rate 0%, validate win rate 50% → ratio 0.00 (edge case)
  - [ ] Subtask 6.4: Test `_detect_degradation()`:
    - Ratio 0.85, threshold 0.80 → False (no degradation, 85% > 80%)
    - Ratio 0.75, threshold 0.80 → True (degradation, 75% < 80%)
    - Ratio 0.80, threshold 0.80 → False (edge case, 80% = 80%)
  - [ ] Subtask 6.5: Test `_calculate_stability_score()`:
    - Create synthetic validation windows with known win rates
    - Win rates: [60%, 61%, 59%, 60%] → low CV (stable)
    - Win rates: [60%, 45%, 70%, 50%] → high CV (unstable)
    - Verify CV calculation matches expected values
  - [ ] Subtask 6.6: Test `_calculate_statistical_significance()`:
    - Create synthetic windows with train > validate consistently
    - Verify p-value < 0.05 (significant difference)
    - Create synthetic windows with train ≈ validate
    - Verify p-value >= 0.05 (no significant difference)
  - [ ] Subtask 6.7: Mock BacktestEngine to return predictable BacktestResults
  - [ ] Subtask 6.8: Test full `walk_forward_test()` with mocked backtests:
    - Verify correct number of windows generated
    - Verify each window has train and validate backtests
    - Verify summary statistics calculated correctly
    - Verify degradation detection works
  - [ ] Subtask 6.9: Follow pytest patterns [Source: architecture/12-testing-strategy.md#Backend-Testing]
  - [ ] Subtask 6.10: Use factory-boy for WalkForwardConfig, ValidationWindow test fixtures

- [ ] **Task 7: Integration Testing with Real Backtests** (AC: 1, 2, 10)
  - [ ] Subtask 7.1: Create `backend/tests/integration/test_walk_forward_integration.py`
  - [ ] Subtask 7.2: Load real historical OHLCV data for test symbols (AAPL, MSFT) from 2020-2024
  - [ ] Subtask 7.3: Create WalkForwardConfig:
    - Symbols: ["AAPL", "MSFT"]
    - Date range: 2020-01-01 to 2023-12-31
    - Train: 6 months, Validate: 3 months
    - Primary metric: win_rate
  - [ ] Subtask 7.4: Run WalkForwardEngine.walk_forward_test() with real BacktestEngine
  - [ ] Subtask 7.5: Verify WalkForwardResult structure:
    - Contains multiple ValidationWindow objects
    - Each window has train_metrics and validate_metrics
    - Summary statistics calculated
    - Stability score calculated
  - [ ] Subtask 7.6: Assert expected number of windows based on date range
  - [ ] Subtask 7.7: Verify window date boundaries are correct (no gaps, proper rolling)
  - [ ] Subtask 7.8: Verify performance ratios calculated correctly for each window
  - [ ] Subtask 7.9: Mark test with `@pytest.mark.slow` since it runs multiple backtests
  - [ ] Subtask 7.10: Skip test if historical data not available (graceful degradation)

- [ ] **Task 8: Visualization Data Preparation** (AC: 7)
  - [ ] Subtask 8.1: Create Pydantic model `WalkForwardChartData` in `backend/src/models/backtest.py`:
    - `window_labels`: List[str] (e.g., ["Window 1", "Window 2", ...])
    - `train_win_rates`: List[Decimal] (training win rates per window)
    - `validate_win_rates`: List[Decimal] (validation win rates per window)
    - `train_avg_r`: List[Decimal]
    - `validate_avg_r`: List[Decimal]
    - `train_profit_factor`: List[Decimal]
    - `validate_profit_factor`: List[Decimal]
    - `degradation_flags`: List[bool] (True for degraded windows)
  - [ ] Subtask 8.2: Implement `_prepare_chart_data(walk_forward_result: WalkForwardResult) -> WalkForwardChartData`:
    - Extract metrics from each ValidationWindow
    - Build arrays for train vs validate metrics
    - Return WalkForwardChartData for frontend charting
  - [ ] Subtask 8.3: Include WalkForwardChartData in WalkForwardResult model
  - [ ] Subtask 8.4: Generate TypeScript types for WalkForwardChartData

- [ ] **Task 9: API Endpoints** (AC: 6)
  - [ ] Subtask 9.1: Create `POST /api/v1/backtest/walk-forward` route in `backend/src/api/routes/backtest.py`
  - [ ] Subtask 9.2: Request body: WalkForwardConfig model
  - [ ] Subtask 9.3: Validate config: date ranges valid, symbols exist, train/validate periods > 0
  - [ ] Subtask 9.4: Execute WalkForwardEngine.walk_forward_test() as BackgroundTask (can be long-running)
  - [ ] Subtask 9.5: Return immediately with walk_forward_id and status: RUNNING
  - [ ] Subtask 9.6: Create `GET /api/v1/backtest/walk-forward/{walk_forward_id}` route
  - [ ] Subtask 9.7: Return WalkForwardResult if completed, status: RUNNING if in progress, 404 if not found
  - [ ] Subtask 9.8: Create `GET /api/v1/backtest/walk-forward` route for listing all walk-forward tests (paginated)
  - [ ] Subtask 9.9: Store WalkForwardResult in database via WalkForwardRepository
  - [ ] Subtask 9.10: Unit tests: test endpoints with mock WalkForwardEngine (pytest)
  - [ ] Subtask 9.11: Integration test: run full walk-forward via API, retrieve results (pytest)

- [ ] **Task 10: Walk-Forward Repository** (AC: 6)
  - [ ] Subtask 10.1: Create `WalkForwardRepository` class in `backend/src/repositories/walk_forward_repository.py`
  - [ ] Subtask 10.2: Implement `save_result(result: WalkForwardResult) -> UUID`
  - [ ] Subtask 10.3: Store WalkForwardResult in walk_forward_results table (new table)
  - [ ] Subtask 10.4: Store windows as JSONB (serialize List[ValidationWindow])
  - [ ] Subtask 10.5: Store config as JSONB
  - [ ] Subtask 10.6: Store summary_statistics, statistical_significance as JSONB
  - [ ] Subtask 10.7: Implement `get_result(walk_forward_id: UUID) -> Optional[WalkForwardResult]`
  - [ ] Subtask 10.8: Deserialize JSONB fields back to Pydantic models
  - [ ] Subtask 10.9: Implement `list_results(limit: int, offset: int) -> List[WalkForwardResult]`
  - [ ] Subtask 10.10: Order by created_at DESC for recent first
  - [ ] Subtask 10.11: Unit tests: verify save/retrieve with full WalkForwardResult, test pagination (pytest)

- [ ] **Task 11: Database Migration** (AC: 6)
  - [ ] Subtask 11.1: Create Alembic migration: `{timestamp}_add_walk_forward_tables.py`
  - [ ] Subtask 11.2: Create `walk_forward_results` table:
    - Columns: walk_forward_id (UUID PK), config (JSONB), windows (JSONB), summary_statistics (JSONB), stability_score (NUMERIC), degradation_windows (JSONB), statistical_significance (JSONB), chart_data (JSONB), created_at (TIMESTAMP)
  - [ ] Subtask 11.3: Add indexes: idx_walk_forward_created_at
  - [ ] Subtask 11.4: Run migration: `alembic upgrade head`
  - [ ] Subtask 11.5: Verify schema matches data models

- [ ] **Task 12: CLI Tool for Local Walk-Forward Testing** (AC: 1, 2, 6)
  - [ ] Subtask 12.1: Create `backend/scripts/run_walk_forward.py` CLI script:
    - Argument: `--symbols` (comma-separated list, e.g., "AAPL,MSFT,GOOGL")
    - Argument: `--start-date` (YYYY-MM-DD)
    - Argument: `--end-date` (YYYY-MM-DD)
    - Argument: `--train-months` (default: 6)
    - Argument: `--validate-months` (default: 3)
    - Argument: `--primary-metric` (win_rate, avg_r_multiple, profit_factor, sharpe_ratio)
    - Argument: `--degradation-threshold` (default: 0.80)
    - Argument: `--output` (path for JSON report)
  - [ ] Subtask 12.2: Implement script logic:
    - Build WalkForwardConfig from CLI arguments
    - Run WalkForwardEngine.walk_forward_test()
    - Print summary statistics to console
    - Print degradation warnings if any
    - Optionally save WalkForwardResult as JSON to --output path
  - [ ] Subtask 12.3: Add colored console output:
    - Green for windows without degradation
    - Red for degraded windows
    - Yellow for statistical significance warnings
  - [ ] Subtask 12.4: Document script usage in README

- [ ] **Task 13: Performance Considerations** (AC: 1, 2)
  - [ ] Subtask 13.1: Optimize walk-forward execution:
    - Walk-forward tests run multiple backtests (e.g., 10+ windows)
    - Each backtest can take 1-5 seconds
    - Total execution time can be 10-50 seconds
  - [ ] Subtask 13.2: Consider parallel execution of independent windows (future optimization):
    - Use asyncio or multiprocessing to run backtests in parallel
    - Trade-off: higher memory usage vs faster execution
    - Initial implementation: sequential execution for simplicity
  - [ ] Subtask 13.3: Cache BacktestEngine results to avoid re-running identical backtests
  - [ ] Subtask 13.4: Log execution time per window and total walk-forward time
  - [ ] Subtask 13.5: Add performance metrics to WalkForwardResult:
    - `total_execution_time_seconds`: float
    - `avg_window_execution_time_seconds`: float

- [ ] **Task 14: Documentation** (AC: all)
  - [ ] Subtask 14.1: Create `backend/docs/walk-forward-testing.md` guide:
    - Explain what walk-forward testing is
    - Why it prevents overfitting
    - How to interpret results
    - Performance ratio guidelines (>80% good, 60-80% acceptable, <60% problematic)
    - Stability score interpretation
    - Statistical significance interpretation
  - [ ] Subtask 14.2: Document WalkForwardConfig parameters and defaults
  - [ ] Subtask 14.3: Provide examples of running walk-forward tests locally via CLI
  - [ ] Subtask 14.4: Document API endpoints for walk-forward testing
  - [ ] Subtask 14.5: Add docstrings to all classes and methods in walk_forward_engine.py
  - [ ] Subtask 14.6: Update main project README with link to walk-forward testing docs
  - [ ] Subtask 14.7: Add inline code comments explaining complex logic (window generation, stability calculations)

- [ ] **Task 15: Error Handling and Edge Cases** (AC: 9, 10)
  - [ ] Subtask 15.1: Handle insufficient data for windows:
    - If date range too short for train + validate periods, raise ValueError
    - Minimum requirement: at least 1 full window (train + validate)
  - [ ] Subtask 15.2: Handle missing OHLCV data:
    - If symbol has no data for window period, skip that window
    - Log warning with missing data details
  - [ ] Subtask 15.3: Handle zero trades in window:
    - If backtest produces zero trades, metrics will be undefined
    - Set metrics to None or Decimal("0") as appropriate
    - Flag window as "no trades" in report
  - [ ] Subtask 15.4: Handle BacktestEngine failures:
    - Wrap BacktestEngine calls in try/except
    - Log errors with window context
    - Continue with next window if one fails (graceful degradation)
  - [ ] Subtask 15.5: Validate WalkForwardConfig:
    - train_period_months > 0
    - validate_period_months > 0
    - overall_end_date > overall_start_date
    - symbols list not empty

- [ ] **Task 16: Integration with CI/CD** (AC: 9, 10)
  - [ ] Subtask 16.1: Add walk-forward tests to CI pipeline:
    - Run unit tests on every PR
    - Run integration tests on merge to main
  - [ ] Subtask 16.2: Create scheduled workflow for walk-forward validation:
    - GitHub Actions workflow: `walk-forward-validation.yaml`
    - Schedule: Monthly (1st of month)
    - Run walk-forward test on standard symbols (AAPL, MSFT, GOOGL, TSLA)
    - Alert if degradation detected in majority of windows
  - [ ] Subtask 16.3: Store walk-forward results in Git:
    - Save WalkForwardResult JSON to `backend/tests/walk_forward_results/`
    - Track trends over time (monthly snapshots)
  - [ ] Subtask 16.4: Document CI integration in project README

## Dev Notes

### Previous Story Context

**Story 12.1 (Custom Backtesting Engine Architecture)** created the foundation for this story:
- `BacktestEngine` class in `backend/src/backtesting/engine.py` [Source: docs/stories/epic-12/12.1.custom-backtesting-engine-architecture.md]
- `BacktestEngine.run_backtest(symbol, start_date, end_date, config) -> BacktestResult` method
- Pydantic models: `BacktestConfig`, `BacktestResult`, `BacktestMetrics`, `BacktestTrade`
- Event-driven backtesting with no look-ahead bias
- Performance target: 10,000 bars in <5 seconds

**Story 12.3 (Detector Accuracy Testing)** created accuracy testing infrastructure:
- AccuracyMetrics model for measuring detector precision/recall
- Regression detection logic (>5% performance drop)
- Baseline management for tracking performance over time
- These concepts apply to walk-forward degradation detection as well

**This story depends on Story 12.1 being complete** - BacktestEngine is the core dependency for running training and validation backtests.

### Epic 12 Context

This story is part of Epic 12: Backtesting & Validation Framework. It delivers the **walk-forward validation system** that ensures the trading system isn't overfit to specific historical periods. This is critical for:
- **Out-of-sample validation**: Proving the edge exists on unseen data
- **Overfitting detection**: Identifying if training performance doesn't generalize
- **Stability assessment**: Measuring consistency across different market regimes
- **Statistical rigor**: Quantifying confidence in performance differences

### Walk-Forward Testing Methodology

**What is Walk-Forward Testing?**
Walk-forward testing is a method of validating trading systems by dividing historical data into multiple rolling windows. Each window consists of:
1. **Training period**: Used to "train" or calibrate the system (though our system has fixed detection rules, not ML training)
2. **Validation period**: Out-of-sample period to test performance on unseen data

**Why Walk-Forward?**
- **Prevents overfitting**: If a system performs well on historical data but fails on out-of-sample data, it's likely overfit
- **Tests robustness**: Consistent performance across multiple validation periods indicates a robust edge
- **Market regime diversity**: Different windows capture different market conditions (bull, bear, sideways)

**Rolling Window Approach** (AC 1, 2):
```
Overall Period: 2020-2022 (train) + 2023-2024 (validate)

Window 1:
  Train:    [2020-01 ========== 2020-06]
  Validate:                              [2020-07 === 2020-09]

Window 2:
  Train:              [2020-04 ========== 2020-09]
  Validate:                                        [2020-10 === 2020-12]

Window 3:
  Train:                        [2020-07 ========== 2020-12]
  Validate:                                                  [2021-01 === 2021-03]

... continue rolling forward until end date
```

**Key Parameters**:
- `train_period_months`: 6 months (default) - length of training window
- `validate_period_months`: 3 months (default) - length of validation window
- Windows overlap in training periods but not in validation periods
- Roll forward by `validate_period_months` each iteration

### Performance Metrics Tracked (AC 4)

For each validation window, track these metrics:
1. **Win Rate**: Percentage of winning trades
2. **Avg R-Multiple**: Average risk-adjusted return per trade
3. **Profit Factor**: Gross profit / Gross loss
4. **Max Drawdown**: Largest peak-to-trough decline
5. **Sharpe Ratio**: Risk-adjusted return (optional)

These metrics are calculated for BOTH training and validation periods, allowing comparison.

### Degradation Detection (AC 5)

**Performance Ratio**:
- `performance_ratio = validate_metric / train_metric`
- Example: Train win rate 60%, Validate win rate 54% → ratio = 0.90 (90%)

**Degradation Threshold**:
- Default threshold: 80% (configurable via `WalkForwardConfig.degradation_threshold`)
- If `performance_ratio < 0.80`, degradation is detected
- Example: Train win rate 60%, Validate win rate 45% → ratio = 0.75 (75%) → DEGRADATION

**Interpretation**:
- Ratio ≥ 0.90 (90%): Excellent - validation very close to training
- Ratio 0.80-0.90 (80-90%): Good - acceptable degradation
- Ratio 0.60-0.80 (60-80%): Warning - significant degradation, review system
- Ratio < 0.60 (<60%): Critical - likely overfitting, do not deploy

**Alerting**:
- Log WARNING for each degraded window
- Track `degradation_windows` list (window numbers with degradation)
- Calculate `degradation_percentage`: (degraded_windows / total_windows) * 100
- System is considered robust if degradation occurs in <30% of windows

### Stability Assessment (AC 3)

**Stability Score (Coefficient of Variation)**:
- Measures consistency of validation performance across all windows
- `CV = standard_deviation(validate_metrics) / mean(validate_metrics)`
- Lower CV = more stable/consistent performance

**Interpretation**:
- CV < 0.20: Highly stable - performance very consistent across market regimes
- CV 0.20-0.40: Moderately stable - acceptable variance
- CV > 0.40: Unstable - performance highly variable, risky for live trading

**Example**:
- Validation win rates: [58%, 60%, 59%, 61%] → CV = 0.02 (very stable)
- Validation win rates: [65%, 45%, 70%, 50%] → CV = 0.19 (moderately stable)

### Statistical Significance (AC 8)

**Paired T-Test**:
- Compare train metrics vs validate metrics across all windows
- Null hypothesis: No difference between train and validate performance
- Use scipy.stats.ttest_rel() for paired t-test

**P-Value Interpretation**:
- p < 0.05: **Statistically significant difference** (WARNING)
  - Training performance significantly better than validation
  - Potential overfitting - system doesn't generalize well
- p ≥ 0.05: **No significant difference** (GOOD)
  - Training and validation performance similar
  - System generalizes well to out-of-sample data

**Multiple Metrics**:
- Calculate p-values for: win_rate, avg_r_multiple, profit_factor, sharpe_ratio
- If ALL metrics have p < 0.05, strong evidence of overfitting
- If SOME metrics have p < 0.05, investigate which aspects don't generalize

### Data Models and Schemas

**ValidationWindow Model** [Source: architecture/4-data-models.md]:
- Represents a single train/validate window pair
- Contains both training and validation BacktestMetrics
- Calculates performance_ratio and degradation_detected
- Links to underlying BacktestResult records via foreign keys

**WalkForwardConfig Model**:
- Configuration for walk-forward test execution
- Specifies date ranges, window sizes, symbols, degradation threshold
- Embeds BacktestConfig for underlying backtest parameters

**WalkForwardResult Model**:
- Complete result of walk-forward analysis
- Contains list of ValidationWindow objects
- Aggregates summary statistics across all windows
- Includes stability score and statistical significance tests

**Decimal Precision** [Source: architecture/15-coding-standards.md#Decimal-Precision]:
- All financial metrics use Decimal type (NOT float)
- Performance ratios, stability scores use Decimal with 4 decimal places
- Example: `performance_ratio: Decimal = Field(..., decimal_places=4, max_digits=6)`

### File Locations

**Source Code** [Source: architecture/10-unified-project-structure.md]:
- Walk-forward engine: `backend/src/backtesting/walk_forward_engine.py`
- Models: `backend/src/models/backtest.py` (add ValidationWindow, WalkForwardConfig, WalkForwardResult)
- API routes: `backend/src/api/routes/backtest.py` (add /walk-forward endpoints)
- Repository: `backend/src/repositories/walk_forward_repository.py`

**Testing** [Source: architecture/10-unified-project-structure.md]:
- Unit tests: `backend/tests/unit/test_walk_forward_engine.py`
- Integration tests: `backend/tests/integration/test_walk_forward_integration.py`
- Walk-forward results storage: `backend/tests/walk_forward_results/` (JSON snapshots)

**Scripts**:
- CLI tool: `backend/scripts/run_walk_forward.py`

**CI/CD** [Source: architecture/10-unified-project-structure.md]:
- Scheduled workflow: `.github/workflows/walk-forward-validation.yaml`

**Documentation**:
- Walk-forward guide: `backend/docs/walk-forward-testing.md`

**Database**:
- New table: `walk_forward_results` (via Alembic migration)

### Tech Stack

**Data Processing** [Source: architecture/3-tech-stack.md]:
- pandas 2.2+ for time-series window generation and date calculations
- numpy 1.26+ for statistical calculations (mean, std, CV)

**Statistical Testing** [Source: architecture/3-tech-stack.md]:
- scipy 1.11+ for paired t-test (scipy.stats.ttest_rel)
- Add to `backend/pyproject.toml` dependencies

**Data Validation** [Source: architecture/3-tech-stack.md]:
- Pydantic 2.5+ for ValidationWindow, WalkForwardConfig, WalkForwardResult models
- Decimal type for financial precision

**Testing** [Source: architecture/3-tech-stack.md]:
- pytest 8.0+ for unit and integration tests
- pytest-mock for mocking BacktestEngine
- factory-boy for test data generation

**Logging** [Source: architecture/3-tech-stack.md]:
- structlog 24.1+ for structured logging of walk-forward execution
- Log context: walk_forward_id, window_number, performance_ratio

### Coding Standards

**Naming Conventions** [Source: architecture/15-coding-standards.md#Naming-Conventions]:
- Classes: PascalCase (e.g., `WalkForwardEngine`, `ValidationWindow`)
- Functions: snake_case (e.g., `walk_forward_test`, `_generate_windows`)
- Files: snake_case (e.g., `walk_forward_engine.py`, `walk_forward_repository.py`)

**Type Safety** [Source: architecture/15-coding-standards.md]:
- Use strict type hints on all functions
- Use Decimal (NOT float) for performance metrics
- Validate all data against Pydantic models

**Decimal Precision** [Source: architecture/15-coding-standards.md#Decimal-Precision]:
- Use `Decimal` type for performance_ratio, stability_score
- Never use `float` for these calculations
- Example: `performance_ratio: Decimal = Field(..., decimal_places=4, max_digits=6)`

### Window Generation Logic (AC 2)

**Rolling Window Algorithm**:
```python
def _generate_windows(config: WalkForwardConfig) -> List[Tuple[date, date, date, date]]:
    windows = []
    current_train_start = config.overall_start_date

    while True:
        # Calculate train period
        train_end = current_train_start + relativedelta(months=config.train_period_months)

        # Calculate validate period
        validate_start = train_end + relativedelta(days=1)
        validate_end = validate_start + relativedelta(months=config.validate_period_months)

        # Check if we've exceeded overall end date
        if validate_end > config.overall_end_date:
            break

        # Add window
        windows.append((current_train_start, train_end, validate_start, validate_end))

        # Roll forward by validate_period_months
        current_train_start = current_train_start + relativedelta(months=config.validate_period_months)

    return windows
```

**Example Output** (6-month train, 3-month validate):
```
Window 1: Train 2020-01-01 to 2020-06-30, Validate 2020-07-01 to 2020-09-30
Window 2: Train 2020-04-01 to 2020-09-30, Validate 2020-10-01 to 2020-12-31
Window 3: Train 2020-07-01 to 2020-12-31, Validate 2021-01-01 to 2021-03-31
...
```

**Key Properties**:
- Training windows overlap (e.g., Window 1 and Window 2 share April-June 2020)
- Validation windows do NOT overlap (each validation period is unique out-of-sample data)
- Roll forward by validate_period_months to ensure non-overlapping validation

### Performance Considerations

**Execution Time**:
- Each window requires 2 backtests (train + validate)
- If 10 windows, that's 20 backtests
- Each backtest: 1-5 seconds (depending on data size)
- Total walk-forward time: 20-100 seconds for 10 windows

**Optimization Strategies**:
1. **Sequential execution** (initial implementation):
   - Simple, reliable
   - Run backtests one at a time
   - Track progress with logging
2. **Parallel execution** (future optimization):
   - Use asyncio or multiprocessing
   - Run independent windows in parallel
   - Trade-off: higher memory usage vs faster execution
   - Requires thread-safe BacktestEngine

**Caching**:
- Store BacktestResults in database
- If re-running walk-forward with same config, check if backtests already exist
- Reuse existing results instead of re-running
- Invalidate cache if code changes (version tracking)

### Testing Strategy

#### Unit Tests [Source: architecture/12-testing-strategy.md]
- Test window generation logic with various date ranges
- Test performance ratio calculations with known inputs
- Test degradation detection with boundary conditions
- Test stability score calculations with synthetic validation data
- Test statistical significance with mock data (known p-values)
- Mock BacktestEngine to return predictable results
- Location: `backend/tests/unit/test_walk_forward_engine.py`

#### Integration Tests
- Run full walk-forward test with real BacktestEngine
- Use real historical data for 2-3 symbols (AAPL, MSFT)
- Smaller date range for speed (e.g., 2020-2021 instead of 2020-2024)
- Verify window count, date boundaries, metrics calculations
- Mark as `@pytest.mark.slow` (can take 30+ seconds)
- Location: `backend/tests/integration/test_walk_forward_integration.py`

#### Test Coverage
- Aim for 90%+ coverage (NFR8)
- Cover all branches in window generation
- Cover all degradation detection paths
- Cover statistical significance calculations
- Cover edge cases: zero trades, missing data, single window

### Error Handling

**Insufficient Data**:
- If date range too short for at least 1 window, raise ValueError
- Message: "Date range too short for walk-forward test. Requires at least {train_months + validate_months} months."

**Missing OHLCV Data**:
- If BacktestEngine fails due to missing data, log warning and skip window
- Continue with next window (don't abort entire walk-forward)
- Report skipped windows in WalkForwardResult

**Zero Trades**:
- If backtest produces zero trades, metrics will be undefined
- Set metrics to None or Decimal("0")
- Flag window with "no_trades" indicator
- Exclude from statistical calculations (can't divide by zero)

**BacktestEngine Failures**:
- Wrap BacktestEngine.run_backtest() in try/except
- Log errors with window context (window number, symbol, date range)
- Continue with next window (graceful degradation)
- Track failed windows in WalkForwardResult

### Monitoring and Observability

**Structured Logging** [Source: architecture/3-tech-stack.md#structlog]:
```python
import structlog

logger = structlog.get_logger()

logger.info(
    "walk_forward_window_completed",
    walk_forward_id=str(walk_forward_id),
    window_number=window_num,
    symbol=symbol,
    train_win_rate=float(train_metrics.win_rate),
    validate_win_rate=float(validate_metrics.win_rate),
    performance_ratio=float(performance_ratio),
    degradation_detected=degradation
)
```

**Metrics to Track**:
- Total walk-forward execution time
- Average window execution time
- Number of degraded windows
- Stability score trends over time (monthly snapshots)
- P-values for statistical significance

**Alerts**:
- WARNING log for each degraded window
- CRITICAL log if >50% of windows show degradation (system likely overfit)
- INFO log for statistical significance results

### Dependencies and Risks

**Dependencies**:
- Story 12.1 (Custom Backtesting Engine) must be complete
- BacktestEngine.run_backtest() must be functional and performant
- Historical OHLCV data must be available for test periods (2020-2024)
- scipy library for statistical tests (add to dependencies)

**Risks**:
1. **Long execution time**: Walk-forward tests can take minutes to run
   - Mitigation: Run as BackgroundTask, provide progress updates, consider parallelization
2. **Data availability**: May not have data for all symbols/periods
   - Mitigation: Graceful degradation, skip missing windows, report data gaps
3. **Statistical interpretation**: Users may misinterpret p-values
   - Mitigation: Comprehensive documentation, clear guidelines in walk-forward guide
4. **Overfitting still possible**: Walk-forward doesn't guarantee no overfitting
   - Mitigation: Combine with other validation methods (paper trading, multiple symbols, labeled dataset)

### Notes for Developer

1. **Start with data models** - defines the data structure for everything else
2. **Implement window generation logic** - critical algorithm, test thoroughly
3. **Integrate with BacktestEngine** - reuse existing backtesting infrastructure
4. **Calculate performance ratios and degradation** - core validation logic
5. **Add statistical significance tests** - provides statistical rigor
6. **Calculate stability score** - measures consistency across windows
7. **Build API endpoints** - enables frontend integration
8. **Create CLI tool** - useful for local testing and debugging
9. **Write comprehensive tests** - unit tests for logic, integration tests for full flow
10. **Document extensively** - walk-forward testing is complex, needs clear explanations

**Key Implementation Order**:
1. Data models (ValidationWindow, WalkForwardConfig, WalkForwardResult)
2. Window generation algorithm (_generate_windows)
3. Core walk-forward logic (walk_forward_test method)
4. Performance ratio and degradation detection
5. Summary statistics and stability score
6. Statistical significance tests
7. Unit tests (mock BacktestEngine)
8. Integration tests (real BacktestEngine)
9. API endpoints and repository
10. CLI tool
11. Database migration
12. Documentation

**Testing as You Go**:
- Write unit tests alongside implementation (TDD approach)
- Test window generation with known date ranges
- Test performance calculations with synthetic metrics
- Mock BacktestEngine for fast unit tests
- Use real BacktestEngine for integration tests

**Degradation Analysis**:
- If degradation detected in >30% of windows, system may be overfit
- Investigate which patterns/symbols show most degradation
- Consider adjusting detection thresholds or confidence scores
- Re-run walk-forward after changes to verify improvement

**Statistical Significance**:
- p < 0.05 is a warning sign, not a failure
- Investigate WHY training differs from validation
- Market regime changes? Pattern frequency differences?
- Use insights to improve system robustness

## Testing

### Testing Framework [Source: architecture/12-testing-strategy.md]
- Use pytest 8.0+ for all backend unit and integration tests
- Test files mirror source structure
- Unit tests: `backend/tests/unit/test_walk_forward_engine.py`
- Integration tests: `backend/tests/integration/test_walk_forward_integration.py`

### Test Coverage Requirements [Source: architecture/12-testing-strategy.md, NFR8]
- Target 90%+ test coverage (NFR8)
- Cover all branches in window generation logic
- Cover all degradation detection paths (threshold boundaries)
- Cover stability score calculations
- Cover statistical significance calculations
- Cover error handling (missing data, zero trades, etc.)

### Unit Test Requirements

**Test Window Generation**:
- Config: 2020-01-01 to 2021-12-31, 6-month train, 3-month validate
- Verify: Correct number of windows (should be ~5-6 windows)
- Verify: Window dates sequential and non-overlapping (validation periods)
- Verify: Train periods overlap correctly
- Test edge case: Date range exactly fits 1 window
- Test edge case: Date range too short for 1 window (should raise error)

**Test Performance Ratio Calculation**:
- Train 60%, Validate 54% → Ratio 0.90 (90%)
- Train 60%, Validate 45% → Ratio 0.75 (75%)
- Train 0%, Validate 50% → Ratio 0.00 (edge case, division by zero)
- Train 50%, Validate 0% → Ratio 0.00 (edge case)

**Test Degradation Detection**:
- Ratio 0.85, Threshold 0.80 → False (no degradation)
- Ratio 0.75, Threshold 0.80 → True (degradation)
- Ratio 0.80, Threshold 0.80 → False (edge case, equal)
- Ratio 0.79, Threshold 0.80 → True (boundary case)

**Test Stability Score**:
- Validation win rates: [60%, 61%, 59%, 60%] → CV ≈ 0.01 (stable)
- Validation win rates: [60%, 45%, 70%, 50%] → CV ≈ 0.18 (moderately stable)
- Validation win rates: [70%, 30%, 80%, 20%] → CV > 0.40 (unstable)

**Test Statistical Significance**:
- Mock windows with train consistently > validate → p < 0.05
- Mock windows with train ≈ validate → p >= 0.05
- Verify scipy.stats.ttest_rel called correctly

**Mock BacktestEngine**:
```python
class MockBacktestEngine:
    def run_backtest(self, symbol, start_date, end_date, config):
        # Return predictable BacktestResult
        # Vary metrics based on date range (train vs validate)
        pass
```

### Integration Test Requirements

**Test Full Walk-Forward Execution**:
- Symbols: ["AAPL", "MSFT"]
- Date range: 2020-01-01 to 2021-12-31 (smaller for speed)
- Train: 6 months, Validate: 3 months
- Run with real BacktestEngine
- Verify:
  - Multiple ValidationWindow objects created
  - Each window has train_metrics and validate_metrics
  - Performance ratios calculated
  - Summary statistics populated
  - Stability score calculated
  - Statistical significance tests run

**Test Window Date Boundaries**:
- Verify first window starts at overall_start_date
- Verify last window ends at or before overall_end_date
- Verify no gaps between windows
- Verify validation periods don't overlap

**Performance Marking**:
```python
@pytest.mark.slow
def test_walk_forward_integration():
    # This test can take 30+ seconds
    pass
```

**Graceful Degradation**:
```python
@pytest.mark.skipif(not has_historical_data(), reason="Historical data not available")
def test_walk_forward_integration():
    pass
```

### Test Data and Fixtures

**Use factory-boy for Models**:
```python
import factory
from backend.src.models.backtest import ValidationWindow, WalkForwardConfig
from decimal import Decimal

class ValidationWindowFactory(factory.Factory):
    class Meta:
        model = ValidationWindow

    window_id = factory.Faker('uuid4')
    window_number = 1
    train_start_date = date(2020, 1, 1)
    train_end_date = date(2020, 6, 30)
    validate_start_date = date(2020, 7, 1)
    validate_end_date = date(2020, 9, 30)
    # ... etc
```

### Mocking Strategy [Source: architecture/3-tech-stack.md]
- Use pytest-mock for mocking BacktestEngine
- Mock BacktestEngine.run_backtest() to return predictable BacktestResults
- Vary metrics based on date range to simulate train vs validate differences
- Use factory-boy for generating synthetic BacktestMetrics

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-20 | 1.0 | Initial story creation with comprehensive walk-forward testing details | Bob (Scrum Master) |

## Dev Agent Record

_This section will be populated by the development agent during implementation._

### Agent Model Used

_To be filled by dev agent_

### Debug Log References

_To be filled by dev agent_

### Completion Notes

_To be filled by dev agent_

### File List

_To be filled by dev agent_

## QA Results

_This section will be populated by the QA agent after story completion._
