# Story 12.2: Labeled Pattern Dataset Creation

## Status
Ready for Development

## Sprint Assignment

**Sprint 1** - Foundation & Dataset (Parallel Work)

- Independent work stream (manual labeling)
- Required by: Story 12.3 (Accuracy Testing)
- Parallel with: Stories 12.1 & 12.5

## Story
**As a** validation engineer,
**I want** a labeled dataset of 200+ known patterns (springs, SOS, UTAD),
**so that** detector accuracy can be measured objectively (NFR18).

## Acceptance Criteria
1. Dataset format: Parquet file with columns: symbol, date, pattern_type, confidence, correctness
2. Data collection: manually label known patterns in historical data (AAPL, MSFT, GOOGL, TSLA)
3. Pattern types covered: Spring, SOS, UTAD, LPS, False Spring (high-volume breakdown)
4. Balanced dataset: equal representation of each pattern type
5. Metadata: includes actual outcome (did Jump target hit? Win/loss?)
6. Version control: stored in Git LFS (large file storage)
7. File: `tests/datasets/labeled_patterns_v1.parquet`
8. Documentation: CSV with pattern IDs and justification for labels
9. Validation: independent reviewer verifies 20% of labels for quality
10. API: load dataset with `pd.read_parquet('labeled_patterns_v1.parquet')`

## Tasks / Subtasks

- [ ] **Task 1: Setup Git LFS for Dataset Version Control** (AC: 6)
  - [ ] Subtask 1.1: Install and initialize Git LFS in the repository
  - [ ] Subtask 1.2: Configure `.gitattributes` to track `*.parquet` files with LFS
  - [ ] Subtask 1.3: Document Git LFS usage in project README

- [ ] **Task 2: Create Dataset Directory Structure** (AC: 7)
  - [ ] Subtask 2.1: Create `backend/tests/datasets/` directory
  - [ ] Subtask 2.2: Create placeholder for `labeled_patterns_v1.parquet`
  - [ ] Subtask 2.3: Ensure directory follows project structure guidelines from [Source: architecture/10-unified-project-structure.md#backtesting]

- [ ] **Task 3: Define Parquet Schema and Data Model** (AC: 1, 5)
  - [ ] Subtask 3.1: Create Pydantic model `LabeledPattern` in `backend/src/models/backtest.py` with fields:
    - `id`: UUID (unique pattern identifier)
    - `symbol`: str (ticker symbol)
    - `date`: datetime (UTC timestamp of pattern bar)
    - `pattern_type`: Literal["SPRING", "SOS", "UTAD", "LPS", "FALSE_SPRING"]
    - `confidence`: int (70-95 range, pattern detection confidence)
    - `correctness`: bool (ground truth: was this a valid pattern?)
    - `outcome_win`: bool (did Jump target get hit?)
    - `phase`: str (Wyckoff phase: A, B, C, D, E)
    - `trading_range_id`: str (associated range identifier)
    - `entry_price`: Decimal
    - `stop_loss`: Decimal
    - `target_price`: Decimal (Jump target)
    - `volume_ratio`: Decimal
    - `spread_ratio`: Decimal
    - `justification`: str (why this label was assigned)
    - `reviewer_verified`: bool (has this been independently verified?)
    - `created_at`: datetime
    - **Wyckoff Campaign Context Fields:**
    - `campaign_id`: UUID (links to parent Accumulation/Distribution campaign)
    - `campaign_type`: Literal["ACCUMULATION", "DISTRIBUTION"] (campaign context)
    - `campaign_phase`: Literal["A", "B", "C", "D", "E"] (specific phase within campaign)
    - `phase_position`: str (granular position: "early Phase C", "late Phase C", "mid Phase D")
    - `volume_characteristics`: Dict[str, Any] (climactic, diminishing, normal volume behavior)
    - `spread_characteristics`: Dict[str, Any] (narrowing, widening, normal spread behavior)
    - `sr_test_result`: str (support/resistance test result: "support held", "resistance broken", "failed test")
    - `preliminary_events`: List[str] (prerequisite events: ["PS", "SC", "AR"] leading to pattern)
    - `subsequent_confirmation`: bool (did expected confirmation occur? SOS after Spring, etc.)
    - `sequential_validity`: bool (does pattern follow correct Wyckoff sequence?)
    - `false_positive_reason`: Optional[str] (if correctness=False, why? "wrong phase", "no campaign", "failed prerequisites")
  - [ ] Subtask 3.2: Add validators to ensure UTC timestamps using same pattern as OHLCVBar [Source: architecture/4-data-models.md#OHLCVBar]
  - [ ] Subtask 3.3: Use Decimal type for financial precision [Source: architecture/15-coding-standards.md#Decimal-Precision]
  - [ ] Subtask 3.4: Create helper function `to_parquet_schema()` to convert Pydantic model to PyArrow schema

- [ ] **Task 4: Collect Historical Data for Target Symbols** (AC: 2)
  - [ ] Subtask 4.1: Fetch OHLCV data for AAPL, MSFT, GOOGL, TSLA (2020-2024, 1h and 1d timeframes)
  - [ ] Subtask 4.2: Calculate volume_ratio and spread_ratio for all bars using 20-bar averages [Source: architecture/4-data-models.md#OHLCVBar]
  - [ ] Subtask 4.3: Store data in temporary staging format for manual labeling

- [ ] **Task 5: Manual Pattern Labeling Workflow** (AC: 2, 3, 4, 5)
  - [ ] Subtask 5.1: Create labeling tool/script `scripts/label_patterns.py` that:
    - Displays OHLCV chart for visual inspection
    - Allows user to mark pattern type, date, and justification
    - Validates entries against Pydantic model
    - Saves incrementally to avoid data loss
  - [ ] Subtask 5.2: Label at least 40 instances of each pattern type:
    - 40+ Spring patterns in Phase C
    - 40+ SOS (Sign of Strength) patterns
    - 40+ UTAD (Upthrust After Distribution) patterns
    - 40+ LPS (Last Point of Support) patterns
    - 40+ False Spring (high-volume breakdown) patterns
  - [ ] Subtask 5.3: For each labeled pattern, record:
    - Pattern detection metadata (entry, stop, target)
    - Actual outcome (did target hit? win/loss?)
    - Phase context
    - Volume and spread ratios at pattern bar
    - Justification text explaining the label
  - [ ] Subtask 5.4: Ensure balanced representation across symbols (distribute patterns across AAPL, MSFT, GOOGL, TSLA)
  - [ ] Subtask 5.5: **Label Wyckoff Campaign Context for Each Pattern**:
    - Identify and assign campaign_id for parent Accumulation/Distribution
    - Mark campaign_type (ACCUMULATION or DISTRIBUTION)
    - Record campaign_phase (A, B, C, D, E) and phase_position
    - Document volume_characteristics (climactic, diminishing, normal)
    - Document spread_characteristics (narrowing, widening, normal)
    - Record sr_test_result for support/resistance tests
    - List preliminary_events (PS, SC, AR, etc.) leading to pattern
    - Mark subsequent_confirmation (was pattern confirmed by next event?)
    - Validate sequential_validity (correct Wyckoff sequence?)
  - [ ] Subtask 5.6: **Label Failure Cases with Detailed Reasons**:
    - Include patterns detected in wrong phase (e.g., Spring in Phase A)
    - Include patterns without valid campaign context
    - Include patterns missing prerequisite events (e.g., Spring without SC/AR)
    - Include patterns that failed confirmation (e.g., Spring without SOS)
    - For each failure case, document false_positive_reason
    - Target: 20% of dataset should be failure cases (40+ patterns)

- [ ] **Task 6: Export Dataset to Parquet Format** (AC: 1, 7, 10)
  - [ ] Subtask 6.1: Create export script using pandas and PyArrow:
    - Load labeled patterns from staging
    - Validate all entries against `LabeledPattern` Pydantic model
    - Convert to pandas DataFrame
    - Export to `backend/tests/datasets/labeled_patterns_v1.parquet`
  - [ ] Subtask 6.2: Verify Parquet file can be loaded with `pd.read_parquet('labeled_patterns_v1.parquet')`
  - [ ] Subtask 6.3: Validate dataset has required columns: symbol, date, pattern_type, confidence, correctness
  - [ ] Subtask 6.4: Commit Parquet file to Git LFS

- [ ] **Task 7: Create Documentation CSV** (AC: 8)
  - [ ] Subtask 7.1: Generate `backend/tests/datasets/labeled_patterns_v1_documentation.csv` with columns:
    - pattern_id (UUID)
    - symbol
    - date
    - pattern_type
    - justification (detailed explanation)
    - labeler (who created the label)
    - verification_status (verified/unverified)
  - [ ] Subtask 7.2: Include human-readable justification for each labeled pattern
  - [ ] Subtask 7.3: Commit documentation CSV to Git (not LFS)

- [ ] **Task 8: Independent Verification** (AC: 9)
  - [ ] Subtask 8.1: Create verification script `scripts/verify_labels.py` that:
    - Randomly selects 20% of labeled patterns
    - Displays pattern context for review
    - Allows reviewer to confirm/reject labels
    - Updates `reviewer_verified` field in dataset
  - [ ] Subtask 8.2: Document verification process and results
  - [ ] Subtask 8.3: Re-export Parquet file with verification status updates

- [ ] **Task 9: Create Dataset Loader Utility** (AC: 10)
  - [ ] Subtask 9.1: Create `backend/src/backtesting/dataset_loader.py` with function:
    ```python
    def load_labeled_patterns(version: str = "v1") -> pd.DataFrame:
        """Load labeled pattern dataset for accuracy testing."""
        path = f"tests/datasets/labeled_patterns_{version}.parquet"
        return pd.read_parquet(path)
    ```
  - [ ] Subtask 9.2: Add type hints and docstrings
  - [ ] Subtask 9.3: Handle file-not-found errors gracefully

- [ ] **Task 10: Unit Testing** (AC: 1, 10)
  - [ ] Subtask 10.1: Write test `backend/tests/unit/test_dataset_loader.py`:
    - Test that Parquet file loads successfully
    - Verify schema matches expected columns
    - Validate all pattern_types are in allowed set
    - Check confidence scores are in 70-95 range
    - Ensure dates are valid UTC timestamps
    - Verify dataset has at least 200 entries (balanced)
  - [ ] Subtask 10.2: Write test for Pydantic model validation
  - [ ] Subtask 10.3: Test that loading with `pd.read_parquet()` works as expected
  - [ ] Subtask 10.4: Follow pytest testing patterns [Source: architecture/12-testing-strategy.md#Backend-Testing]

## Dev Notes

### Previous Story Context
- **No previous story (12.1) exists yet.** This is the first story in Epic 12. Story 12.1 (Custom Backtesting Engine Architecture) should be completed before 12.2 in the ideal flow, but this story is standalone and can be implemented independently since it focuses on dataset creation, not backtesting execution.

### Epic 12 Context
This story is part of Epic 12: Backtesting & Validation Framework. The labeled dataset created here will be used in Story 12.3 (Detector Accuracy Testing) to measure precision/recall of pattern detectors against known ground truth. This is critical for meeting NFR18 (objective accuracy measurement).

### Data Models and Schema

**Pattern Types** [Source: architecture/4-data-models.md#Pattern]:
- Standard pattern types in the system: Spring, SOS, UTAD, LPS, SC, AR, ST
- For this dataset, focus on: Spring, SOS, UTAD, LPS, False Spring

**Pydantic Model Location** [Source: architecture/10-unified-project-structure.md]:
- Create new model in `backend/src/models/backtest.py`
- Follow existing pattern from `backend/src/models/pattern.py` and `backend/src/models/ohlcv.py`

**Decimal Precision** [Source: architecture/15-coding-standards.md#Decimal-Precision]:
- Use `Decimal` type for all financial values (entry_price, stop_loss, target_price)
- Never use `float` for prices

**Timestamp Handling** [Source: architecture/4-data-models.md#OHLCVBar]:
- All timestamps must be UTC
- Use Pydantic validator to enforce UTC timezone:
```python
@validator('date', 'created_at', pre=True)
def ensure_utc(cls, v):
    if v.tzinfo is None:
        return v.replace(tzinfo=timezone.utc)
    return v.astimezone(timezone.utc)
```

### File Locations

**Dataset Storage** [Source: architecture/10-unified-project-structure.md#backtesting]:
- Parquet file: `backend/tests/datasets/labeled_patterns_v1.parquet`
- Documentation CSV: `backend/tests/datasets/labeled_patterns_v1_documentation.csv`
- Loader utility: `backend/src/backtesting/dataset_loader.py`

**Testing** [Source: architecture/10-unified-project-structure.md]:
- Unit tests: `backend/tests/unit/test_dataset_loader.py`
- Fixtures location: `backend/tests/fixtures/` (will be created if needed)

### Tech Stack

**Data Processing** [Source: architecture/3-tech-stack.md]:
- pandas 2.2+ for OHLCV data manipulation and Parquet export
- numpy 1.26+ for numerical operations (volume/spread ratios)
- PyArrow for Parquet file format handling

**Data Validation** [Source: architecture/3-tech-stack.md]:
- Pydantic 2.5+ for data modeling and runtime validation
- All models must have proper type hints and validators

**Version Control** [Source: architecture/3-tech-stack.md]:
- Git LFS for large Parquet files (not currently in tech stack, needs to be added)
- Regular Git for documentation CSV and code

**Testing** [Source: architecture/3-tech-stack.md]:
- pytest 8.0+ for unit tests
- pytest-mock for mocking if needed
- factory-boy for test data generation (already in tech stack)

### Coding Standards

**Naming Conventions** [Source: architecture/15-coding-standards.md#Naming-Conventions]:
- Python Classes: PascalCase (e.g., `LabeledPattern`)
- Python Functions: snake_case (e.g., `load_labeled_patterns`)
- Files: snake_case (e.g., `dataset_loader.py`)

**Type Safety**:
- Use strict type hints on all functions
- Validate all data against Pydantic models before writing to Parquet

### Project Structure Notes

The dataset will be stored in `backend/tests/datasets/`, which is consistent with the project structure defined in [Source: architecture/10-unified-project-structure.md#backtesting]. The fixtures section mentions `backend/tests/fixtures/labeled_patterns.json`, but for this story we're creating a more robust Parquet-based dataset in a dedicated `datasets/` subdirectory.

### Git LFS Configuration

Git LFS is not explicitly mentioned in the architecture document, but AC 6 requires it. This story will need to:
1. Add Git LFS configuration to the repository
2. Configure `.gitattributes` to track `*.parquet` files
3. Document the setup for other developers

### Performance Considerations

- Parquet format provides columnar storage, which is efficient for analytics
- Dataset size: ~200-250 entries × ~15 columns = small file (<1MB likely)
- Loading should be near-instantaneous with pandas

### NFR Alignment

**NFR18 (Objective Accuracy Measurement)**:
This dataset is the foundation for meeting NFR18. By creating ground-truth labeled patterns, we can objectively measure detector precision, recall, and F1-scores in Story 12.3.

**NFR8 (90%+ Test Coverage)**:
The dataset itself will be tested with unit tests to ensure data integrity.

### Testing

#### Testing Framework [Source: architecture/12-testing-strategy.md]
- Use pytest 8.0+ for all backend unit tests
- Test files should mirror source structure: `backend/tests/unit/test_dataset_loader.py`

#### Test Coverage Requirements
- Unit tests must validate:
  - Parquet file loads correctly
  - Schema matches expected structure
  - All data types are correct (Decimal, datetime, etc.)
  - Pattern types are valid (in allowed set)
  - Confidence scores are in range [70, 95]
  - Dataset is balanced (each pattern type has similar count)
  - Dataset has minimum 200 entries

#### Test Data
- Use pytest fixtures for mock data if needed
- The actual labeled dataset will serve as integration test data for Story 12.3

### Dependencies and Risks

**Dependencies**:
- pandas, PyArrow (already in tech stack)
- Git LFS (needs to be added to project)

**Risks**:
1. **Manual labeling effort**: Labeling 200+ patterns is time-intensive
   - Mitigation: Create efficient labeling tool/script
2. **Subjectivity in labels**: Pattern identification can be subjective
   - Mitigation: Independent verification of 20% of labels (AC 9)
3. **Balanced dataset**: May be hard to find equal numbers of all pattern types
   - Mitigation: Use multiple symbols and longer time periods (2020-2024)

### Notes for Developer

1. **Start with Git LFS setup** before creating large files
2. **Create the Pydantic model first** to define the schema
3. **Build a simple labeling tool** - doesn't need to be fancy, just functional
4. **Label incrementally** and save progress frequently
5. **Verify data quality** continuously during labeling process
6. **Document justifications clearly** - this helps with verification and future debugging
7. **Consider using existing OHLCV data** if already in database, or fetch fresh from Polygon.io

### Wyckoff Campaign Context (CRITICAL)

The enhanced LabeledPattern model now includes **Wyckoff campaign context fields** to enable proper validation of pattern detection accuracy. This is essential because:

1. **Patterns are not isolated events** - They must occur within valid Accumulation or Distribution campaigns
2. **Phase matters** - A Spring in Phase A is invalid; it must occur in Phase C
3. **Sequential logic** - Patterns follow predictable sequences (PS → SC → AR → Spring → SOS → LPS)
4. **Confirmation required** - A Spring without subsequent SOS confirmation is a failed pattern

When labeling patterns:

- **Always identify the parent campaign** - Assign campaign_id and campaign_type
- **Validate phase correctness** - Spring in Phase C, SOS in Phase D, etc.
- **Record preliminary events** - What PS/SC/AR events preceded this pattern?
- **Track confirmation** - Did the expected next event occur (SOS after Spring)?
- **Document failure reasons** - For incorrect patterns, explain why (wrong phase, no campaign, etc.)

**Failure case examples to include**:

- Spring detected in Phase A (wrong phase)
- Spring without prior SC/AR (missing prerequisites)
- Spring without subsequent SOS (failed confirmation)
- UTAD detected during Accumulation (wrong campaign type)
- LPS without prior SOS (incorrect sequence)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-20 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

_This section will be populated by the development agent during implementation._

### Agent Model Used

_To be filled by dev agent_

### Debug Log References

_To be filled by dev agent_

### Completion Notes

_To be filled by dev agent_

### File List

_To be filled by dev agent_

## QA Results

_This section will be populated by the QA agent after story completion._
