# Story 9.2: Validation Metrics Dashboard & Analytics

**Epic:** 9 - Validation Analytics & Optimization
**Status:** Backlog
**Story Points:** 8
**Priority:** P2 - High Value
**Dependencies:** Story 8.2 (Multi-Stage Validation), Story 8.3.1 (Forex Volume Validation), Story 9.1 (UTAD Optimization)

---

## User Story

**As a** Wyckoff trading system operator,
**I want** a comprehensive validation metrics dashboard with real-time analytics,
**so that** I can monitor signal quality, identify optimization opportunities, and ensure system health across both stock and forex markets.

---

## Problem Statement

The multi-stage validation system (Stories 8.1-8.5) generates valuable operational data, but currently lacks:

1. **Real-Time Visibility** - No dashboard showing validation pass/fail rates
2. **Pattern-Specific Analytics** - Can't easily see which patterns have lowest pass rates
3. **Asset-Class Comparison** - Can't compare stock vs forex validation performance
4. **Anomaly Detection** - No automated alerts for unusual validation patterns
5. **Historical Trends** - Can't track validation quality over time

This story implements a comprehensive metrics dashboard to provide **operational visibility** and **data-driven optimization** capabilities.

---

## Background

### Current Validation Pipeline (Story 8.1-8.5)

```
Pattern Detected
    ↓
[VolumeValidator] → PASS/FAIL
    ↓
[PhaseValidator] → PASS/FAIL
    ↓
[LevelValidator] → PASS/FAIL
    ↓
TradeSignal Generated (if all PASS)
```

**What We Log (Story 8.3.1):**
- `volume_validation_passed` / `volume_validation_failed`
- `forex_utad_near_miss` (UTAD patterns at 200-250% volume)
- `forex_volume_spike_anomaly` (5x+ volume spikes)
- `forex_news_spike_detected` (patterns during NFP/FOMC)

**What We DON'T Have:**
- Aggregated pass/fail rates by pattern type
- Session-specific metrics (OVERLAP vs ASIAN performance)
- Alert system for anomalous validation patterns
- Comparison dashboards (stock vs forex, spring vs SOS)

---

## Acceptance Criteria

### Phase 1: Metrics Collection Infrastructure (AC: 1-4)

1. **Metrics Service** - Centralized service for collecting validation metrics
   - Real-time metrics aggregation (pass/fail counts, durations)
   - Time-series storage (15-minute buckets for charting)
   - Multi-dimensional grouping (pattern, asset_class, session, symbol)

2. **Metric Types** - Capture all validation stages
   - **Volume Validation:** pass_rate, avg_volume_ratio, anomaly_count, near_miss_count
   - **Phase Validation:** pass_rate, phase_mismatch_count (Phase C vs Phase D rejections)
   - **Level Validation:** pass_rate, penetration_score_avg, recovery_score_avg
   - **Overall Pipeline:** end-to-end_pass_rate, avg_validation_duration_ms

3. **Storage Backend** - Time-series database for efficient querying
   - Options: TimescaleDB (PostgreSQL extension), InfluxDB, or Prometheus
   - 90-day retention for raw metrics, 2-year retention for daily aggregates
   - Efficient range queries for dashboard charts

4. **Metrics API Endpoints** - REST API for dashboard consumption
   - `GET /api/metrics/validation/summary` - Overall stats (last 24h)
   - `GET /api/metrics/validation/trends` - Time-series data (last 7/30/90 days)
   - `GET /api/metrics/validation/breakdown` - Grouped by pattern/asset_class/session
   - `GET /api/metrics/validation/anomalies` - Recent anomalies (last 24h)

### Phase 2: Dashboard UI (AC: 5-8)

5. **Overview Dashboard** - High-level system health
   - **KPI Cards:** Overall pass rate (24h), total patterns validated (24h), avg validation time
   - **Validation Funnel:** Chart showing pass/fail at each stage (Volume → Phase → Level)
   - **Alert Banner:** Display active alerts (anomalies, threshold breaches)

6. **Pattern-Specific Dashboard** - Deep dive by pattern type
   - **Pass Rate by Pattern:** Bar chart (Spring: 75%, SOS: 82%, UTAD: 68%, LPS: 79%)
   - **Volume Distribution:** Histogram of volume ratios for each pattern
   - **Time-Series Chart:** 7-day trend of pass rates per pattern
   - **Top Rejection Reasons:** List of most common failure reasons

7. **Asset-Class Comparison Dashboard** - Stock vs Forex performance
   - **Pass Rate Comparison:** Side-by-side bar chart (Stock: 78%, Forex: 72%)
   - **Session Performance (Forex Only):** Pass rates by session (OVERLAP: 85%, ASIAN: 62%)
   - **Volume Threshold Effectiveness:** Chart showing near-miss counts by session
   - **Anomaly Comparison:** Forex spike detector effectiveness

8. **Symbol-Level Dashboard** - Per-symbol analysis
   - **Top Performers:** Symbols with highest pass rates (EUR/USD: 88%, AAPL: 82%)
   - **Low-Quality Symbols:** Symbols with <60% pass rates (requires investigation)
   - **Volume Characteristics:** Session-specific volume profiles per symbol
   - **Pattern Mix:** Distribution of pattern types per symbol

### Phase 3: Alerting System (AC: 9-11)

9. **Threshold-Based Alerts** - Automated notifications for anomalies
   - **Overall Pass Rate Drop:** Alert if pass rate drops below 65% (7-day rolling avg)
   - **Pattern-Specific Degradation:** Alert if any pattern drops below 60% pass rate
   - **Volume Spike Surge:** Alert if >5 volume spike anomalies in 1 hour (broker issue?)
   - **UTAD Near-Miss Surge:** Alert if >20 UTAD near-misses in 24h (threshold miscalibrated?)

10. **Alert Channels** - Multi-channel notification delivery
    - **In-App:** Dashboard banner (highest priority, always visible)
    - **Email:** Daily digest of alerts + critical alerts immediately
    - **Slack/Discord:** Integration for team notifications (optional)
    - **SMS:** Critical alerts only (system halt, max drawdown)

11. **Alert Management** - Configure and acknowledge alerts
    - **Alert Configuration UI:** Set thresholds per metric
    - **Acknowledgement:** Mark alerts as "reviewed" to clear dashboard
    - **Alert History:** View past alerts and resolutions

### Phase 4: Advanced Analytics (AC: 12-14)

12. **Correlation Analysis** - Identify patterns in validation failures
    - **Time-of-Day Analysis:** Do certain hours have lower pass rates? (market open volatility?)
    - **Session Correlation (Forex):** Are ASIAN session failures due to low liquidity?
    - **Symbol Clustering:** Group symbols by validation characteristics

13. **Optimization Recommendations** - Data-driven suggestions
    - **Threshold Recommendations:** "UTAD pass rate is 58%, consider lowering threshold to 230%"
    - **Session-Specific Tuning:** "OVERLAP session shows 15% near-miss rate, optimize threshold"
    - **Pattern Mix Insights:** "SOS patterns have 90% pass rate, prioritize SOS detection"

14. **Export & Reporting** - Data export for external analysis
    - **CSV Export:** Download metrics for any date range
    - **PDF Reports:** Weekly/monthly validation quality reports
    - **API Access:** Programmatic access for custom dashboards (Grafana, Tableau)

---

## Tasks / Subtasks

### Phase 1: Metrics Collection Infrastructure (12 hours)

- [ ] Design metrics data model - **2 hours**
  - [ ] Create file: `backend/src/models/metrics.py`
  - [ ] Define `ValidationMetric` Pydantic model:
    - `id: UUID` - Unique metric ID
    - `timestamp: datetime` - When metric recorded (UTC)
    - `metric_type: Literal["VOLUME", "PHASE", "LEVEL", "OVERALL"]` - Which validator
    - `pattern_type: Literal["SPRING", "SOS", "UTAD", "LPS"] | None` - Pattern type
    - `asset_class: Literal["STOCK", "FOREX", "CRYPTO"]` - Asset class
    - `symbol: str` - Symbol validated
    - `forex_session: ForexSession | None` - Forex session (if FOREX)
    - `result: Literal["PASS", "FAIL"]` - Validation result
    - `duration_ms: int` - Validation duration
    - `volume_ratio: Decimal | None` - Volume ratio (if VOLUME metric)
    - `rejection_reason: str | None` - Failure reason (if FAIL)
    - `metadata: dict[str, Any]` - Additional context
  - [ ] Define `ValidationMetricSummary` model (for API responses):
    - `time_bucket: datetime` - 15-minute bucket timestamp
    - `pattern_type: str | None` - Grouping dimension
    - `asset_class: str | None` - Grouping dimension
    - `session: str | None` - Grouping dimension
    - `total_validations: int` - Count in bucket
    - `pass_count: int` - Passed validations
    - `fail_count: int` - Failed validations
    - `pass_rate: float` - pass_count / total_validations
    - `avg_duration_ms: float` - Average validation time
    - `avg_volume_ratio: float | None` - Average volume ratio

- [ ] Implement MetricsService - **4 hours**
  - [ ] Create file: `backend/src/services/metrics_service.py`
  - [ ] Implement `class MetricsService`:
    - `async def record_validation_metric(metric: ValidationMetric) -> None`:
      - Insert metric into time-series database
      - Update in-memory aggregates (for fast dashboard queries)
      - Trigger alert checks if configured
    - `async def get_summary(start: datetime, end: datetime, filters: dict) -> ValidationMetricSummary`:
      - Query time-series database with date range
      - Apply filters (pattern_type, asset_class, session)
      - Return aggregated summary
    - `async def get_time_series(start: datetime, end: datetime, bucket_size: str, filters: dict) -> List[ValidationMetricSummary]`:
      - Query with bucketing (15min, 1hour, 1day)
      - Return list of time-bucketed summaries for charting
    - `async def get_breakdown(dimension: str, start: datetime, end: datetime) -> List[ValidationMetricSummary]`:
      - Group by dimension (pattern_type, asset_class, session, symbol)
      - Return list of summaries per group
    - `async def get_anomalies(start: datetime, end: datetime) -> List[ValidationMetric]`:
      - Query for metrics flagged as anomalies (e.g., volume spikes, low pass rates)
      - Return list of anomalous events

- [ ] Integrate metrics recording into validators - **3 hours**
  - [ ] Update `backend/src/signal_generator/validators/volume_validator.py`:
    - Add `metrics_service` dependency injection
    - After validation completes, call:
      ```python
      await self.metrics_service.record_validation_metric(
          ValidationMetric(
              timestamp=datetime.now(timezone.utc),
              metric_type="VOLUME",
              pattern_type=context.pattern.pattern_type,
              asset_class=context.asset_class,
              symbol=context.symbol,
              forex_session=context.forex_session,
              result="PASS" if result.status == ValidationStatus.PASS else "FAIL",
              duration_ms=validation_duration_ms,
              volume_ratio=context.volume_analysis.volume_ratio,
              rejection_reason=result.reason if result.status == ValidationStatus.FAIL else None,
              metadata=result.metadata or {},
          )
      )
      ```
  - [ ] Repeat for `phase_validator.py` and `level_validator.py`
  - [ ] Add duration tracking (start timer before validation, stop after)

- [ ] Set up time-series database - **3 hours**
  - [ ] Choose storage backend: **TimescaleDB** (PostgreSQL extension, familiar to team)
  - [ ] Create migration: `backend/alembic/versions/0XX_create_validation_metrics_table.py`
  - [ ] Create `validation_metrics` hypertable:
    ```sql
    CREATE TABLE validation_metrics (
        id UUID PRIMARY KEY,
        timestamp TIMESTAMPTZ NOT NULL,
        metric_type VARCHAR(20) NOT NULL,
        pattern_type VARCHAR(10),
        asset_class VARCHAR(10) NOT NULL,
        symbol VARCHAR(20) NOT NULL,
        forex_session VARCHAR(10),
        result VARCHAR(4) NOT NULL,
        duration_ms INTEGER NOT NULL,
        volume_ratio DECIMAL(10, 4),
        rejection_reason TEXT,
        metadata JSONB
    );

    SELECT create_hypertable('validation_metrics', 'timestamp');
    ```
  - [ ] Create indexes:
    ```sql
    CREATE INDEX idx_metrics_pattern_time ON validation_metrics (pattern_type, timestamp DESC);
    CREATE INDEX idx_metrics_asset_time ON validation_metrics (asset_class, timestamp DESC);
    CREATE INDEX idx_metrics_symbol_time ON validation_metrics (symbol, timestamp DESC);
    CREATE INDEX idx_metrics_session_time ON validation_metrics (forex_session, timestamp DESC);
    ```
  - [ ] Create retention policy:
    ```sql
    SELECT add_retention_policy('validation_metrics', INTERVAL '90 days');
    ```

### Phase 2: Metrics API (8 hours)

- [ ] Implement API endpoints - **5 hours**
  - [ ] Create file: `backend/src/api/routes/metrics.py`
  - [ ] Implement `GET /api/metrics/validation/summary`:
    - Query params: `start_date`, `end_date`, `pattern_type`, `asset_class`, `session`
    - Response:
      ```json
      {
        "total_validations": 1523,
        "pass_count": 1187,
        "fail_count": 336,
        "pass_rate": 0.779,
        "avg_duration_ms": 45.3,
        "avg_volume_ratio": 1.85
      }
      ```
  - [ ] Implement `GET /api/metrics/validation/trends`:
    - Query params: `start_date`, `end_date`, `bucket_size` (15min/1hour/1day), `filters`
    - Response:
      ```json
      {
        "buckets": [
          {"timestamp": "2025-03-01T00:00:00Z", "pass_rate": 0.78, "total": 42},
          {"timestamp": "2025-03-01T01:00:00Z", "pass_rate": 0.82, "total": 38},
          ...
        ]
      }
      ```
  - [ ] Implement `GET /api/metrics/validation/breakdown`:
    - Query params: `dimension` (pattern/asset_class/session/symbol), `start_date`, `end_date`
    - Response:
      ```json
      {
        "dimension": "pattern_type",
        "groups": [
          {"group_value": "SPRING", "pass_rate": 0.75, "total": 452},
          {"group_value": "SOS", "pass_rate": 0.82, "total": 389},
          {"group_value": "UTAD", "pass_rate": 0.68, "total": 298},
          {"group_value": "LPS", "pass_rate": 0.79, "total": 384}
        ]
      }
      ```
  - [ ] Implement `GET /api/metrics/validation/anomalies`:
    - Query params: `start_date`, `end_date`, `anomaly_types`
    - Response:
      ```json
      {
        "anomalies": [
          {
            "timestamp": "2025-03-01T14:23:45Z",
            "type": "volume_spike",
            "symbol": "EUR/USD",
            "volume_ratio": 5.8,
            "threshold": 5.0
          },
          ...
        ]
      }
      ```

- [ ] Add API authentication and rate limiting - **1 hour**
  - [ ] Require API key for metrics endpoints (sensitive operational data)
  - [ ] Rate limit: 100 requests/minute per API key
  - [ ] Add to API documentation

- [ ] Write API tests - **2 hours**
  - [ ] Create file: `backend/tests/integration/api/test_metrics_endpoints.py`
  - [ ] Test: Summary endpoint returns correct aggregates
  - [ ] Test: Trends endpoint returns time-bucketed data
  - [ ] Test: Breakdown endpoint groups by dimension correctly
  - [ ] Test: Anomalies endpoint filters by type

### Phase 3: Dashboard UI (16 hours)

- [ ] Set up frontend dashboard framework - **2 hours**
  - [ ] Choose framework: **React + Recharts** (lightweight charting library)
  - [ ] Create: `frontend/src/pages/ValidationDashboard.tsx`
  - [ ] Install dependencies: `npm install recharts date-fns`
  - [ ] Create layout with sidebar navigation:
    - Overview
    - By Pattern
    - By Asset Class
    - By Symbol
    - Alerts

- [ ] Build Overview Dashboard - **3 hours**
  - [ ] Create component: `frontend/src/components/dashboard/OverviewDashboard.tsx`
  - [ ] Implement KPI cards (4 cards across top):
    - **Overall Pass Rate** (24h): Large percentage with trend arrow (↑ 2.3% vs yesterday)
    - **Total Validations** (24h): Count with sparkline chart
    - **Avg Validation Time**: Milliseconds with performance indicator (green if <50ms)
    - **Active Alerts**: Count with red badge if >0
  - [ ] Implement validation funnel chart:
    - Use Recharts `<BarChart>` with stacked bars
    - X-axis: Volume → Phase → Level
    - Y-axis: Count
    - Colors: Green (PASS), Red (FAIL)
  - [ ] Implement alert banner (top of dashboard):
    - Display if any active alerts
    - Show alert type, timestamp, message
    - "Acknowledge" button to dismiss

- [ ] Build Pattern-Specific Dashboard - **4 hours**
  - [ ] Create component: `frontend/src/components/dashboard/PatternDashboard.tsx`
  - [ ] Implement pass rate by pattern bar chart:
    - Use Recharts `<BarChart>`
    - X-axis: SPRING, SOS, UTAD, LPS
    - Y-axis: Pass rate (0-100%)
    - Color code: Green (>75%), Yellow (65-75%), Red (<65%)
  - [ ] Implement volume distribution histogram:
    - Use Recharts `<AreaChart>`
    - X-axis: Volume ratio bins (0-0.5x, 0.5-1.0x, 1.0-1.5x, etc.)
    - Y-axis: Count of patterns
    - Separate line per pattern type
  - [ ] Implement 7-day trend chart:
    - Use Recharts `<LineChart>`
    - X-axis: Date (last 7 days)
    - Y-axis: Pass rate
    - 4 lines (one per pattern type)
  - [ ] Implement rejection reasons table:
    - Material-UI `<Table>`
    - Columns: Reason, Count, Percentage
    - Sortable by count

- [ ] Build Asset-Class Comparison Dashboard - **3 hours**
  - [ ] Create component: `frontend/src/components/dashboard/AssetClassDashboard.tsx`
  - [ ] Implement pass rate comparison bar chart:
    - Side-by-side bars (Stock vs Forex vs Crypto)
    - Show overall pass rate + breakdown by pattern
  - [ ] Implement session performance chart (Forex only):
    - Use Recharts `<BarChart>`
    - X-axis: ASIAN, LONDON, NY, OVERLAP
    - Y-axis: Pass rate
    - Include count labels on bars
  - [ ] Implement near-miss heatmap:
    - Use Material-UI `<Grid>` with color-coded cells
    - Rows: Sessions, Columns: Pattern types
    - Cell color intensity = near-miss count (lighter → darker)

- [ ] Build Symbol-Level Dashboard - **2 hours**
  - [ ] Create component: `frontend/src/components/dashboard/SymbolDashboard.tsx`
  - [ ] Implement top performers table:
    - Material-UI `<Table>`
    - Columns: Symbol, Pass Rate, Total Validations, Primary Pattern
    - Sortable by pass rate
    - Top 10 symbols
  - [ ] Implement low-quality symbols table:
    - Same structure as top performers
    - Filter: pass_rate < 60%
    - Flag for investigation
  - [ ] Implement symbol search:
    - Autocomplete input to search for specific symbol
    - Show detailed stats for selected symbol

- [ ] Implement real-time updates (WebSocket) - **2 hours**
  - [ ] Create WebSocket endpoint: `ws://api/metrics/validation/live`
  - [ ] Stream validation events in real-time
  - [ ] Update dashboard charts every 15 seconds
  - [ ] Flash notification for new alerts

### Phase 4: Alerting System (8 hours)

- [ ] Implement alert evaluation engine - **3 hours**
  - [ ] Create file: `backend/src/services/alert_service.py`
  - [ ] Define `AlertRule` Pydantic model:
    - `id: UUID` - Alert rule ID
    - `name: str` - Human-readable name
    - `metric_type: str` - What to monitor (overall_pass_rate, pattern_pass_rate, etc.)
    - `condition: str` - Comparison operator (less_than, greater_than, equals)
    - `threshold: float` - Trigger value
    - `window: str` - Time window (1hour, 24hours, 7days)
    - `severity: Literal["INFO", "WARNING", "CRITICAL"]` - Alert severity
    - `channels: List[str]` - Where to send (in_app, email, slack, sms)
    - `enabled: bool` - Whether rule is active
  - [ ] Implement `async def evaluate_alert_rules() -> List[Alert]`:
    - Query metrics for each rule's time window
    - Check if condition met (e.g., pass_rate < 0.65)
    - If triggered: Create `Alert` object
    - Deduplicate: Don't re-trigger same alert within cooldown period (1 hour)
  - [ ] Run evaluation every 5 minutes via background task

- [ ] Implement notification channels - **3 hours**
  - [ ] Create file: `backend/src/services/notification_service.py`
  - [ ] Implement `async def send_alert(alert: Alert, channels: List[str]) -> None`:
    - Route to appropriate channel handlers
  - [ ] Implement in-app notifications:
    - Store in `alerts` database table
    - Expose via `GET /api/alerts` endpoint
    - Dashboard polls endpoint every 30 seconds
  - [ ] Implement email notifications:
    - Use SendGrid or AWS SES
    - Email template with alert details
    - Include link to dashboard
  - [ ] Implement Slack notifications (optional):
    - Webhook integration
    - Post to #validation-alerts channel
    - Format with color-coded severity

- [ ] Build alert management UI - **2 hours**
  - [ ] Create component: `frontend/src/components/dashboard/AlertsPanel.tsx`
  - [ ] Display active alerts:
    - Material-UI `<List>` with `<ListItem>` per alert
    - Show: severity icon, message, timestamp
    - "Acknowledge" button to dismiss
  - [ ] Display alert history:
    - Table with filters (date range, severity, type)
    - Paginated (50 per page)
  - [ ] Implement alert rule configuration UI:
    - Form to create/edit alert rules
    - Toggle to enable/disable rules
    - Test button to preview alert

### Phase 5: Advanced Analytics (8 hours)

- [ ] Implement correlation analysis - **3 hours**
  - [ ] Create file: `backend/src/services/analytics_service.py`
  - [ ] Implement `async def analyze_time_of_day_patterns() -> Dict[int, float]`:
    - Group metrics by hour of day (UTC)
    - Calculate pass rate per hour
    - Return dict: {0: 0.78, 1: 0.82, ..., 23: 0.75}
  - [ ] Implement `async def analyze_session_correlations() -> pd.DataFrame`:
    - Analyze forex session performance
    - Calculate correlation between session and pass rate
    - Identify if certain sessions consistently underperform
  - [ ] Implement `async def cluster_symbols_by_validation_characteristics() -> List[List[str]]`:
    - Use k-means clustering on:
      - Pass rate
      - Avg volume ratio
      - Pattern mix (% SPRING vs SOS vs UTAD vs LPS)
    - Return clusters: [[EUR/USD, GBP/USD], [USD/CHF, USD/JPY], ...]

- [ ] Implement optimization recommendations - **3 hours**
  - [ ] Implement `async def generate_threshold_recommendations() -> List[Recommendation]`:
    - For each pattern type:
      - If pass_rate < 65% and near_miss_count > 20: Suggest lowering threshold
      - If pass_rate > 90% and near_miss_count < 5: Suggest raising threshold (too loose)
    - Return list of `Recommendation` objects with:
      - `pattern_type: str`
      - `current_threshold: Decimal`
      - `suggested_threshold: Decimal`
      - `rationale: str`
      - `expected_impact: str` (e.g., "+12% signal generation")
  - [ ] Implement `async def generate_session_tuning_recommendations() -> List[Recommendation]`:
    - Analyze session-specific near-miss rates
    - Suggest session-specific threshold adjustments
  - [ ] Display recommendations in dashboard:
    - Create component: `frontend/src/components/dashboard/RecommendationsPanel.tsx`
    - Show as cards with "Apply" button (creates Story 9.1-style optimization task)

- [ ] Implement export and reporting - **2 hours**
  - [ ] Implement `GET /api/metrics/validation/export`:
    - Query params: `start_date`, `end_date`, `format` (csv/json)
    - Generate CSV with all metrics in date range
    - Stream response (don't load entire dataset in memory)
  - [ ] Implement PDF report generation:
    - Use library: `pdfkit` or `reportlab`
    - Generate weekly/monthly report with:
      - Executive summary (overall pass rate, key trends)
      - Charts (pass rate trends, pattern breakdown)
      - Top issues (highest rejection reasons)
      - Recommendations
    - Save to S3 or local storage
    - Email to stakeholders
  - [ ] Create scheduled task:
    - Generate weekly report every Monday at 8am UTC
    - Generate monthly report on 1st of month

---

## Technical Design

### System Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     Validation Pipeline                      │
│  (VolumeValidator, PhaseValidator, LevelValidator)          │
└──────────────────────┬──────────────────────────────────────┘
                       │ record_metric()
                       ↓
┌─────────────────────────────────────────────────────────────┐
│                      MetricsService                          │
│  - Record validation metrics                                 │
│  - Aggregate in-memory (fast dashboard queries)             │
│  - Trigger alert evaluation                                  │
└──────────────────────┬──────────────────────────────────────┘
                       │ insert
                       ↓
┌─────────────────────────────────────────────────────────────┐
│                  TimescaleDB (PostgreSQL)                    │
│  - validation_metrics hypertable (90-day retention)         │
│  - Indexed by: pattern, asset_class, session, symbol        │
└──────────────────────┬──────────────────────────────────────┘
                       │ query
                       ↓
┌─────────────────────────────────────────────────────────────┐
│                      Metrics API                             │
│  GET /api/metrics/validation/summary                        │
│  GET /api/metrics/validation/trends                         │
│  GET /api/metrics/validation/breakdown                      │
│  GET /api/metrics/validation/anomalies                      │
└──────────────────────┬──────────────────────────────────────┘
                       │ fetch
                       ↓
┌─────────────────────────────────────────────────────────────┐
│                   Dashboard UI (React)                       │
│  - OverviewDashboard                                        │
│  - PatternDashboard                                         │
│  - AssetClassDashboard                                      │
│  - SymbolDashboard                                          │
│  - AlertsPanel                                              │
│  - RecommendationsPanel                                     │
└─────────────────────────────────────────────────────────────┘

                       Alerting Flow

┌─────────────────────────────────────────────────────────────┐
│                    AlertService                              │
│  - Evaluate alert rules every 5 minutes                     │
│  - Check thresholds (pass_rate < 65%, etc.)                │
│  - Deduplicate alerts (1-hour cooldown)                    │
└──────────────────────┬──────────────────────────────────────┘
                       │ send_alert()
                       ↓
┌─────────────────────────────────────────────────────────────┐
│                NotificationService                           │
│  - In-app: Store in database, poll from dashboard          │
│  - Email: SendGrid/SES with template                        │
│  - Slack: Webhook to #validation-alerts                     │
│  - SMS: Twilio (critical alerts only)                       │
└─────────────────────────────────────────────────────────────┘
```

### Database Schema

**validation_metrics (TimescaleDB hypertable):**

```sql
CREATE TABLE validation_metrics (
    id UUID PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    metric_type VARCHAR(20) NOT NULL,      -- VOLUME, PHASE, LEVEL, OVERALL
    pattern_type VARCHAR(10),               -- SPRING, SOS, UTAD, LPS
    asset_class VARCHAR(10) NOT NULL,       -- STOCK, FOREX, CRYPTO
    symbol VARCHAR(20) NOT NULL,
    forex_session VARCHAR(10),              -- ASIAN, LONDON, NY, OVERLAP
    result VARCHAR(4) NOT NULL,             -- PASS, FAIL
    duration_ms INTEGER NOT NULL,
    volume_ratio DECIMAL(10, 4),
    rejection_reason TEXT,
    metadata JSONB
);

SELECT create_hypertable('validation_metrics', 'timestamp');
```

**alerts table:**

```sql
CREATE TABLE alerts (
    id UUID PRIMARY KEY,
    rule_id UUID NOT NULL,
    triggered_at TIMESTAMPTZ NOT NULL,
    severity VARCHAR(20) NOT NULL,          -- INFO, WARNING, CRITICAL
    message TEXT NOT NULL,
    metric_type VARCHAR(20),
    threshold_value DECIMAL(10, 4),
    actual_value DECIMAL(10, 4),
    acknowledged BOOLEAN DEFAULT FALSE,
    acknowledged_at TIMESTAMPTZ,
    metadata JSONB
);
```

**alert_rules table:**

```sql
CREATE TABLE alert_rules (
    id UUID PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    metric_type VARCHAR(50) NOT NULL,
    condition VARCHAR(20) NOT NULL,         -- less_than, greater_than
    threshold DECIMAL(10, 4) NOT NULL,
    window VARCHAR(20) NOT NULL,            -- 1hour, 24hours, 7days
    severity VARCHAR(20) NOT NULL,
    channels TEXT[] NOT NULL,               -- {in_app, email, slack}
    enabled BOOLEAN DEFAULT TRUE,
    cooldown_minutes INTEGER DEFAULT 60,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

---

## Testing Strategy

### Unit Tests (6 hours)

**File:** `backend/tests/unit/services/test_metrics_service.py`

```python
@pytest.mark.asyncio
async def test_record_validation_metric():
    """Test: Metric is recorded and stored in database."""
    metric = ValidationMetric(
        timestamp=datetime.now(timezone.utc),
        metric_type="VOLUME",
        pattern_type="SPRING",
        asset_class="FOREX",
        symbol="EUR/USD",
        forex_session=ForexSession.OVERLAP,
        result="PASS",
        duration_ms=42,
        volume_ratio=Decimal("0.75"),
    )

    await metrics_service.record_validation_metric(metric)

    # Verify metric stored in database
    stored = await db.query(ValidationMetric).filter_by(id=metric.id).first()
    assert stored is not None
    assert stored.symbol == "EUR/USD"
```

**File:** `backend/tests/unit/services/test_alert_service.py`

```python
@pytest.mark.asyncio
async def test_evaluate_alert_rule_triggers():
    """Test: Alert triggers when threshold breached."""
    # Create rule: Alert if pass rate < 65% in last 24h
    rule = AlertRule(
        name="Low Pass Rate",
        metric_type="overall_pass_rate",
        condition="less_than",
        threshold=0.65,
        window="24hours",
        severity="WARNING",
    )

    # Create metrics with 60% pass rate
    for i in range(100):
        await metrics_service.record_validation_metric(
            create_test_metric(result="PASS" if i < 60 else "FAIL")
        )

    # Evaluate rule
    alerts = await alert_service.evaluate_alert_rules()

    assert len(alerts) == 1
    assert alerts[0].message == "Overall pass rate (60.0%) below threshold (65.0%)"
```

### Integration Tests (4 hours)

**File:** `backend/tests/integration/api/test_metrics_endpoints.py`

```python
@pytest.mark.asyncio
async def test_metrics_summary_endpoint():
    """Test: Summary endpoint returns correct aggregates."""
    # Create 100 test metrics (75 PASS, 25 FAIL)
    for i in range(100):
        await create_test_metric(result="PASS" if i < 75 else "FAIL")

    response = await client.get("/api/metrics/validation/summary?start_date=2025-03-01")

    assert response.status_code == 200
    data = response.json()
    assert data["total_validations"] == 100
    assert data["pass_rate"] == 0.75
```

**File:** `frontend/src/components/dashboard/__tests__/OverviewDashboard.test.tsx`

```tsx
test("renders KPI cards with correct values", async () => {
  // Mock API response
  mockAPI("/api/metrics/validation/summary", {
    pass_rate: 0.78,
    total_validations: 1523,
    avg_duration_ms: 45.3,
  });

  render(<OverviewDashboard />);

  await waitFor(() => {
    expect(screen.getByText("78%")).toBeInTheDocument();
    expect(screen.getByText("1,523")).toBeInTheDocument();
    expect(screen.getByText("45.3ms")).toBeInTheDocument();
  });
});
```

---

## Success Metrics

### Primary Metrics (Must Achieve)
1. **Dashboard Load Time:** <2 seconds for all views
2. **Real-Time Updates:** Metrics update within 30 seconds of validation
3. **Alert Latency:** Alerts trigger within 5 minutes of threshold breach
4. **API Response Time:** 95th percentile <200ms for all endpoints

### Secondary Metrics (Nice-to-Have)
5. **User Engagement:** Dashboard accessed ≥5 times/day by operators
6. **Alert Actionability:** ≥80% of alerts lead to optimization actions
7. **Optimization Impact:** Recommendations lead to ≥10% improvement in pass rates

---

## Related Stories

- **Story 8.1** - Master Orchestrator (provides validation pipeline)
- **Story 8.2** - Multi-Stage Validation (provides metrics to track)
- **Story 8.3.1** - Forex Volume Validation (provides forex-specific metrics)
- **Story 9.1** - UTAD Optimization (consumes near-miss analytics from dashboard)

---

## Completion Criteria

- [ ] All acceptance criteria (1-14) met
- [ ] All tasks completed (52 hours estimated)
- [ ] Unit tests passing (100% coverage for services)
- [ ] Integration tests passing (API + UI)
- [ ] Dashboard accessible at `/validation-dashboard` route
- [ ] Metrics API fully documented in OpenAPI spec
- [ ] Alert system tested with production-like data
- [ ] Performance benchmarks met (dashboard <2s load, API <200ms)
- [ ] User acceptance testing complete (operators can navigate dashboard)
- [ ] Monitoring infrastructure deployed (logs, error tracking)

---

**Story Status:** Ready for Planning
**Next Action:** Schedule design review for dashboard UI mockups
