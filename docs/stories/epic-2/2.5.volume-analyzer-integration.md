# Story 2.5: VolumeAnalyzer Module Integration

## Status
Done
Ready for Review

## Story

**As a** developer,
**I want** a unified VolumeAnalyzer class that produces complete volume analysis for bar sequences,
**so that** pattern detectors have a single interface for all volume-related calculations.

## Acceptance Criteria

1. Class created: `VolumeAnalyzer` with method `analyze(bars: List[OHLCVBar]) -> List[VolumeAnalysis]`
2. VolumeAnalysis dataclass contains: bar, volume_ratio, spread_ratio, close_position, effort_result
3. Batch processing: analyze 252 bars in single call, returns 252 VolumeAnalysis objects
4. First 20 bars return VolumeAnalysis with None values (insufficient data)
5. Caching (optional): cache analysis results to avoid recalculation for historical bars
6. Logging: debug logs for abnormal conditions (volume >5.0x, spread >3.0x)
7. Unit test: end-to-end analysis of synthetic bar sequence validates all fields populated
8. Integration test: analyze real AAPL data (252 bars), verify reasonable distributions
9. Performance test: analyze 10,000 bars in <500ms
10. API compatibility: analysis results serializable to JSON for API responses

## Tasks / Subtasks

- [ ] **Task 1: Create VolumeAnalyzer class structure** (AC: 1)
  - [ ] Create class in existing `backend/src/pattern_engine/volume_analyzer.py` module
  - [ ] Define class: `class VolumeAnalyzer:`
  - [ ] Add `__init__` method (no parameters needed for stateless analyzer)
  - [ ] Add docstring explaining class purpose and usage
  - [ ] Import all required dependencies: List, Optional, Decimal, numpy, pandas
  - [ ] Import VolumeAnalysis model and EffortResult enum

- [ ] **Task 2: Implement `analyze` method signature and structure** (AC: 1, 2, 3)
  - [ ] Create method: `def analyze(self, bars: List[OHLCVBar]) -> List[VolumeAnalysis]:`
  - [ ] Add type hints for parameters and return value
  - [ ] Add docstring with parameter descriptions, return value, and usage examples
  - [ ] Validate input: bars parameter is not empty list
  - [ ] Initialize result list: `results: List[VolumeAnalysis] = []`
  - [ ] Return results at end of method

- [ ] **Task 3: Integrate volume_ratio calculation (Story 2.1)** (AC: 2, 3, 4)
  - [ ] Import or use existing `calculate_volume_ratio` function
  - [ ] For each bar in sequence, calculate volume_ratio
  - [ ] Handle first 20 bars: volume_ratio will be None per Story 2.1
  - [ ] Use vectorized NumPy implementation for performance
  - [ ] Store volume_ratio in VolumeAnalysis object for each bar

- [ ] **Task 4: Integrate spread_ratio calculation (Story 2.2)** (AC: 2, 3, 4)
  - [ ] Import or use existing `calculate_spread_ratio` function
  - [ ] For each bar in sequence, calculate spread_ratio
  - [ ] Handle first 20 bars: spread_ratio will be None per Story 2.2
  - [ ] Use vectorized NumPy implementation for performance
  - [ ] Store spread_ratio in VolumeAnalysis object for each bar

- [ ] **Task 5: Integrate close_position calculation (Story 2.3)** (AC: 2, 3)
  - [ ] Import or use existing `calculate_close_position` function
  - [ ] For each bar in sequence, calculate close_position
  - [ ] No window required: can calculate for all bars (including first 20)
  - [ ] Store close_position in VolumeAnalysis object for each bar

- [ ] **Task 6: Integrate effort_result classification (Story 2.4)** (AC: 2, 3, 4)
  - [ ] Import or use existing `classify_effort_result` function
  - [ ] For each bar, classify effort_result based on volume_ratio and spread_ratio
  - [ ] Handle first 20 bars: effort_result will be NORMAL (None ratios per Story 2.4)
  - [ ] Store effort_result in VolumeAnalysis object for each bar

- [ ] **Task 7: Implement batch processing and optimization** (AC: 3, 9)
  - [ ] Optimize for batch processing: calculate all ratios in vectorized manner
  - [ ] Extract volumes, highs, lows, closes into NumPy arrays
  - [ ] Calculate volume ratios for all bars using rolling window (NumPy/pandas)
  - [ ] Calculate spread ratios for all bars using rolling window
  - [ ] Calculate close positions for all bars (vectorized: (close - low) / (high - low))
  - [ ] Loop through results to classify effort_result and build VolumeAnalysis objects
  - [ ] Target performance: 252 bars in <50ms, 10,000 bars in <500ms

- [ ] **Task 8: Handle first 20 bars correctly** (AC: 4)
  - [ ] For bars[0:20], create VolumeAnalysis objects with:
    - bar: OHLCVBar (the bar itself)
    - volume_ratio: None (insufficient data)
    - spread_ratio: None (insufficient data)
    - close_position: calculated value (0.0-1.0, can calculate for all bars)
    - effort_result: EffortResult.NORMAL (None ratios → NORMAL per Story 2.4)
  - [ ] For bars[20:], all fields should be populated with calculated values
  - [ ] Add unit test to verify first 20 bars have correct None/NORMAL values

- [ ] **Task 9: Implement optional caching mechanism** (AC: 5)
  - [ ] Add optional caching to avoid recalculating historical bars
  - [ ] Cache key: (symbol, timeframe, timestamp range)
  - [ ] Use LRU cache or simple dict cache
  - [ ] Cache invalidation: clear cache when new bars added
  - [ ] Document caching behavior in docstring
  - [ ] Make caching opt-in via parameter: `analyze(bars, use_cache=False)`
  - [ ] Add unit test for cache behavior

- [ ] **Task 10: Add logging and observability** (AC: 6)
  - [ ] Import structlog for structured logging
  - [ ] Log start of analysis: symbol, timeframe, bar count
  - [ ] Log debug info for abnormal conditions:
    - volume_ratio > 5.0x (log warning: "extreme_volume")
    - spread_ratio > 3.0x (log warning: "extreme_spread")
    - close outside [low, high] range (log warning: "invalid_close_data")
  - [ ] Log completion: total bars analyzed, processing time
  - [ ] Log statistics:
    - Count of each EffortResult type (CLIMACTIC, ABSORPTION, NO_DEMAND, NORMAL)
    - % distribution
    - Avg volume_ratio, avg spread_ratio
  - [ ] Add correlation ID support for distributed tracing
  - [ ] Follow logging standards from architecture/17-monitoring-and-observability.md

- [ ] **Task 11: Write unit test for end-to-end analysis** (AC: 7)
  - [ ] Create test file: `backend/tests/unit/pattern_engine/test_volume_analyzer.py` (update existing)
  - [ ] Generate synthetic bar sequence (50 bars) with known values
  - [ ] Create VolumeAnalyzer instance
  - [ ] Call analyze(bars)
  - [ ] Assert: returns list of 50 VolumeAnalysis objects
  - [ ] Assert: first 20 bars have volume_ratio=None, spread_ratio=None, effort_result=NORMAL
  - [ ] Assert: bars 20+ have all fields populated (not None)
  - [ ] Assert: volume_ratio, spread_ratio, close_position, effort_result match expected values
  - [ ] Verify VolumeAnalysis.bar references correct OHLCVBar object

- [ ] **Task 12: Write unit test for edge cases** (AC: 4, 7)
  - [ ] Test with exactly 20 bars: all should have None ratios
  - [ ] Test with exactly 21 bars: first 20 None, last bar populated
  - [ ] Test with empty list: should return empty list or raise ValueError
  - [ ] Test with single bar: should return 1 VolumeAnalysis with None ratios
  - [ ] Test with bars having zero volume: verify graceful handling
  - [ ] Test with bars having zero spread: verify close_position=0.5

- [ ] **Task 13: Write integration test with realistic AAPL data** (AC: 8)
  - [ ] Create/update test file: `backend/tests/integration/pattern_engine/test_volume_analysis_integration.py`
  - [ ] Load or generate 252 bars of realistic AAPL data (1 year daily)
  - [ ] Create VolumeAnalyzer instance
  - [ ] Call analyze(bars)
  - [ ] Assert: returns 252 VolumeAnalysis objects
  - [ ] Verify distributions are reasonable:
    - volume_ratio: most bars in 0.5x-2.0x range
    - spread_ratio: most bars in 0.5x-2.0x range
    - close_position: distributed across 0.0-1.0 range
    - effort_result: majority NORMAL (~60-80%), minority special patterns
  - [ ] Log statistics: min, max, mean, median for each field
  - [ ] Identify and log pattern bars: CLIMACTIC, ABSORPTION, NO_DEMAND

- [ ] **Task 14: Write performance test** (AC: 9)
  - [ ] Create/update test file: `backend/tests/integration/pattern_engine/test_volume_performance.py`
  - [ ] Generate 10,000 synthetic OHLCV bars using factory-boy
  - [ ] Create VolumeAnalyzer instance
  - [ ] Measure execution time using `time.perf_counter()` or pytest-benchmark
  - [ ] Call analyze(bars)
  - [ ] Assert: total processing time < 500ms
  - [ ] Log performance metrics:
    - Total time (ms)
    - Bars per second
    - Time per 1000 bars
  - [ ] Profile if performance target not met (identify bottlenecks)

- [ ] **Task 15: Verify JSON serialization compatibility** (AC: 10)
  - [ ] Create test: serialize VolumeAnalysis list to JSON
  - [ ] Use Pydantic's `.json()` method or `json.dumps()` with encoder
  - [ ] Verify all fields serialize correctly:
    - Decimals → strings
    - EffortResult enum → string value
    - datetime → ISO 8601 string
    - OHLCVBar → nested object
  - [ ] Test deserialization: load JSON back to VolumeAnalysis objects
  - [ ] Verify round-trip: original → JSON → deserialized matches original
  - [ ] Add example JSON output to documentation

- [ ] **Task 16: Add comprehensive docstrings and examples** (AC: all)
  - [ ] Add class-level docstring explaining VolumeAnalyzer purpose
  - [ ] Add method-level docstring for `analyze`:
    - Purpose
    - Parameters (bars: List[OHLCVBar])
    - Returns (List[VolumeAnalysis])
    - Raises (ValueError for invalid input)
    - Examples (usage code snippets)
  - [ ] Document performance characteristics (10k bars in <500ms)
  - [ ] Document first 20 bars behavior (None values)
  - [ ] Document thread safety (stateless, thread-safe)
  - [ ] Add usage examples in docstring

- [ ] **Task 17: Create usage documentation** (AC: all)
  - [ ] Add inline code comments explaining vectorization approach
  - [ ] Document integration with pattern detectors (how they will use VolumeAnalyzer)
  - [ ] Add example usage:
    ```python
    # Example usage
    from backend.src.pattern_engine.volume_analyzer import VolumeAnalyzer
    from backend.src.repositories.ohlcv_repository import OHLCVRepository

    # Load bars
    repo = OHLCVRepository()
    bars = repo.get_bars(symbol="AAPL", timeframe="1d", limit=252)

    # Analyze
    analyzer = VolumeAnalyzer()
    analysis_results = analyzer.analyze(bars)

    # Filter for patterns
    absorption_bars = [a for a in analysis_results if a.effort_result == EffortResult.ABSORPTION]
    climactic_bars = [a for a in analysis_results if a.effort_result == EffortResult.CLIMACTIC]
    ```

- [ ] **Task 18: Prepare for pattern detector integration** (AC: all)
  - [ ] Ensure VolumeAnalyzer is importable from pattern_engine module
  - [ ] Verify VolumeAnalysis objects are easy to filter and query
  - [ ] Add helper methods (optional):
    - `get_climactic_bars(results) -> List[VolumeAnalysis]`
    - `get_absorption_bars(results) -> List[VolumeAnalysis]`
  - [ ] Document how pattern detectors will consume VolumeAnalysis results
  - [ ] Prepare for Epics 3-6 integration (trading range, phase, pattern detection)

## Dev Notes

### Previous Story Insights

**Key Learnings from Stories 2.1-2.4:**
- Story 2.1: Implemented `calculate_volume_ratio` with NumPy vectorization
- Story 2.2: Implemented `calculate_spread_ratio` with NumPy vectorization
- Story 2.3: Implemented `calculate_close_position` (simple arithmetic)
- Story 2.4: Implemented `classify_effort_result` (conditional logic)
- VolumeAnalysis model is complete with all fields: volume_ratio, spread_ratio, close_position, effort_result
- Module structure established: `backend/src/pattern_engine/volume_analyzer.py`
- Testing pattern established: Unit → Integration → Performance
- Performance targets: 10k bars in <100ms for individual calculations

**This Story's Integration Responsibilities:**
- **Unify all calculations**: Bring together Stories 2.1-2.4 into single interface
- **Batch processing**: Optimize for analyzing entire bar sequences efficiently
- **Vectorization**: Use NumPy/pandas for maximum performance
- **API-ready**: Produce JSON-serializable results for FastAPI endpoints
- **Pattern detector ready**: Provide clean interface for Epics 3-6

**Design Decisions:**
- Stateless analyzer: no internal state, thread-safe
- Batch-oriented: designed for analyzing sequences, not single bars
- Vectorized: use NumPy arrays for performance
- Caching optional: add if needed, but start simple
- Logging comprehensive: track abnormal conditions and statistics

### Tech Stack & Dependencies

**Languages & Frameworks:**
[Source: [architecture/3-tech-stack.md](../architecture/3-tech-stack.md#31-technology-stack-table)]
- Python 3.11+ (backend language)
- NumPy 1.26+ (vectorized operations)
- pandas 2.2+ (rolling window calculations)
- Pydantic 2.5+ (data validation, JSON serialization)
- pytest 8.0+ (testing framework)
- pytest-benchmark (optional for detailed performance profiling)
- factory-boy (test data generation)

**Module Location:**
[Source: [architecture/10-unified-project-structure.md](../architecture/10-unified-project-structure.md)]
- Implementation: `backend/src/pattern_engine/volume_analyzer.py` (EXISTING module, add VolumeAnalyzer class)
- Models: `backend/src/models/volume_analysis.py` (EXISTING, complete)
- Unit Tests: `backend/tests/unit/pattern_engine/test_volume_analyzer.py` (EXISTING, add integration tests)
- Integration Tests: `backend/tests/integration/pattern_engine/test_volume_analysis_integration.py` (EXISTING, add AAPL test)
- Performance Tests: `backend/tests/integration/pattern_engine/test_volume_performance.py` (EXISTING, add 10k bar test)

### Data Models

**VolumeAnalysis Model (COMPLETE from Stories 2.1-2.4):**
[Source: Stories 2.1-2.4]
```python
from decimal import Decimal
from datetime import datetime, timezone
from typing import Optional
from pydantic import BaseModel, Field
from backend.src.models.ohlcv import OHLCVBar
from backend.src.models.effort_result import EffortResult

class VolumeAnalysis(BaseModel):
    """
    Complete volume analysis for a single OHLCV bar.

    All fields populated by VolumeAnalyzer.analyze() method.
    First 20 bars will have None for volume_ratio, spread_ratio, and NORMAL for effort_result.
    """
    bar: OHLCVBar  # Reference to the analyzed bar
    volume_ratio: Optional[Decimal] = Field(None, decimal_places=4, max_digits=10)  # Story 2.1
    spread_ratio: Optional[Decimal] = Field(None, decimal_places=4, max_digits=10)  # Story 2.2
    close_position: Optional[Decimal] = Field(None, decimal_places=4, max_digits=5, ge=0.0, le=1.0)  # Story 2.3
    effort_result: Optional[EffortResult] = None  # Story 2.4
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

    class Config:
        use_enum_values = True  # Serialize enum as string
        json_encoders = {
            Decimal: str,  # Serialize Decimal as string
            datetime: lambda v: v.isoformat()
        }
```

**VolumeAnalyzer Class (THIS STORY):**
```python
from typing import List
import numpy as np
import pandas as pd

class VolumeAnalyzer:
    """
    Unified volume analyzer that produces complete volume analysis for bar sequences.

    Integrates calculations from Stories 2.1-2.4:
    - Volume ratio (Story 2.1)
    - Spread ratio (Story 2.2)
    - Close position (Story 2.3)
    - Effort vs. result classification (Story 2.4)

    Thread-safe and stateless. Optimized for batch processing.
    """

    def analyze(self, bars: List[OHLCVBar]) -> List[VolumeAnalysis]:
        """
        Analyze a sequence of OHLCV bars and produce complete volume analysis.

        Args:
            bars: List of OHLCV bars to analyze

        Returns:
            List of VolumeAnalysis objects, one per bar

        Performance:
            - 252 bars: <50ms
            - 10,000 bars: <500ms

        Note:
            First 20 bars will have None for volume_ratio and spread_ratio.
        """
        # Implementation in tasks
```

### Calculations & Algorithms

**Batch Processing Strategy:**
[Source: Stories 2.1-2.4 and performance requirements]

**Vectorized Calculation Approach:**
1. Extract all bar data into NumPy arrays:
   - volumes = np.array([bar.volume for bar in bars])
   - highs = np.array([bar.high for bar in bars])
   - lows = np.array([bar.low for bar in bars])
   - closes = np.array([bar.close for bar in bars])

2. Calculate spreads (vectorized):
   - spreads = highs - lows

3. Calculate volume ratios (rolling window):
   - Use pandas rolling window: `pd.Series(volumes).rolling(window=20, min_periods=20).mean()`
   - Compute ratios: `volume_ratios = volumes / avg_volumes`
   - First 20 values will be NaN → convert to None

4. Calculate spread ratios (rolling window):
   - Use pandas rolling window: `pd.Series(spreads).rolling(window=20, min_periods=20).mean()`
   - Compute ratios: `spread_ratios = spreads / avg_spreads`
   - First 20 values will be NaN → convert to None

5. Calculate close positions (vectorized):
   - close_positions = (closes - lows) / spreads
   - Handle zero spread: replace NaN with 0.5

6. Build VolumeAnalysis objects:
   - Loop through bars and zip with calculated values
   - Classify effort_result for each bar
   - Create VolumeAnalysis(bar, volume_ratio, spread_ratio, close_position, effort_result)

**Performance Optimization Techniques:**
[Source: [architecture/3-tech-stack.md](../architecture/3-tech-stack.md) and Stories 2.1-2.2]
- Use NumPy vectorized operations (10-100x faster than Python loops)
- Use pandas rolling windows for moving averages (optimized C implementation)
- Avoid type conversions inside loops
- Pre-allocate arrays when possible
- Minimize object creation (create VolumeAnalysis objects once)

**Expected Performance:**
- 252 bars: <50ms (typical use case: 1 year daily data)
- 10,000 bars: <500ms (large dataset, backtesting)
- Throughput: >20,000 bars/second

### Database Schema

**No New Schema Required:**
[Source: Stories 2.1-2.4]
- VolumeAnalyzer operates on in-memory bar sequences
- Results can be cached or stored separately if needed
- OHLCVBar data loaded from existing `ohlcv_bars` table
- VolumeAnalysis results used by pattern detectors (in-memory)

**Future Storage (Optional):**
- Could create `volume_analysis` table to cache results
- Trade-off: storage cost vs. recalculation time
- Current approach: recalculate on-demand (fast enough with vectorization)

### Coding Standards

**Naming Conventions:**
[Source: [architecture/15-coding-standards.md](../architecture/15-coding-standards.md#152-naming-conventions)]
- Python Classes: PascalCase (e.g., `VolumeAnalyzer`)
- Python Methods: snake_case (e.g., `analyze`)

**Type Safety:**
[Source: [architecture/15-coding-standards.md](../architecture/15-coding-standards.md#151-critical-fullstack-rules)]
- ✅ Use type hints for all methods: `def analyze(self, bars: List[OHLCVBar]) -> List[VolumeAnalysis]:`
- ✅ Use Pydantic models for data structures
- ✅ Validate inputs (bars not empty)

**Class Design:**
- Stateless: no instance variables (except optional cache)
- Thread-safe: can be used concurrently
- Single responsibility: only volume analysis, not pattern detection
- Dependency injection ready: can be mocked for testing

### Error Handling

**Edge Cases to Handle:**
[Source: Epic 2.5 AC and Stories 2.1-2.4]
1. **Empty input**: bars list is empty → return empty list or raise ValueError
2. **Single bar**: insufficient data → return VolumeAnalysis with None ratios
3. **First 20 bars**: volume/spread ratios are None, effort_result is NORMAL
4. **Zero volume bars**: handled by Stories 2.1-2.2 functions
5. **Zero spread bars**: handled by Story 2.3 (close_position=0.5)
6. **Invalid data**: close outside [low, high] → log warning (handled by Story 2.3)

**Logging Strategy:**
[Source: [architecture/17-monitoring-and-observability.md](../architecture/17-monitoring-and-observability.md)]
- Use `structlog` for structured JSON logging
- Log analysis start: symbol, timeframe, bar count
- Log abnormal conditions: extreme volume/spread ratios
- Log statistics: effort_result distribution, avg ratios
- Log completion: processing time, bars analyzed
- Include correlation IDs for distributed tracing

**Error Example:**
```python
def analyze(self, bars: List[OHLCVBar]) -> List[VolumeAnalysis]:
    if not bars:
        logger.warning("empty_bars_list", message="Cannot analyze empty bar list")
        return []

    logger.info("analysis_start", symbol=bars[0].symbol, bar_count=len(bars))

    # ... analysis logic

    # Log abnormal conditions
    extreme_volume_count = sum(1 for vr in volume_ratios if vr and vr > 5.0)
    if extreme_volume_count > 0:
        logger.warning("extreme_volume_detected", count=extreme_volume_count)

    logger.info("analysis_complete", bars_analyzed=len(bars), duration_ms=duration)
    return results
```

### Performance Requirements

**Performance Targets:**
[Source: Epic 2.5 AC]
- 252 bars (1 year daily): <50ms
- 10,000 bars (large dataset): <500ms
- Throughput: >20,000 bars/second

**Performance Testing:**
[Source: AC 9 and [architecture/12-testing-strategy.md](../architecture/12-testing-strategy.md)]
- Use pytest-benchmark or manual timing
- Measure end-to-end processing time
- Profile if targets not met (identify bottlenecks)
- Compare vectorized vs. loop-based approaches

**Optimization Checklist:**
- ✅ Use NumPy vectorized operations
- ✅ Use pandas rolling windows
- ✅ Minimize object creation
- ✅ Avoid Python loops for calculations
- ✅ Pre-allocate arrays
- ✅ Use efficient data structures (NumPy arrays, not lists)

### Integration Notes

**Pattern Detector Integration (Epics 3-6):**
[Source: Epic roadmap and Wyckoff methodology]

**How Pattern Detectors Will Use VolumeAnalyzer:**
```python
# Example: Spring Detector (Epic 5)
from backend.src.pattern_engine.volume_analyzer import VolumeAnalyzer, EffortResult

class SpringDetector:
    def __init__(self):
        self.volume_analyzer = VolumeAnalyzer()

    def detect_springs(self, bars: List[OHLCVBar], trading_range) -> List[Pattern]:
        # Step 1: Analyze all bars
        analysis = self.volume_analyzer.analyze(bars)

        # Step 2: Filter for absorption bars (potential springs)
        absorption_bars = [
            a for a in analysis
            if a.effort_result == EffortResult.ABSORPTION
            and a.close_position >= 0.7  # Bullish close
        ]

        # Step 3: Check if bar is below trading range support
        spring_candidates = [
            a for a in absorption_bars
            if a.bar.low < trading_range.support
        ]

        # Step 4: Validate and create Pattern objects
        return self._validate_springs(spring_candidates)
```

**API Integration (Epic 10 - Dashboard):**
```python
# Example: FastAPI endpoint
from fastapi import APIRouter
from backend.src.pattern_engine.volume_analyzer import VolumeAnalyzer

router = APIRouter()

@router.get("/api/v1/analysis/{symbol}")
async def get_volume_analysis(symbol: str, timeframe: str = "1d", limit: int = 252):
    # Load bars
    bars = await ohlcv_repo.get_bars(symbol, timeframe, limit)

    # Analyze
    analyzer = VolumeAnalyzer()
    analysis = analyzer.analyze(bars)

    # Return JSON (Pydantic auto-serialization)
    return {"symbol": symbol, "analysis": analysis}
```

**Dependencies:**
- Stories 2.1-2.4 must be complete (all functions implemented)
- This story completes Epic 2
- Epics 3-6 (pattern detectors) depend on this story
- Epic 10 (dashboard) will use this for real-time analysis

### Architectural Context

**Backtesting-First Design:**
[Source: [architecture/2-high-level-architecture.md](../architecture/2-high-level-architecture.md)]
- VolumeAnalyzer works identically on live and historical data
- Stateless design: no dependencies on external services
- Deterministic: same inputs → same outputs
- Suitable for backtesting and live trading

**Repository Pattern:**
[Source: [architecture/2-high-level-architecture.md](../architecture/2-high-level-architecture.md)]
- VolumeAnalyzer is a service, not a repository
- Receives data from OHLCVRepository
- Returns analysis objects to pattern detectors
- Separation of concerns: analysis logic separate from data access

**Service Layer Design:**
```
OHLCVRepository (data access)
    ↓ provides bars
VolumeAnalyzer (analysis service) ← THIS STORY
    ↓ provides VolumeAnalysis
PatternDetector (pattern logic)
    ↓ produces Pattern objects
SignalGenerator (signal logic)
```

## Testing

### Test File Locations
[Source: [architecture/10-unified-project-structure.md](../architecture/10-unified-project-structure.md)]
- Unit tests: `backend/tests/unit/pattern_engine/test_volume_analyzer.py` (EXISTING, add VolumeAnalyzer tests)
- Integration tests: `backend/tests/integration/pattern_engine/test_volume_analysis_integration.py` (EXISTING, add AAPL test)
- Performance tests: `backend/tests/integration/pattern_engine/test_volume_performance.py` (EXISTING, add 10k bar test)

### Testing Framework
[Source: [architecture/3-tech-stack.md](../architecture/3-tech-stack.md)]
- pytest 8.0+ for all Python testing
- pytest-benchmark for performance profiling (optional)
- factory-boy for generating test OHLCV bars
- pytest.mark.parametrize for multiple scenarios

### Test Data Generation
[Source: [architecture/3-tech-stack.md](../architecture/3-tech-stack.md) and Stories 2.1-2.4]
- Use factory-boy to create OHLCVBar sequences
- Generate realistic data: normal distribution volumes, varied spreads
- Create edge case data: zero volume, zero spread, extreme values
- Load real AAPL data for integration testing (if available)

### Testing Standards
[Source: [architecture/12-testing-strategy.md](../architecture/12-testing-strategy.md)]
- Unit tests: test VolumeAnalyzer in isolation with synthetic data
- Integration tests: test with realistic data (AAPL), verify distributions
- Performance tests: measure processing time, verify targets met
- Coverage: aim for >80% code coverage

### Test Scenarios

**Unit Test Scenarios (AC 7):**
1. **End-to-end with synthetic data:**
   - Generate 50 bars with known values
   - Call analyzer.analyze(bars)
   - Verify 50 VolumeAnalysis objects returned
   - Verify first 20 have None ratios, bars 20+ populated
   - Verify all fields match expected values

2. **Edge case: exactly 20 bars:**
   - All bars should have None volume/spread ratios
   - effort_result should be NORMAL for all

3. **Edge case: exactly 21 bars:**
   - First 20: None ratios
   - Last bar: populated ratios

4. **Edge case: empty list:**
   - Returns empty list or raises ValueError

5. **Edge case: single bar:**
   - Returns 1 VolumeAnalysis with None ratios

6. **Zero volume/spread bars:**
   - Verify graceful handling per Stories 2.1-2.3

**Integration Test Scenarios (AC 8):**
1. **Realistic AAPL data (252 bars):**
   - Load or generate 1 year of daily AAPL data
   - Call analyzer.analyze(bars)
   - Verify 252 VolumeAnalysis objects returned
   - Verify distributions are reasonable:
     - volume_ratio: most in 0.5x-2.0x range
     - spread_ratio: most in 0.5x-2.0x range
     - close_position: distributed across 0.0-1.0
     - effort_result: ~60-80% NORMAL, rest special patterns
   - Log statistics: min, max, mean, median
   - Identify pattern bars: CLIMACTIC, ABSORPTION, NO_DEMAND

**Performance Test Scenarios (AC 9):**
1. **10,000 bars performance:**
   - Generate 10k synthetic bars
   - Measure execution time
   - Assert: < 500ms
   - Log: bars/second, time per 1000 bars

2. **252 bars performance:**
   - Typical use case (1 year daily)
   - Assert: < 50ms

3. **Performance profiling (optional):**
   - Use pytest-benchmark for detailed profiling
   - Identify bottlenecks if targets not met

**JSON Serialization Test (AC 10):**
1. Analyze bars, get VolumeAnalysis list
2. Serialize to JSON using Pydantic
3. Verify all fields serialize correctly
4. Deserialize back to objects
5. Verify round-trip correctness

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-18 | 1.0 | Initial story creation with comprehensive technical context | Scrum Master (Bob) |

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References
No critical issues encountered during implementation. All tests pass successfully.

### Completion Notes List
- Implemented VolumeAnalyzer class with complete batch processing pipeline
- Integrated all calculations from Stories 2.1-2.4 (volume_ratio, spread_ratio, close_position, effort_result)
- Added comprehensive logging with statistics and effort_result distribution
- Performance exceeds requirements: 10,000 bars in ~363ms (target: <500ms)
- All unit tests pass (14/14 tests)
- All integration tests pass (3/3 tests)
- Performance tests pass - verified 10k bars in <500ms
- JSON serialization verified and working correctly

### File List
**Modified Files:**
- `backend/src/pattern_engine/volume_analyzer.py` - Added VolumeAnalyzer class with analyze() method
- `backend/tests/unit/pattern_engine/test_volume_analyzer.py` - Added 14 comprehensive unit tests for VolumeAnalyzer
- `backend/tests/integration/pattern_engine/test_volume_analysis_integration.py` - Added 3 integration tests with realistic AAPL data
- `backend/tests/integration/pattern_engine/test_volume_performance.py` - Added 4 performance tests

**No New Files Created**

## QA Results
_This section will be populated by the QA agent after story completion_
