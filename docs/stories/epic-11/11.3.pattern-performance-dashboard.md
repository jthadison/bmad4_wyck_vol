# Story 11.3: Pattern Performance Dashboard

## Status
Done

## Story
**As a** trader,
**I want** a dashboard showing performance statistics for each pattern type,
**so that** I know which patterns are working best for me.

## Acceptance Criteria
1. Pattern cards: Spring, SOS, LPS, UTAD each with own card
2. Metrics per pattern: Win rate, Avg R, Profit factor, Trade count, Best/worst trades
3. Time period selector: Last 7/30/90 days, All time
4. Sector breakdown: "Best: Technology (85% WR), Worst: Energy (45% WR)"
5. Chart: win rate trend over time (line chart)
6. Drill-down: click pattern to see individual trade list
7. Export: download performance report as PDF
8. Component: `PatternPerformanceDashboard.tsx`
9. API: GET /api/analytics/pattern-performance?days=30
10. Caching: data refreshed daily, not real-time (performance optimization)
11. **[WYCKOFF]** Test quality tracking: separate metrics for patterns with successful tests vs. without tests
12. **[WYCKOFF]** Volume Spread Analysis (VSA) metrics: No Demand bars, No Supply bars, Stopping Volume events per pattern
13. **[WYCKOFF]** Preliminary events tracking: display PS, SC, AR, ST events detected before Springs
14. **[WYCKOFF]** Relative strength analysis: compare pattern performance to market/sector (leadership identification)

## Tasks / Subtasks

- [ ] **Task 1: Backend - Create Analytics Data Models** (AC: 2, 4)
  - [ ] Subtask 1.1: Create `PatternPerformanceMetrics` Pydantic model in `backend/src/models/analytics.py`
  - [ ] Subtask 1.2: Add fields: pattern_type, win_rate, average_r_multiple, profit_factor, trade_count, best_trade, worst_trade
  - [ ] Subtask 1.3: Create `SectorBreakdown` Pydantic model with sector_name, win_rate, trade_count, average_r_multiple
  - [ ] Subtask 1.4: Create `PatternPerformanceResponse` model containing list of PatternPerformanceMetrics and sector_breakdown list
  - [ ] Subtask 1.5: Create `TrendDataPoint` model with date and win_rate for time-series data
  - [ ] Subtask 1.6: Create `TradeDetail` model for drill-down with signal_id, symbol, entry_price, exit_price, r_multiple_achieved, status
  - [ ] Subtask 1.7: Run `pydantic-to-typescript` to generate TypeScript types in `frontend/src/types/`
  - [ ] Subtask 1.8: Unit test model validation with pytest and sample data

- [ ] **Task 2: Backend - Create Analytics Repository** (AC: 2, 3, 4, 9, 10)
  - [ ] Subtask 2.1: Create `analytics_repository.py` in `backend/src/repositories/`
  - [ ] Subtask 2.2: Implement `get_pattern_performance(days: int | None)` query using SQLAlchemy async
  - [ ] Subtask 2.3: Query signals table with JOIN to patterns table, aggregate by pattern_type
  - [ ] Subtask 2.4: Calculate win_rate: COUNT(CASE WHEN status='TARGET_HIT') / COUNT(*) per pattern
  - [ ] Subtask 2.5: Calculate average_r_multiple: AVG(r_multiple) per pattern from completed trades
  - [ ] Subtask 2.6: Calculate profit_factor: SUM(winning R) / ABS(SUM(losing R)) per pattern
  - [ ] Subtask 2.7: Implement `get_sector_breakdown(pattern_type: str, days: int | None)` with JOIN to external symbol sector mapping
  - [ ] Subtask 2.8: Implement `get_win_rate_trend(pattern_type: str, days: int)` with daily aggregation
  - [ ] Subtask 2.9: Implement `get_trade_details(pattern_type: str, days: int | None)` for drill-down list
  - [ ] Subtask 2.10: Add Redis caching layer with 24-hour TTL using `redis.asyncio` (cache key: `analytics:pattern_performance:{days}`)
  - [ ] Subtask 2.11: Unit test repository methods with pytest-asyncio and factory-boy fixtures
  - [ ] Subtask 2.12: Integration test with real PostgreSQL database and labeled test data

- [ ] **Task 3: Backend - Create Analytics API Endpoint** (AC: 9, 10)
  - [ ] Subtask 3.1: Create `analytics.py` route file in `backend/src/api/routes/`
  - [ ] Subtask 3.2: Implement `GET /api/v1/analytics/pattern-performance` endpoint with query param `days: int | None`
  - [ ] Subtask 3.3: Add validation: days must be 7, 30, 90, or None (all time)
  - [ ] Subtask 3.4: Inject `AnalyticsRepository` dependency
  - [ ] Subtask 3.5: Return `PatternPerformanceResponse` with proper HTTP 200 response
  - [ ] Subtask 3.6: Add Cache-Control header: `max-age=86400` (24 hours)
  - [ ] Subtask 3.7: Implement `GET /api/v1/analytics/pattern-performance/{pattern_type}/trend` endpoint
  - [ ] Subtask 3.8: Implement `GET /api/v1/analytics/pattern-performance/{pattern_type}/trades` endpoint for drill-down
  - [ ] Subtask 3.9: Add OpenAPI documentation with example responses
  - [ ] Subtask 3.10: Unit test endpoints with pytest and mock repository
  - [ ] Subtask 3.11: Integration test with real database and sample signals data

- [ ] **Task 4: Backend - Implement PDF Export Service** (AC: 7)
  - [ ] Subtask 4.1: Create `pdf_export_service.py` in `backend/src/services/`
  - [ ] Subtask 4.2: Add `reportlab` library to Poetry dependencies (for PDF generation)
  - [ ] Subtask 4.3: Implement `generate_performance_report(data: PatternPerformanceResponse, days: int)` method
  - [ ] Subtask 4.4: Create PDF template with header "Pattern Performance Report - Last {days} days"
  - [ ] Subtask 4.5: Add table for each pattern card with metrics (win rate, avg R, profit factor, trade count)
  - [ ] Subtask 4.6: Add sector breakdown section
  - [ ] Subtask 4.7: Add best/worst trades section for each pattern
  - [ ] Subtask 4.8: Return PDF as BytesIO stream
  - [ ] Subtask 4.9: Create `GET /api/v1/analytics/pattern-performance/export` endpoint returning PDF with Content-Type: application/pdf
  - [ ] Subtask 4.10: Add proper Content-Disposition header: `attachment; filename="pattern-performance-{date}.pdf"`
  - [ ] Subtask 4.11: Unit test PDF generation with sample data and verify structure

- [ ] **Task 5: Frontend - Create Pinia Analytics Store** (AC: 3, 6, 10)
  - [ ] Subtask 5.1: Create `analyticsStore.ts` in `frontend/src/stores/`
  - [ ] Subtask 5.2: Define state: patternMetrics, selectedPeriod (7/30/90/all), isLoading, error, trendData, drillDownTrades
  - [ ] Subtask 5.3: Define action `fetchPatternPerformance(days: number | null)` calling API endpoint
  - [ ] Subtask 5.4: Define action `fetchTrendData(patternType: string)` for chart data
  - [ ] Subtask 5.5: Define action `fetchTradeDetails(patternType: string)` for drill-down
  - [ ] Subtask 5.6: Define action `exportPDF()` triggering download of PDF report
  - [ ] Subtask 5.7: Implement client-side caching: store last fetch timestamp, skip API call if < 1 hour old
  - [ ] Subtask 5.8: Use `apiClient` from `frontend/src/services/api.ts` for all API calls
  - [ ] Subtask 5.9: Convert Decimal string fields to Big.js for display formatting
  - [ ] Subtask 5.10: Unit test store actions with Vitest and mock API responses

- [ ] **Task 6: Frontend - Create Pattern Performance Card Component** (AC: 1, 2)
  - [ ] Subtask 6.1: Create `PatternPerformanceCard.vue` in `frontend/src/components/analytics/`
  - [ ] Subtask 6.2: Accept props: patternType, metrics (PatternPerformanceMetrics)
  - [ ] Subtask 6.3: Display pattern name as card header with icon (use PrimeVue Card component)
  - [ ] Subtask 6.4: Display Win Rate as percentage with color: green (>70%), yellow (50-70%), red (<50%)
  - [ ] Subtask 6.5: Display Avg R-Multiple formatted to 2 decimals
  - [ ] Subtask 6.6: Display Profit Factor formatted to 2 decimals
  - [ ] Subtask 6.7: Display Trade Count as integer
  - [ ] Subtask 6.8: Display Best Trade (highest R-multiple achieved) with symbol and value
  - [ ] Subtask 6.9: Display Worst Trade (lowest/negative R-multiple) with symbol and value
  - [ ] Subtask 6.10: Add click handler emitting 'drill-down' event with patternType
  - [ ] Subtask 6.11: Use Tailwind CSS for styling and responsive grid layout
  - [ ] Subtask 6.12: Component test with Vitest + Vue Testing Library

- [ ] **Task 7: Frontend - Create Time Period Selector Component** (AC: 3)
  - [ ] Subtask 7.1: Create `TimePeriodSelector.vue` in `frontend/src/components/analytics/`
  - [ ] Subtask 7.2: Use PrimeVue SelectButton component with options: [7 days, 30 days, 90 days, All time]
  - [ ] Subtask 7.3: Bind v-model to analyticsStore.selectedPeriod
  - [ ] Subtask 7.4: Emit event on selection change to trigger fetchPatternPerformance()
  - [ ] Subtask 7.5: Add loading state during data fetch
  - [ ] Subtask 7.6: Component test with Vitest

- [ ] **Task 8: Frontend - Create Sector Breakdown Component** (AC: 4)
  - [ ] Subtask 8.1: Create `SectorBreakdown.vue` in `frontend/src/components/analytics/`
  - [ ] Subtask 8.2: Accept props: sectorData (array of SectorBreakdown)
  - [ ] Subtask 8.3: Use PrimeVue DataTable to display sectors sorted by win_rate descending
  - [ ] Subtask 8.4: Add columns: Sector Name, Win Rate (%), Trade Count, Avg R
  - [ ] Subtask 8.5: Highlight best sector (highest win rate) in green
  - [ ] Subtask 8.6: Highlight worst sector (lowest win rate) in red
  - [ ] Subtask 8.7: Format win rate as percentage with 1 decimal place
  - [ ] Subtask 8.8: Component test with Vitest and mock sector data

- [ ] **Task 9: Frontend - Create Win Rate Trend Chart Component** (AC: 5)
  - [ ] Subtask 9.1: Create `WinRateTrendChart.vue` in `frontend/src/components/analytics/`
  - [ ] Subtask 9.2: Use Lightweight Charts library (same as existing charts)
  - [ ] Subtask 9.3: Accept props: trendData (array of TrendDataPoint)
  - [ ] Subtask 9.4: Create line chart with X-axis: date, Y-axis: win rate percentage
  - [ ] Subtask 9.5: Add multiple series: one line per pattern type (Spring, SOS, LPS, UTAD)
  - [ ] Subtask 9.6: Add legend showing pattern names with corresponding line colors
  - [ ] Subtask 9.7: Format Y-axis as percentage (0-100%)
  - [ ] Subtask 9.8: Enable zoom/pan interactions
  - [ ] Subtask 9.9: Add tooltip showing exact values on hover
  - [ ] Subtask 9.10: Component test with Vitest and mock chart data

- [ ] **Task 10: Frontend - Create Trade Drill-Down Modal** (AC: 6)
  - [ ] Subtask 10.1: Create `TradeDetailsModal.vue` in `frontend/src/components/analytics/`
  - [ ] Subtask 10.2: Use PrimeVue Dialog component for modal
  - [ ] Subtask 10.3: Accept props: visible (boolean), patternType (string)
  - [ ] Subtask 10.4: Use PrimeVue DataTable to display trade list from analyticsStore.drillDownTrades
  - [ ] Subtask 10.5: Add columns: Symbol, Entry Date, Entry Price, Exit Price, R-Multiple, Status
  - [ ] Subtask 10.6: Add sorting functionality on all columns
  - [ ] Subtask 10.7: Add pagination (50 trades per page)
  - [ ] Subtask 10.8: Color-code R-Multiple: green (positive), red (negative)
  - [ ] Subtask 10.9: Add close button emitting 'close' event
  - [ ] Subtask 10.10: Component test with Vitest

- [ ] **Task 11: Frontend - Create Main Dashboard View** (AC: 1, 3, 8)
  - [ ] Subtask 11.1: Create `PatternPerformanceDashboard.vue` in `frontend/src/views/` (note: Epic says .tsx but project uses Vue)
  - [ ] Subtask 11.2: Import and use analyticsStore
  - [ ] Subtask 11.3: Add TimePeriodSelector component at top
  - [ ] Subtask 11.4: Display 4 PatternPerformanceCard components in responsive grid (2x2 on desktop, 1 column on mobile)
  - [ ] Subtask 11.5: Add SectorBreakdown component below pattern cards
  - [ ] Subtask 11.6: Add WinRateTrendChart component below sector breakdown
  - [ ] Subtask 11.7: Add "Export PDF" button using PrimeVue Button with download icon
  - [ ] Subtask 11.8: Implement click handler for pattern card drill-down opening TradeDetailsModal
  - [ ] Subtask 11.9: Add loading skeleton using PrimeVue Skeleton while data fetches
  - [ ] Subtask 11.10: Add error state display with PrimeVue Message component
  - [ ] Subtask 11.11: Call fetchPatternPerformance() on component mount
  - [ ] Subtask 11.12: Component test with Vitest

- [ ] **Task 12: Frontend - Implement PDF Download Handler** (AC: 7)
  - [ ] Subtask 12.1: Create utility function `downloadPDF()` in `frontend/src/utils/download.ts`
  - [ ] Subtask 12.2: Use apiClient to call `GET /api/v1/analytics/pattern-performance/export?days={days}`
  - [ ] Subtask 12.3: Set responseType: 'blob' for binary PDF data
  - [ ] Subtask 12.4: Create blob URL and trigger download with proper filename
  - [ ] Subtask 12.5: Show success toast notification using PrimeVue Toast
  - [ ] Subtask 12.6: Handle errors with error toast
  - [ ] Subtask 12.7: Unit test download function with mock blob response

- [ ] **Task 13: Wyckoff Enhancement - Test Quality Tracking** (AC: 11)
  - [ ] Subtask 13.1: Add `test_confirmed` field tracking to PatternPerformanceMetrics model
  - [ ] Subtask 13.2: Modify analytics query to separate patterns with successful tests vs. without tests
  - [ ] Subtask 13.3: Calculate separate win rates: "Spring with Test: 85% WR" vs "Spring without Test: 68% WR"
  - [ ] Subtask 13.4: Add test quality badge to pattern cards: "Test Confirmed: 75%" (percentage of patterns with tests)
  - [ ] Subtask 13.5: Update PDF export to include test quality metrics
  - [ ] Subtask 13.6: Unit test test quality calculations (pytest)

- [ ] **Task 14: Wyckoff Enhancement - Volume Spread Analysis (VSA)** (AC: 12)
  - [ ] Subtask 14.1: Create `VSAMetrics` Pydantic model with fields: no_demand_count, no_supply_count, stopping_volume_count
  - [ ] Subtask 14.2: Implement VSA detection logic in backend: No Demand = narrow spread + low volume at resistance
  - [ ] Subtask 14.3: Implement No Supply detection: narrow spread + low volume at support
  - [ ] Subtask 14.4: Implement Stopping Volume detection: wide spread + high volume with no follow-through
  - [ ] Subtask 14.5: Add VSA metrics to pattern performance cards: "VSA Events: 12 No Supply, 5 Stopping Volume"
  - [ ] Subtask 14.6: Create VSA section in PatternPerformanceCard component
  - [ ] Subtask 14.7: Add VSA metrics to PDF export report
  - [ ] Subtask 14.8: Unit test VSA detection algorithms (pytest)

- [ ] **Task 15: Wyckoff Enhancement - Preliminary Events Tracking** (AC: 13)
  - [ ] Subtask 15.1: Create `PreliminaryEvents` Pydantic model with fields: ps_count, sc_count, ar_count, st_count
  - [ ] Subtask 15.2: Query patterns table for preliminary events (PS, SC, AR, ST) before Spring patterns
  - [ ] Subtask 15.3: Add preliminary events timeline to drill-down modal: "PS → SC → AR → ST → Spring"
  - [ ] Subtask 15.4: Display preliminary events count on Spring pattern cards: "Springs with full PS-SC-AR-ST sequence: 12/42"
  - [ ] Subtask 15.5: Create visual timeline component showing event sequence
  - [ ] Subtask 15.6: Add correlation analysis: Springs with complete preliminary sequence vs. incomplete
  - [ ] Subtask 15.7: Unit test preliminary event detection and sequencing (pytest)

- [ ] **Task 16: Wyckoff Enhancement - Relative Strength Analysis** (AC: 14)
  - [ ] Subtask 16.1: Create `RelativeStrengthMetrics` Pydantic model with fields: symbol, rs_score, sector_rs, market_rs
  - [ ] Subtask 16.2: Implement relative strength calculation: (Stock % change) / (Market % change) over pattern timeframe
  - [ ] Subtask 16.3: Query market index data (SPY or equivalent) for RS comparison
  - [ ] Subtask 16.4: Calculate sector-relative strength using sector ETFs
  - [ ] Subtask 16.5: Add RS ranking to sector breakdown: "Technology (RS: 1.42 - Leadership)" vs "Energy (RS: 0.78 - Weakness)"
  - [ ] Subtask 16.6: Create RS visualization: scatter plot of win rate vs. relative strength
  - [ ] Subtask 16.7: Add leadership filter: "Show only patterns with RS > 1.0 (leading market)"
  - [ ] Subtask 16.8: Add RS metrics to PDF export
  - [ ] Subtask 16.9: Unit test RS calculations (pytest)

- [ ] **Task 17: Wyckoff Enhancement - Phase Context Dimension** (AC: NEW - From Team Review 2025-12-09)
  - [ ] Subtask 17.1: Add `detection_phase` field to PatternPerformanceMetrics model (A, B, C, D, E)
  - [ ] Subtask 17.2: Modify analytics query to include phase information from pattern detection
  - [ ] Subtask 17.3: Calculate separate win rates by phase: "Spring in Phase C: 85% WR" vs "Spring in Phase A: 45% WR"
  - [ ] Subtask 17.4: Add phase filter to dashboard: dropdown to filter by detection phase (All, A, B, C, D, E)
  - [ ] Subtask 17.5: Update pattern cards to show phase distribution: "Phase C: 32 trades, Phase A: 8 trades"
  - [ ] Subtask 17.6: Add phase context tooltip explaining why phase matters (e.g., "Phase C Springs are high-probability")
  - [ ] Subtask 17.7: Update drill-down modal to display detection phase for each trade
  - [ ] Subtask 17.8: Add phase context section to PDF export
  - [ ] Subtask 17.9: Update SectorBreakdown to optionally group by phase
  - [ ] Subtask 17.10: Unit test phase-based analytics calculations (pytest)

- [ ] **Task 18: Integration Testing** (AC: All)
  - [ ] Subtask 18.1: E2E test: Load dashboard and verify all pattern cards render (Playwright)
  - [ ] Subtask 18.2: E2E test: Change time period selector and verify data updates (Playwright)
  - [ ] Subtask 18.3: E2E test: Click pattern card and verify drill-down modal opens with trade list (Playwright)
  - [ ] Subtask 18.4: E2E test: Verify sector breakdown displays correctly (Playwright)
  - [ ] Subtask 18.5: E2E test: Verify win rate trend chart renders with all pattern lines (Playwright + screenshot comparison)
  - [ ] Subtask 18.6: E2E test: Click export PDF button and verify download triggers (Playwright)
  - [ ] Subtask 18.7: Backend integration test: Verify caching works (make 2 identical requests, verify 2nd hits cache) (pytest)
  - [ ] Subtask 18.8: E2E test: Verify test quality metrics display correctly on pattern cards (Playwright)
  - [ ] Subtask 18.9: E2E test: Verify VSA metrics appear in pattern cards (Playwright)
  - [ ] Subtask 18.10: E2E test: Verify preliminary events timeline in drill-down modal (Playwright)
  - [ ] Subtask 18.11: E2E test: Verify relative strength analysis and leadership filter (Playwright)
  - [ ] Subtask 18.12: E2E test: Verify phase context filter and phase-based win rates (Playwright)

## Dev Notes

### Previous Story Insights
Story 11.2 (Backtest Preview) established the pattern for async execution, WebSocket progress updates, and comparative analytics. Story 11.3 continues the analytics theme but focuses on historical performance aggregation rather than real-time simulation. Unlike 11.2, this story uses daily caching rather than real-time data, which simplifies the implementation - no WebSocket needed.

### Epic Context
Story 11.3 is part of Epic 11 (Configuration & Analytics) and delivers the analytics visibility component. This story provides traders with historical pattern performance data to inform configuration decisions (from Story 11.1) and validate backtest results (from Story 11.2). The dashboard answers the critical question: "Which patterns are actually making money?"

### Wyckoff Methodology Enhancement - Phase Context Dimension (Team Review 2025-12-09)

**IMPORTANT:** This story has been enhanced based on Wyckoff team review feedback. Task 17 adds phase context tracking to pattern performance analytics.

#### Why Phase Context Matters:

**The Wyckoff Principle:**
- A Spring detected in **Phase C** is a high-probability setup (85%+ win rate expected)
- A Spring detected in **Phase A** is often a trap (45-55% win rate - premature)
- Pattern type alone is insufficient - phase context determines setup quality

**Example Scenario:**
Without phase tracking, a trader sees: "Springs: 68% win rate overall"
With phase tracking, they learn: "Springs in Phase C: 85% WR, Springs in Phase A: 48% WR"

This insight is CRITICAL for learning proper Wyckoff timing and avoiding premature entries.

#### Implementation Details:

1. **Data Model Enhancement:**
   - Add `detection_phase` field (Literal["A", "B", "C", "D", "E"]) to analytics models
   - Source phase data from pattern detection (Story 4.4 PhaseClassification)

2. **Analytics Query Modification:**
   ```sql
   SELECT
       p.pattern_type,
       p.detection_phase,
       COUNT(*) as trade_count,
       AVG(CASE WHEN s.status = 'TARGET_HIT' THEN 1.0 ELSE 0.0 END) as win_rate
   FROM signals s
   JOIN patterns p ON s.pattern_id = p.id
   WHERE s.status IN ('TARGET_HIT', 'STOPPED')
   GROUP BY p.pattern_type, p.detection_phase;
   ```

3. **UI Enhancements:**
   - Phase filter dropdown (All, Phase A, Phase B, Phase C, Phase D, Phase E)
   - Pattern cards show phase distribution: "Phase C: 32 trades (85% WR), Phase A: 8 trades (48% WR)"
   - Drill-down modal includes "Detection Phase" column
   - Tooltip: "Springs in Phase C are high-probability tests. Phase A Springs are premature."

4. **PDF Export:**
   - Add "Performance by Detection Phase" section
   - Table showing win rates broken down by pattern × phase matrix

#### Educational Value:

This enhancement transforms the dashboard from a basic performance tracker into a **Wyckoff teaching tool**:
- Traders learn that timing (phase) is as important as pattern identification
- Reinforces proper Wyckoff phase progression understanding
- Helps traders avoid common mistake of entering on patterns detected in wrong phase

### Data Models

**Analytics Models:**
[Source: docs/architecture/4-data-models.md#4.2]

New Pydantic models will be created in `backend/src/models/analytics.py`:

```python
from pydantic import BaseModel, Field
from decimal import Decimal
from typing import List, Optional
from datetime import date

class PatternPerformanceMetrics(BaseModel):
    """Performance metrics for a single pattern type"""
    pattern_type: str  # "SPRING" | "SOS" | "LPS" | "UTAD"
    win_rate: Decimal  # 0.0 - 1.0
    average_r_multiple: Decimal
    profit_factor: Decimal  # Total wins / abs(total losses)
    trade_count: int
    best_trade: Optional['TradeDetail'] = None  # Highest R-multiple
    worst_trade: Optional['TradeDetail'] = None  # Lowest R-multiple

class SectorBreakdown(BaseModel):
    """Performance breakdown by sector for a pattern"""
    sector_name: str
    win_rate: Decimal  # 0.0 - 1.0
    trade_count: int
    average_r_multiple: Decimal

class TrendDataPoint(BaseModel):
    """Single data point for win rate trend chart"""
    date: date
    win_rate: Decimal  # 0.0 - 1.0

class TradeDetail(BaseModel):
    """Individual trade details for drill-down"""
    signal_id: UUID
    symbol: str
    entry_date: date
    entry_price: Decimal
    exit_price: Optional[Decimal] = None
    r_multiple_achieved: Decimal
    status: str  # "TARGET_HIT" | "STOPPED" | "ACTIVE"

class PatternPerformanceResponse(BaseModel):
    """Complete response for pattern performance dashboard"""
    patterns: List[PatternPerformanceMetrics]
    sector_breakdown: List[SectorBreakdown]  # Aggregated across all patterns
    time_period_days: Optional[int] = None  # None = all time
    generated_at: datetime
    cache_expires_at: datetime
```

**TypeScript Types (Auto-generated):**
[Source: docs/architecture/3-tech-stack.md - pydantic-to-typescript]

After running `pydantic-to-typescript`, types will be available at:
- `frontend/src/types/PatternPerformanceMetrics.ts`
- `frontend/src/types/PatternPerformanceResponse.ts`
- `frontend/src/types/TrendDataPoint.ts`
- `frontend/src/types/TradeDetail.ts`

All Decimal fields serialize as strings for precision (e.g., "0.7200" for 72% win rate).

### API Specifications

**New REST Endpoints:**
[Source: docs/architecture/5-api-specification.md#5.2]

```
GET /api/v1/analytics/pattern-performance?days={7|30|90|null}
Response: 200 OK
Cache-Control: max-age=86400
{
  "patterns": [
    {
      "pattern_type": "SPRING",
      "win_rate": "0.7200",
      "average_r_multiple": "3.45",
      "profit_factor": "2.80",
      "trade_count": 42,
      "best_trade": {...},
      "worst_trade": {...}
    },
    ...
  ],
  "sector_breakdown": [...],
  "time_period_days": 30,
  "generated_at": "2024-03-13T13:00:00Z",
  "cache_expires_at": "2024-03-14T13:00:00Z"
}

Error Responses:
- 400 Bad Request: Invalid days parameter (must be 7, 30, 90, or null)
- 503 Service Unavailable: Database connection error
```

```
GET /api/v1/analytics/pattern-performance/{pattern_type}/trend?days={7|30|90}
Response: 200 OK
{
  "pattern_type": "SPRING",
  "trend_data": [
    {"date": "2024-03-01", "win_rate": "0.7000"},
    {"date": "2024-03-02", "win_rate": "0.7200"},
    ...
  ]
}
```

```
GET /api/v1/analytics/pattern-performance/{pattern_type}/trades?days={7|30|90|null}
Response: 200 OK
{
  "pattern_type": "SPRING",
  "trades": [
    {
      "signal_id": "uuid",
      "symbol": "AAPL",
      "entry_date": "2024-03-01",
      "entry_price": "175.50",
      "exit_price": "182.30",
      "r_multiple_achieved": "3.40",
      "status": "TARGET_HIT"
    },
    ...
  ],
  "pagination": {
    "returned_count": 50,
    "total_count": 127,
    "limit": 50,
    "offset": 0
  }
}
```

```
GET /api/v1/analytics/pattern-performance/export?days={7|30|90|null}
Response: 200 OK
Content-Type: application/pdf
Content-Disposition: attachment; filename="pattern-performance-2024-03-13.pdf"
[PDF binary data]
```

### Component Specifications

**Backend Components:**
[Source: docs/architecture/6-components.md#6.1]

**Analytics Repository** (`backend/src/repositories/analytics_repository.py`):
- New component for performance analytics queries
- Aggregates data from signals and patterns tables
- Implements Redis caching with 24-hour TTL
- Technology: SQLAlchemy async queries, `redis.asyncio`, pandas for complex aggregations

**PDF Export Service** (`backend/src/services/pdf_export_service.py`):
- New service for generating PDF reports
- Technology: reportlab library for PDF generation

**Frontend Components:**
[Source: docs/architecture/6-components.md#6.2]

**PatternPerformanceDashboard.vue**:
- New view location: `frontend/src/views/PatternPerformanceDashboard.vue`
- Responsibilities: orchestrate all analytics subcomponents, handle time period selection, trigger PDF export
- Technology: Vue 3 Composition API, Pinia store integration

**PatternPerformanceCard.vue**:
- New component location: `frontend/src/components/analytics/PatternPerformanceCard.vue`
- Responsibilities: display metrics for single pattern type, emit drill-down event
- Technology: PrimeVue Card, Tailwind CSS for styling

**WinRateTrendChart.vue**:
- New component location: `frontend/src/components/analytics/WinRateTrendChart.vue`
- Responsibilities: multi-line chart showing win rate trends for all patterns
- Technology: Lightweight Charts library (same as existing chart components)

**TradeDetailsModal.vue**:
- New component location: `frontend/src/components/analytics/TradeDetailsModal.vue`
- Responsibilities: display paginated list of individual trades for drill-down
- Technology: PrimeVue Dialog, DataTable

**Pinia Store - useAnalyticsStore**:
- New store location: `frontend/src/stores/analyticsStore.ts`
- State: patternMetrics, selectedPeriod, isLoading, error, trendData, drillDownTrades
- Actions: fetchPatternPerformance(days), fetchTrendData(pattern), fetchTradeDetails(pattern), exportPDF()
- Implements client-side caching: skip API calls if data < 1 hour old

### File Locations

**Backend Files to Create/Modify:**
[Source: docs/architecture/10-unified-project-structure.md]

```
backend/
├── src/
│   ├── api/
│   │   └── routes/
│   │       └── analytics.py              # CREATE: New analytics endpoints
│   ├── models/
│   │   └── analytics.py                  # CREATE: PatternPerformanceMetrics, etc.
│   ├── repositories/
│   │   └── analytics_repository.py       # CREATE: Analytics queries with caching
│   ├── services/
│   │   └── pdf_export_service.py         # CREATE: PDF generation service
│   └── config.py                         # MODIFY: Add Redis cache config
├── tests/
│   ├── unit/
│   │   ├── test_analytics_models.py      # CREATE: Model validation tests
│   │   ├── test_analytics_repository.py  # CREATE: Repository unit tests
│   │   └── test_pdf_export.py            # CREATE: PDF service tests
│   └── integration/
│       └── test_analytics_integration.py # CREATE: Full analytics flow tests
└── pyproject.toml                        # MODIFY: Add reportlab, redis dependencies
```

**Frontend Files to Create/Modify:**
[Source: docs/architecture/10-unified-project-structure.md]

```
frontend/
├── src/
│   ├── components/
│   │   └── analytics/                    # CREATE: New folder for analytics components
│   │       ├── PatternPerformanceCard.vue
│   │       ├── TimePeriodSelector.vue
│   │       ├── SectorBreakdown.vue
│   │       ├── WinRateTrendChart.vue
│   │       └── TradeDetailsModal.vue
│   ├── stores/
│   │   └── analyticsStore.ts             # CREATE: Analytics Pinia store
│   ├── views/
│   │   └── PatternPerformanceDashboard.vue  # CREATE: Main dashboard view
│   ├── types/                            # AUTO-GENERATED: TypeScript types from Pydantic
│   │   ├── PatternPerformanceMetrics.ts
│   │   ├── PatternPerformanceResponse.ts
│   │   ├── TrendDataPoint.ts
│   │   └── TradeDetail.ts
│   ├── utils/
│   │   └── download.ts                   # CREATE: PDF download utility
│   └── services/
│       └── api.ts                        # EXISTING: Use for REST API calls
├── tests/
│   └── components/
│       └── analytics/                    # CREATE: Component tests folder
│           ├── PatternPerformanceCard.spec.ts
│           ├── SectorBreakdown.spec.ts
│           ├── WinRateTrendChart.spec.ts
│           └── TradeDetailsModal.spec.ts
```

**E2E Tests:**
```
tests/
└── e2e/
    └── pattern-performance-dashboard.spec.ts  # CREATE: Playwright E2E tests
```

### Technical Constraints

**Caching Strategy:**
[Source: docs/prd/epic-11-configuration-analytics.md#Story 11.3 AC10]

- Data refreshed daily, NOT real-time
- Backend: Redis cache with 24-hour TTL
- Cache key pattern: `analytics:pattern_performance:{days}`
- Frontend: Additional 1-hour client-side cache to reduce API calls
- Cache invalidation: automatic expiry after 24 hours, manual invalidation not needed for MVP

**Performance Optimization:**
[Source: docs/architecture/14-security-and-performance.md]

- Complex aggregations should use pandas for efficiency
- For large datasets (>10,000 trades), consider PostgreSQL materialized views
- Chart rendering: max 90 data points (daily aggregation) to avoid performance issues
- Pagination: 50 trades per page in drill-down modal

**Decimal Precision:**
[Source: docs/architecture/15-coding-standards.md#15.1]

- Backend: Use Python `Decimal` for all financial calculations (win_rate, r_multiple, etc.)
- Frontend: Convert decimal strings to `Big` from `big.js` library for calculations
- Never use `float` (Python) or `number` (TypeScript) for financial values

**Database Queries:**
[Source: docs/architecture/9-database-schema.md]

Analytics queries will aggregate data from `signals` and `patterns` tables:

```sql
-- Example query for pattern performance
SELECT
    p.pattern_type,
    COUNT(*) as trade_count,
    AVG(CASE WHEN s.status = 'TARGET_HIT' THEN 1.0 ELSE 0.0 END) as win_rate,
    AVG(s.r_multiple) as average_r_multiple,
    SUM(CASE WHEN s.status = 'TARGET_HIT' THEN s.r_multiple ELSE 0 END) /
        ABS(SUM(CASE WHEN s.status = 'STOPPED' THEN s.r_multiple ELSE 0 END)) as profit_factor
FROM signals s
JOIN patterns p ON s.pattern_id = p.id
WHERE s.status IN ('TARGET_HIT', 'STOPPED')
    AND s.generated_at >= NOW() - INTERVAL '30 days'
GROUP BY p.pattern_type;
```

For sector breakdown, join with external sector mapping (implementation detail: sectors can be derived from symbol or stored separately).

### Redis Configuration

**Redis Setup:**
[Source: docs/architecture/3-tech-stack.md]

Add Redis to docker-compose.yml:
```yaml
redis:
  image: redis:7-alpine
  ports:
    - "6379:6379"
  volumes:
    - redis_data:/data
```

Backend configuration in `backend/src/config.py`:
```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    redis_url: str = "redis://localhost:6379"
    analytics_cache_ttl: int = 86400  # 24 hours
```

### PDF Report Structure

**Report Layout:**
[Source: AC 7]

1. Header: "Pattern Performance Report - Last {days} days" (or "All Time")
2. Summary Table: Overall statistics across all patterns
3. Pattern Sections: One section per pattern (Spring, SOS, LPS, UTAD) with:
   - Win Rate, Avg R, Profit Factor, Trade Count
   - Best Trade details (symbol, date, R-multiple)
   - Worst Trade details
4. Sector Breakdown Table: Sorted by win rate
5. Footer: Generated timestamp, disclaimer

Use reportlab's SimpleDocTemplate with Paragraph, Table, and Spacer elements.

### Testing

**Backend Testing:**
[Source: docs/architecture/12-testing-strategy.md]

- **Unit Tests (pytest)**: Test analytics repository queries with mock data, verify metric calculations, test caching logic, test PDF generation
- **Integration Tests (pytest)**: Test full analytics flow with real PostgreSQL and Redis, verify cache hits/misses, test with labeled signal dataset from `backend/tests/fixtures/labeled_patterns.json`

**Frontend Testing:**
[Source: docs/architecture/12-testing-strategy.md]

- **Component Tests (Vitest + Vue Testing Library)**: Test PatternPerformanceCard rendering, TimePeriodSelector interactions, SectorBreakdown table, TradeDetailsModal drill-down
- **Store Tests (Vitest)**: Test analyticsStore actions and state updates, verify client-side caching logic
- **Chart Tests (Vitest)**: Test WinRateTrendChart rendering with mock data

**E2E Testing:**
[Source: docs/architecture/12-testing-strategy.md]

- **Playwright Tests**: Full user flow from dashboard load → time period selection → pattern card drill-down → PDF export
- Test caching: verify subsequent loads are faster (check network tab for cache hits)
- Test responsive layout on mobile/tablet viewports

### Testing Standards

**Test File Locations:**
[Source: docs/architecture/12-testing-strategy.md]

- Backend unit tests: `backend/tests/unit/test_analytics_*.py`
- Backend integration tests: `backend/tests/integration/test_analytics_integration.py`
- Frontend component tests: `frontend/tests/components/analytics/*.spec.ts`
- E2E tests: `tests/e2e/pattern-performance-dashboard.spec.ts` (Playwright)

**Testing Frameworks:**
[Source: docs/architecture/3-tech-stack.md]

- Backend: pytest 8.0+ with pytest-asyncio, pytest-redis, factory-boy for fixtures
- Frontend: Vitest 1.2+ for component tests, Vue Testing Library integration
- E2E: Playwright 1.41+ with screenshot/video recording

**Test Coverage Requirements:**

- Unit tests: 80% coverage minimum for new analytics code
- Integration tests: Cover happy path (various time periods), caching scenarios, error handling (database down, Redis down)
- E2E tests: Cover complete user journey through all dashboard features

**Mock Data Requirements:**

- Use existing `backend/tests/fixtures/labeled_patterns.json` and extend with completed signals
- Create factory for signals with various statuses (TARGET_HIT, STOPPED) and R-multiples
- Mock Redis cache for unit tests
- Use real Redis instance for integration tests

### Error Handling

**Error Scenarios:**
[Source: docs/architecture/16-error-handling-strategy.md]

```json
{
  "error": {
    "code": "ANALYTICS_DATA_UNAVAILABLE",
    "message": "No trade data available for the selected time period",
    "details": {
      "time_period_days": 7,
      "total_signals": 0
    },
    "timestamp": "2024-03-13T13:00:00Z",
    "request_id": "uuid"
  }
}
```

Frontend error handling:
- Show empty state message: "No data available for this time period"
- Gracefully handle cache failures: fall back to database queries
- Show toast notification on PDF download errors
- Use PrimeVue Message component for persistent error display

### Logging and Observability

**Structured Logging:**
[Source: docs/architecture/17-monitoring-and-observability.md]

```python
logger.info(
    "Analytics query executed",
    extra={
        "time_period_days": days,
        "cache_hit": cache_hit,
        "query_duration_ms": duration,
        "correlation_id": correlation_id
    }
)
```

Monitor cache hit rate to ensure caching is effective (target: >90% cache hit rate for analytics endpoints).

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-20 | 1.0 | Initial story draft created with comprehensive technical details from PRD and Architecture docs | Scrum Master (Bob) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent during implementation_

### Debug Log References
_To be populated by dev agent during implementation_

### Completion Notes List
_To be populated by dev agent during implementation_

### File List
_To be populated by dev agent during implementation_

## QA Results

### Review Date: 2025-12-10

### Reviewed By: Quinn (Test Architect)

### Executive Summary

Story 11.3 delivers a **high-quality backend foundation** for the Pattern Performance Dashboard with excellent architecture, comprehensive testing, and proper Wyckoff methodology integration. However, this represents **only 40% completion** - the entire frontend implementation (Tasks 5-12), Wyckoff enhancement logic (Tasks 13-17), and E2E testing (Task 18) remain unimplemented.

**Key Strengths:**
- ✅ Production-ready backend API with comprehensive data models
- ✅ Excellent test coverage (25/25 model tests, 14/14 repository tests passing)
- ✅ Proper Redis caching strategy with 24-hour TTL
- ✅ Professional PDF export with ReportLab
- ✅ Full Wyckoff phase context dimension support in data models
- ✅ Decimal precision for financial calculations

**Critical Gaps:**
- ❌ **Zero frontend implementation** - No components, stores, or views created
- ❌ **Wyckoff logic not implemented** - Models exist but VSA, preliminary events, RS calculations are placeholder stubs
- ❌ **Database queries are MVP placeholders** - Returns empty/zero data, not production-ready
- ❌ **No integration/E2E tests** - Only unit tests exist
- ❌ **API router not registered** - Endpoints cannot be accessed

### Code Quality Assessment

**Backend Implementation: EXCELLENT (A-)**

The implemented backend code demonstrates professional software engineering:

1. **Data Models** ([backend/src/models/analytics.py](backend/src/models/analytics.py)) - 270 lines
   - Comprehensive Pydantic models with proper validation
   - Full Wyckoff enhancement fields (VSA, preliminary events, RS, phase distribution)
   - Decimal precision for win_rate, average_r_multiple, profit_factor
   - Proper JSON serialization configuration
   - **Issue**: Uses deprecated Pydantic `class Config` instead of `ConfigDict` (generates warnings)

2. **Repository Layer** ([backend/src/repositories/analytics_repository.py](backend/src/repositories/analytics_repository.py)) - 405 lines
   - Clean dependency injection pattern
   - Redis caching with graceful degradation
   - Structured logging with correlation IDs
   - **Critical Issue**: All query methods return placeholder/empty data - NOT production-ready
   - Excellent documentation with example SQL queries in docstrings

3. **API Endpoints** ([backend/src/api/routes/analytics.py](backend/src/api/routes/analytics.py)) - 306 lines
   - Proper FastAPI patterns with dependency injection
   - Comprehensive error handling (400/500 status codes)
   - OpenAPI documentation with examples
   - Cache-Control headers for browser caching
   - **Issue**: Router not registered in `main.py` - endpoints are inaccessible

4. **PDF Export** ([backend/src/services/pdf_export_service.py](backend/src/services/pdf_export_service.py)) - 298 lines
   - Professional PDF layout with ReportLab
   - Custom styles and color-coded tables
   - Handles all Wyckoff enhancements (phase distribution, test quality, VSA)
   - Proper error handling and logging

**Frontend Implementation: NOT STARTED (Grade: F - Incomplete)**

Despite developer claims in STATUS.md, NO functional frontend code exists:
- ❌ Store exists but uses raw `fetch()` instead of existing `apiClient` from [frontend/src/services/api.ts](frontend/src/services/api.ts)
- ❌ Dashboard view exists but all components are placeholder implementations
- ❌ No tests for any frontend code
- ❌ No router integration
- ❌ Components exist but are incomplete implementations

### Refactoring Performed

**NONE** - This is initial implementation review, not refactoring pass.

### Compliance Check

- **Coding Standards**: ⚠️ PARTIAL
  - ✅ Python: Proper async/await, type hints, docstrings
  - ✅ Decimal precision for financial values
  - ⚠️ Uses deprecated Pydantic v1 `class Config` pattern (should use `ConfigDict`)
  - ⚠️ Frontend mixing fetch() with existing apiClient inconsistent

- **Project Structure**: ✅ PASS
  - Files organized correctly in `backend/src/{models,repositories,services,api/routes}/`
  - Tests properly located in `backend/tests/unit/{models,repositories}/`
  - **Issue**: No integration tests directory created

- **Testing Strategy**: ⚠️ CONCERNS
  - ✅ Unit tests excellent (39/39 passing)
  - ❌ No integration tests (required per AC)
  - ❌ No E2E tests (Task 18 completely missing)
  - ❌ No frontend component tests

- **All ACs Met**: ❌ FAIL
  - ACs 1-10: **Partially met** - Backend API exists but frontend missing
  - ACs 11-14 (Wyckoff): **Structure only** - Models exist, logic not implemented
  - AC 17 (Task 17): **Phase context** - Data models support it but not connected to Story 4.4

### Requirements Traceability Matrix

| AC | Requirement | Backend Status | Frontend Status | Tests | Overall |
|----|-------------|----------------|-----------------|-------|---------|
| 1 | Pattern cards (Spring, SOS, LPS, UTAD) | ✅ API endpoint | ❌ No component | ❌ None | ❌ INCOMPLETE |
| 2 | Metrics per pattern (Win rate, Avg R, PF, etc.) | ✅ PatternPerformanceMetrics model | ❌ No display | ✅ 25 unit tests | ⚠️ PARTIAL |
| 3 | Time period selector (7/30/90/All) | ✅ API validates periods | ❌ No selector UI | ❌ None | ❌ INCOMPLETE |
| 4 | Sector breakdown | ✅ SectorBreakdown model | ❌ No table | ❌ None | ❌ INCOMPLETE |
| 5 | Win rate trend chart | ✅ Trend endpoint | ❌ No chart | ❌ None | ❌ INCOMPLETE |
| 6 | Drill-down to trade list | ✅ Trade details endpoint | ❌ No modal | ❌ None | ❌ INCOMPLETE |
| 7 | PDF export | ✅ COMPLETE | ✅ Store method | ❌ No tests | ⚠️ PARTIAL |
| 8 | Component: PatternPerformanceDashboard | N/A | ❌ Shell only | ❌ None | ❌ INCOMPLETE |
| 9 | API: GET /analytics/pattern-performance | ✅ COMPLETE | N/A | ❌ No integration | ⚠️ PARTIAL |
| 10 | Caching (daily refresh) | ✅ Redis 24hr TTL | ⚠️ Store has 1hr cache | ❌ No cache tests | ⚠️ PARTIAL |
| 11 | Test quality tracking | ⚠️ Model only | ❌ No UI | ❌ No logic | ❌ INCOMPLETE |
| 12 | VSA metrics (No Demand/Supply/Stopping Vol) | ⚠️ Model only | ❌ No UI | ❌ Returns zeros | ❌ INCOMPLETE |
| 13 | Preliminary events (PS/SC/AR/ST) | ⚠️ Model only | ❌ No UI | ❌ Returns zeros | ❌ INCOMPLETE |
| 14 | Relative strength analysis | ⚠️ Model only | ❌ No UI | ❌ Returns zeros | ❌ INCOMPLETE |

**Coverage Summary:**
- **Fully Implemented**: 0/14 ACs (0%)
- **Partially Implemented**: 5/14 ACs (36%) - Backend structure only
- **Not Implemented**: 9/14 ACs (64%)

**Given-When-Then Test Mapping:**

**AC 2: Pattern Metrics**
- ✅ GIVEN pattern performance data WHEN requesting metrics THEN return win_rate, avg_r, profit_factor validated by Pydantic
  - Test: `test_valid_pattern_metrics` (PASS)
- ✅ GIVEN pattern with test quality WHEN including test_confirmed fields THEN validate separate win rates
  - Test: `test_pattern_metrics_with_test_quality` (PASS)
- ❌ GIVEN actual signal data WHEN querying database THEN aggregate real metrics
  - Test: MISSING - No integration test

**AC 10: Caching**
- ✅ GIVEN cache miss WHEN fetching analytics THEN query database and cache result
  - Test: `test_cache_miss_queries_database` (PASS)
- ✅ GIVEN cache hit WHEN fetching analytics THEN return cached data without DB query
  - Test: `test_cache_hit` (PASS)
- ❌ GIVEN 24 hours elapsed WHEN fetching analytics THEN refresh cache
  - Test: MISSING - No TTL expiry test

### Security Review

**Status**: ✅ PASS (Low Risk Context)

**Positive Findings:**
- ✅ No authentication required (analytics is read-only, low sensitivity)
- ✅ Input validation via Pydantic (days must be 7/30/90/None)
- ✅ SQL injection not possible - uses SQLAlchemy ORM (when implemented)
- ✅ No user-generated content in PDF exports
- ✅ Proper error handling prevents information leakage

**Recommendations:**
- ⚠️ **Future**: Add rate limiting on PDF export endpoint to prevent abuse
- ⚠️ **Future**: Consider adding authentication if deployed publicly
- ℹ️ Redis caching does not store sensitive data (aggregated analytics only)

### Performance Considerations

**Architecture**: ✅ EXCELLENT

The caching strategy is well-designed:
1. **Backend Redis Cache**: 24-hour TTL reduces database load
2. **Frontend Client Cache**: 1-hour TTL reduces API calls
3. **Target**: >90% cache hit rate achievable

**Concerns**:
- ⚠️ **Database Queries Not Implemented** - Performance unknown until actual SQL queries written
- ⚠️ **No Indexes Specified** - Story notes recommend indexes on `signals.status`, `signals.generated_at`, `patterns.pattern_type` but no migration created
- ⚠️ **Large Dataset Handling** - No evidence of pagination limits or query timeouts for  >10k trades
- ℹ️ PDF Generation: Synchronous blocking operation, should be async or queued for production

**Recommendations**:
1. Add database indexes before production deployment
2. Test query performance with realistic dataset (10k+ signals)
3. Consider materialized views for complex aggregations
4. Add query timeout limits

### Reliability Assessment

**Error Handling**: ✅ GOOD

- ✅ Graceful Redis cache failures (logs warning, continues with DB query)
- ✅ HTTP error codes properly set (400 for validation, 500 for server errors)
- ✅ Structured logging with correlation IDs for debugging
- ⚠️ No retry logic for transient failures
- ⚠️ No circuit breaker for Redis connection issues

**Missing**:
- ❌ No health check endpoint for analytics service
- ❌ No monitoring/alerting integration
- ❌ No graceful degradation if database query times out

### Maintainability Assessment

**Code Quality**: ✅ EXCELLENT

- ✅ Comprehensive docstrings with example SQL queries
- ✅ Type hints throughout (AsyncSession, Redis, Decimal)
- ✅ Clear separation of concerns (models/repositories/services/api)
- ✅ Consistent naming conventions
- ✅ Well-structured test files mirroring production code

**Technical Debt Identified**:

1. **HIGH Priority**: Deprecated Pydantic Config Pattern
   - **Location**: [backend/src/models/analytics.py](backend/src/models/analytics.py) lines 94-97, 145-147, 164-166, 189-191, 214-216, 241-244, 262-264
   - **Issue**: Uses `class Config` instead of `model_config = ConfigDict(...)`
   - **Impact**: 42 deprecation warnings in tests, will break in Pydantic v3
   - **Fix**: Replace all `class Config` with `model_config = ConfigDict(json_encoders={...})`

2. **HIGH Priority**: Placeholder Database Queries
   - **Location**: [backend/src/repositories/analytics_repository.py](backend/src/repositories/analytics_repository.py) lines 163-183, 215-217, 299-303
   - **Issue**: Methods return empty/zero data instead of querying database
   - **Impact**: API endpoints return useless data
   - **Fix**: Implement actual SQLAlchemy queries per docstring examples

3. **MEDIUM Priority**: API Router Not Registered
   - **Location**: [backend/src/api/main.py](backend/src/api/main.py)
   - **Issue**: Analytics router not added to FastAPI app
   - **Impact**: Endpoints return 404
   - **Fix**: Add `app.include_router(analytics.router, prefix="/api/v1")`

4. **MEDIUM Priority**: Frontend Inconsistency
   - **Location**: [frontend/src/stores/analyticsStore.ts](frontend/src/stores/analyticsStore.ts) lines 83-93
   - **Issue**: Uses raw `fetch()` instead of existing `apiClient` service
   - **Impact**: Bypasses centralized error handling, auth, interceptors
   - **Fix**: Import and use `apiClient.get()` from `@/services/api.ts`

### Improvements Checklist

- [x] Reviewed backend implementation (models, repository, services, API)
- [x] Verified unit test coverage (39/39 passing)
- [x] Identified technical debt (Pydantic deprecations, placeholder queries)
- [ ] **DEVELOPER ACTION REQUIRED**: Replace deprecated `class Config` with `ConfigDict`
- [ ] **DEVELOPER ACTION REQUIRED**: Implement actual database queries in repository
- [ ] **DEVELOPER ACTION REQUIRED**: Register analytics router in main.py
- [ ] **DEVELOPER ACTION REQUIRED**: Complete frontend implementation (Tasks 5-12)
- [ ] **DEVELOPER ACTION REQUIRED**: Implement Wyckoff enhancement logic (Tasks 13-17)
- [ ] **DEVELOPER ACTION REQUIRED**: Create integration tests with real PostgreSQL
- [ ] **DEVELOPER ACTION REQUIRED**: Create E2E tests with Playwright (Task 18)
- [ ] **DEVELOPER ACTION REQUIRED**: Add database migration with indexes
- [ ] **DEVELOPER ACTION REQUIRED**: Use existing apiClient in frontend store

### Files Modified During Review

**NONE** - This is assessment only, no code modifications performed.

### Gate Status

**Gate: CONCERNS** → [docs/qa/gates/11.3-pattern-performance-dashboard.yml](docs/qa/gates/11.3-pattern-performance-dashboard.yml)

**Risk Profile**: [docs/qa/assessments/11.3-risk-20251210.md](docs/qa/assessments/11.3-risk-20251210.md)

**Rationale**: While the implemented backend code is high-quality and well-tested, this story is only 40% complete. The entire frontend implementation (Tasks 5-12), Wyckoff enhancement logic (Tasks 13-17), and E2E testing (Task 18) are missing. Database queries are placeholders returning empty data. The implementation is not production-ready and requires significant additional work.

### Recommended Status

**❌ Changes Required - Major Work Remaining**

**Story should be split:**
- **Story 11.3a**: Backend Analytics API Foundation - **READY FOR DONE** (with fixes)
- **Story 11.3b**: Frontend Dashboard UI - **NEW STORY** (Tasks 5-12)
- **Story 11.3c**: Wyckoff Analytics Logic - **NEW STORY** (Tasks 13-17 implementation)

**Before marking 11.3a as Done:**
1. Fix Pydantic deprecation warnings (use `ConfigDict`)
2. Implement actual database queries or document as "MVP placeholder"
3. Register analytics router in main.py OR document as deferred
4. Create database migration with required indexes
5. Add integration tests OR create Story 11.3d for test completion

**Estimated Remaining Effort for Full Story 11.3:**
- Frontend (Tasks 5-12): 8-12 hours
- Wyckoff Logic (Tasks 13-17): 6-10 hours
- E2E Tests (Task 18): 3-4 hours
- Database Query Implementation: 4-6 hours
- **Total: 21-32 additional hours**

---

**Quality Gate Decision Details**: See [docs/qa/gates/11.3-pattern-performance-dashboard.yml](docs/qa/gates/11.3-pattern-performance-dashboard.yml) for risk scoring and recommendations.
