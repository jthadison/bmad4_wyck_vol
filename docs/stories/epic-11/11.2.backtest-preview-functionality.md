# Story 11.2: Backtest Preview Functionality

## Status
Done

## Story
**As a** trader,
**I want** to run a backtest with proposed parameters before applying them,
**so that** I can see actual performance impact on historical data.

## Acceptance Criteria
1. Backtest button: triggers 90-day simulation with proposed config
2. Progress indicator: shows "Analyzing 2,268 bars... 45% complete"
3. Comparative results table: Current vs Proposed side-by-side
4. Metrics compared: Total signals, Win rate, Avg R, Profit factor, Max drawdown
5. Visual comparison: equity curves for current vs proposed overlaid
6. Recommendation: "Performance degraded - not recommended" or "Improvement detected"
7. API: POST /api/backtest/preview with config payload
8. Async execution: backtest runs in background, results pushed via WebSocket
9. Component: `BacktestPreview.tsx`
10. Timeout: 5-minute max, show partial results if incomplete

## Tasks / Subtasks

- [ ] **Task 1: Backend - Create Backtest Preview API Endpoint** (AC: 1, 7, 8, 10)
  - [ ] Subtask 1.1: Create `POST /api/v1/backtest/preview` route in `backend/src/api/routes/backtest.py`
  - [ ] Subtask 1.2: Define `BacktestPreviewRequest` Pydantic model in `backend/src/models/backtest.py` with config payload schema
  - [ ] Subtask 1.3: Define `BacktestPreviewResponse` Pydantic model with backtest_run_id, status fields
  - [ ] Subtask 1.4: Implement async background task execution using FastAPI BackgroundTasks for 90-day simulation
  - [ ] Subtask 1.5: Add 5-minute timeout logic with partial results fallback
  - [ ] Subtask 1.6: Integrate with existing `backend/src/backtesting/engine.py` to run simulation with proposed config
  - [ ] Subtask 1.7: Store preview results in backtest_results table with preview flag
  - [ ] Subtask 1.8: Unit test endpoint with mock backtest engine (pytest)
  - [ ] Subtask 1.9: Integration test with real database and 90-day dataset (pytest)

- [ ] **Task 2: Backend - Progress Tracking and WebSocket Updates** (AC: 2, 8)
  - [ ] Subtask 2.1: Create progress tracking mechanism in `backend/src/backtesting/engine.py`
  - [ ] Subtask 2.2: Emit progress updates via WebSocket endpoint at `ws://localhost:8000/ws`
  - [ ] Subtask 2.3: Define `BacktestProgressUpdate` WebSocket message type with sequence_number, bars_analyzed, total_bars, percent_complete
  - [ ] Subtask 2.4: Update WebSocket message handler in `backend/src/api/websocket.py` to support backtest progress messages
  - [ ] Subtask 2.5: Handle connection drops gracefully - store progress in memory/cache for reconnection
  - [ ] Subtask 2.6: Unit test progress tracking with mock bar data (pytest)
  - [ ] Subtask 2.7: Integration test WebSocket progress messages (pytest with websockets library)

- [ ] **Task 3: Backend - Comparative Results Generation** (AC: 3, 4, 5, 6)
  - [ ] Subtask 3.1: Create `BacktestComparison` Pydantic model in `backend/src/models/backtest.py`
  - [ ] Subtask 3.2: Add fields: current_metrics, proposed_metrics, recommendation, equity_curve_current, equity_curve_proposed
  - [ ] Subtask 3.3: Implement comparison logic in `backend/src/backtesting/engine.py` to run both current and proposed configs
  - [ ] Subtask 3.4: Calculate metrics: total_signals, win_rate, average_r_multiple, profit_factor, max_drawdown (use existing `backend/src/backtesting/metrics.py`)
  - [ ] Subtask 3.5: Generate equity curves as time-series arrays (timestamp, equity_value pairs)
  - [ ] Subtask 3.6: Implement recommendation algorithm: compare win_rate, avg_r_multiple, max_drawdown with thresholds
  - [ ] Subtask 3.7: Emit final `BacktestCompleted` WebSocket message with full comparison data
  - [ ] Subtask 3.8: Unit test comparison logic with fixture data (pytest with factory-boy)
  - [ ] Subtask 3.9: Integration test with labeled historical dataset (pytest)

- [ ] **Task 4: Backend - Type Generation and API Documentation** (AC: 7, 9)
  - [ ] Subtask 4.1: Run `pydantic-to-typescript` to generate TypeScript types from new Pydantic models
  - [ ] Subtask 4.2: Verify generated types in `frontend/src/types/` include BacktestPreviewRequest, BacktestPreviewResponse, BacktestComparison, BacktestProgressUpdate
  - [ ] Subtask 4.3: Update FastAPI OpenAPI schema with backtest preview endpoint documentation
  - [ ] Subtask 4.4: Add example request/response payloads to OpenAPI docs

- [ ] **Task 5: Frontend - BacktestPreview Component Structure** (AC: 9)
  - [ ] Subtask 5.1: Create `BacktestPreview.vue` component in `frontend/src/components/configuration/` (note: Epic references .tsx but project uses Vue)
  - [ ] Subtask 5.2: Import generated TypeScript types from `frontend/src/types/`
  - [ ] Subtask 5.3: Create Pinia store `useBacktestStore.ts` in `frontend/src/stores/`
  - [ ] Subtask 5.4: Define store state: backtestStatus, progress, comparison, isRunning, error
  - [ ] Subtask 5.5: Define store actions: startBacktestPreview(config), subscribeToProgress(), cancelBacktest()
  - [ ] Subtask 5.6: Unit test store with mock API responses (Vitest)

- [ ] **Task 6: Frontend - Backtest Button and Progress Indicator** (AC: 1, 2)
  - [ ] Subtask 6.1: Add "Save & Backtest" button to BacktestPreview.vue using PrimeVue Button component
  - [ ] Subtask 6.2: Implement click handler to call store action startBacktestPreview() with proposed config payload
  - [ ] Subtask 6.3: Add PrimeVue ProgressBar component showing percent_complete from store state
  - [ ] Subtask 6.4: Display dynamic text: "Analyzing {bars_analyzed} bars... {percent_complete}% complete"
  - [ ] Subtask 6.5: Add loading state to disable button during execution
  - [ ] Subtask 6.6: Add cancel button to abort long-running backtest
  - [ ] Subtask 6.7: Component test for button and progress display (Vitest + Vue Testing Library)

- [ ] **Task 7: Frontend - WebSocket Integration for Real-Time Updates** (AC: 8)
  - [ ] Subtask 7.1: Use existing `useWebSocket` composable from `frontend/src/composables/useWebSocket.ts`
  - [ ] Subtask 7.2: Subscribe to backtest progress messages in BacktestPreview component onMounted hook
  - [ ] Subtask 7.3: Update store state reactively when progress messages received
  - [ ] Subtask 7.4: Handle WebSocket reconnection using existing reconnection strategy from architecture
  - [ ] Subtask 7.5: Fall back to REST polling if WebSocket unavailable (GET /api/v1/backtest/status/{run_id})
  - [ ] Subtask 7.6: Integration test WebSocket message handling (Vitest with mock WebSocket)

- [ ] **Task 8: Frontend - Comparative Results Table** (AC: 3, 4)
  - [ ] Subtask 8.1: Create results table using PrimeVue DataTable component
  - [ ] Subtask 8.2: Define columns: Metric, Current, Proposed, Change
  - [ ] Subtask 8.3: Populate rows with: Total Signals, Win Rate (%), Avg R-Multiple, Profit Factor, Max Drawdown (%)
  - [ ] Subtask 8.4: Format metrics using locale formatting (toFixed for decimals, % for percentages)
  - [ ] Subtask 8.5: Add color coding: green for improvements, red for degradation, gray for neutral
  - [ ] Subtask 8.6: Display Change column with +/- indicators and percentage differences
  - [ ] Subtask 8.7: Component test for table rendering with mock comparison data (Vitest)

- [ ] **Task 9: Frontend - Equity Curve Visualization** (AC: 5)
  - [ ] Subtask 9.1: Use Lightweight Charts library to create dual-line chart component
  - [ ] Subtask 9.2: Create EquityCurveChart.vue component in `frontend/src/components/charts/`
  - [ ] Subtask 9.3: Plot current equity curve as blue line
  - [ ] Subtask 9.4: Plot proposed equity curve as green/red line (green if better, red if worse)
  - [ ] Subtask 9.5: Add legend showing "Current Config" and "Proposed Config"
  - [ ] Subtask 9.6: Enable zoom/pan interactions using Lightweight Charts built-in features
  - [ ] Subtask 9.7: Format X-axis as dates, Y-axis as currency/percentage
  - [ ] Subtask 9.8: Component test for chart rendering (Vitest with mock chart data)

- [ ] **Task 10: Frontend - Recommendation Display** (AC: 6)
  - [ ] Subtask 10.1: Add recommendation banner below results table
  - [ ] Subtask 10.2: Display recommendation text from backend: "Performance degraded - not recommended" or "Improvement detected"
  - [ ] Subtask 10.3: Style banner: red background for degradation, green for improvement
  - [ ] Subtask 10.4: Add icon: warning icon for degradation, checkmark for improvement (PrimeVue icons)
  - [ ] Subtask 10.5: Component test for recommendation rendering (Vitest)

- [ ] **Task 11: Frontend - Timeout and Error Handling** (AC: 10)
  - [ ] Subtask 11.1: Handle 5-minute timeout from backend
  - [ ] Subtask 11.2: Display partial results message: "Backtest timed out - showing partial results"
  - [ ] Subtask 11.3: Show error toast using PrimeVue Toast for API errors
  - [ ] Subtask 11.4: Implement error boundary for component failures
  - [ ] Subtask 11.5: Add retry button for failed backtests
  - [ ] Subtask 11.6: Component test for error states (Vitest)

- [ ] **Task 12: Integration Testing** (AC: All)
  - [ ] Subtask 12.1: E2E test: Complete backtest preview flow from button click to results display (Playwright)
  - [ ] Subtask 12.2: E2E test: Progress updates appear correctly during execution (Playwright)
  - [ ] Subtask 12.3: E2E test: WebSocket disconnection and reconnection during backtest (Playwright)
  - [ ] Subtask 12.4: E2E test: Timeout scenario shows partial results (Playwright with network throttling)
  - [ ] Subtask 12.5: E2E test: Verify equity curve chart renders correctly (Playwright with screenshot comparison)

## Dev Notes

### Previous Story Insights
No previous Epic 11 stories exist yet. This is the second story in the Configuration & Analytics epic. Story 11.1 (Configuration Wizard UI) would precede this and provide the config payload needed for the backtest preview.

### Epic Context
This story is part of Epic 11 (Configuration & Analytics) which delivers FR29 (configuration wizard with impact analysis) and FR30 UI components. Story 11.2 specifically implements the "Save & Backtest" functionality that runs a historical simulation before applying proposed configuration changes. This allows traders to validate parameter changes before impacting live trading.

### Data Models

**Backtest Preview Models:**
[Source: docs/architecture/4-data-models.md#4.2]

The existing `BacktestResult` Pydantic model will be extended with new models:

```python
# backend/src/models/backtest.py

class BacktestPreviewRequest(BaseModel):
    """Request payload for backtest preview"""
    proposed_config: Dict[str, Any]  # Configuration changes to test
    days: int = Field(default=90, ge=7, le=365)  # Backtest duration
    symbol: Optional[str] = None  # Optional symbol filter
    timeframe: str = Field(default="1d")

class BacktestMetrics(BaseModel):
    """Performance metrics for a backtest run"""
    total_signals: int
    win_rate: Decimal  # 0.0 - 1.0
    average_r_multiple: Decimal
    profit_factor: Decimal  # Total wins / total losses
    max_drawdown: Decimal  # 0.0 - 1.0

class EquityCurvePoint(BaseModel):
    """Single point on equity curve"""
    timestamp: datetime
    equity_value: Decimal

class BacktestComparison(BaseModel):
    """Comparison between current and proposed configs"""
    current_metrics: BacktestMetrics
    proposed_metrics: BacktestMetrics
    recommendation: Literal["improvement", "degraded", "neutral"]
    recommendation_text: str
    equity_curve_current: List[EquityCurvePoint]
    equity_curve_proposed: List[EquityCurvePoint]

class BacktestPreviewResponse(BaseModel):
    """Response for backtest preview initiation"""
    backtest_run_id: UUID
    status: Literal["queued", "running", "completed", "failed", "timeout"]
    estimated_duration_seconds: int

class BacktestProgressUpdate(BaseModel):
    """WebSocket progress update message"""
    type: Literal["backtest_progress"] = "backtest_progress"
    sequence_number: int
    backtest_run_id: UUID
    bars_analyzed: int
    total_bars: int
    percent_complete: int  # 0-100
    timestamp: datetime

class BacktestCompletedMessage(BaseModel):
    """WebSocket completion message"""
    type: Literal["backtest_completed"] = "backtest_completed"
    sequence_number: int
    backtest_run_id: UUID
    comparison: BacktestComparison
    timestamp: datetime
```

**TypeScript Types (Auto-generated):**
[Source: docs/architecture/3-tech-stack.md - pydantic-to-typescript]

After running `pydantic-to-typescript`, types will be available at:
- `frontend/src/types/BacktestPreviewRequest.ts`
- `frontend/src/types/BacktestComparison.ts`
- `frontend/src/types/BacktestProgressUpdate.ts`

All Decimal fields serialize as strings to maintain precision (e.g., "0.7200" for 72% win rate).

### API Specifications

**New REST Endpoint:**
[Source: docs/architecture/5-api-specification.md#5.2]

```
POST /api/v1/backtest/preview
Content-Type: application/json
Request Body: BacktestPreviewRequest

Response: 202 Accepted
{
  "backtest_run_id": "uuid",
  "status": "queued",
  "estimated_duration_seconds": 120
}

Error Responses:
- 400 Bad Request: Invalid config payload
- 422 Unprocessable Entity: days parameter out of range
- 503 Service Unavailable: Backtest engine overloaded
```

```
GET /api/v1/backtest/status/{run_id}
Response: 200 OK
{
  "status": "running",
  "progress": {
    "bars_analyzed": 1245,
    "total_bars": 2268,
    "percent_complete": 54
  }
}
```

**WebSocket Message Types:**
[Source: docs/architecture/5-api-specification.md#5.3]

The existing WebSocket endpoint `ws://localhost:8000/ws` will support two new message types:

1. **backtest_progress** (sent every 5% progress or 10 seconds, whichever is first)
2. **backtest_completed** (sent when backtest finishes or times out)

WebSocket reconnection strategy must be followed: buffer messages during reconnection, fetch missed updates via REST `/api/v1/backtest/status/{run_id}`.

### Component Specifications

**Backend Components:**
[Source: docs/architecture/6-components.md#6.1.6]

**Backtesting Engine** (`backend/src/backtesting/engine.py`):
- Existing component: replays historical data through pattern detection engines
- Extend with preview mode: run two parallel simulations (current config vs proposed config)
- Calculate metrics using existing `backend/src/backtesting/metrics.py`
- Technology: FastAPI BackgroundTasks, pandas for metrics, BarSequence abstraction

**Frontend Components:**
[Source: docs/architecture/6-components.md#6.2]

**BacktestPreview.vue**:
- New component location: `frontend/src/components/configuration/BacktestPreview.vue`
- Responsibilities: trigger backtest, display progress, show comparative results table, render equity curves, show recommendation
- Technology: Vue 3 Composition API, PrimeVue (Button, ProgressBar, DataTable, Toast), Pinia store integration

**EquityCurveChart.vue**:
- New component location: `frontend/src/components/charts/EquityCurveChart.vue`
- Responsibilities: dual-line chart for current vs proposed equity curves
- Technology: Lightweight Charts library (same as existing chart components)

**Pinia Store - useBacktestStore**:
- New store location: `frontend/src/stores/backtestStore.ts`
- State: backtestRunId, status, progress, comparison, isRunning, error
- Actions: startBacktestPreview(config), subscribeToProgress(), cancelBacktest(), fetchStatus(runId)
- Integrates with existing `useWebSocket` composable

### File Locations

**Backend Files to Create/Modify:**
[Source: docs/architecture/10-unified-project-structure.md]

```
backend/
├── src/
│   ├── api/
│   │   └── routes/
│   │       └── backtest.py              # ADD: POST /backtest/preview route
│   ├── models/
│   │   └── backtest.py                  # MODIFY: Add BacktestPreviewRequest, BacktestComparison, etc.
│   ├── backtesting/
│   │   ├── engine.py                    # MODIFY: Add preview_mode with dual simulation
│   │   └── metrics.py                   # EXISTING: Use for metric calculations
│   └── api/
│       └── websocket.py                 # MODIFY: Add backtest_progress message handling
├── tests/
│   ├── unit/
│   │   └── test_backtest_preview.py     # CREATE: Unit tests for backtest preview
│   └── integration/
│       └── test_backtest_integration.py # CREATE: Integration tests with database
```

**Frontend Files to Create/Modify:**
[Source: docs/architecture/10-unified-project-structure.md]

```
frontend/
├── src/
│   ├── components/
│   │   ├── configuration/
│   │   │   └── BacktestPreview.vue      # CREATE: Main backtest preview component
│   │   └── charts/
│   │       └── EquityCurveChart.vue     # CREATE: Equity curve visualization
│   ├── stores/
│   │   └── backtestStore.ts             # CREATE: Pinia store for backtest state
│   ├── composables/
│   │   └── useWebSocket.ts              # EXISTING: Use for WebSocket connection
│   ├── types/                           # AUTO-GENERATED: TypeScript types from Pydantic
│   │   ├── BacktestPreviewRequest.ts
│   │   ├── BacktestComparison.ts
│   │   └── BacktestProgressUpdate.ts
│   └── services/
│       └── api.ts                       # EXISTING: Use for REST API calls
├── tests/
│   └── components/
│       ├── BacktestPreview.spec.ts      # CREATE: Component tests
│       └── EquityCurveChart.spec.ts     # CREATE: Chart component tests
```

**E2E Tests:**
```
tests/
└── e2e/
    └── backtest-preview.spec.ts         # CREATE: Playwright E2E tests
```

### Technical Constraints

**Performance Requirements:**
[Source: docs/prd/epic-11-configuration-analytics.md#Story 11.2 AC10]

- Maximum backtest execution time: 5 minutes (300 seconds)
- Progress updates: emit every 5% progress increment or 10 seconds (whichever is first)
- Partial results: if timeout occurs, show metrics calculated from bars analyzed so far
- WebSocket message rate: throttle to max 10 messages/second to avoid overwhelming frontend

**Async Execution Pattern:**
[Source: docs/architecture/2-high-level-architecture.md#2.5]

Use FastAPI BackgroundTasks for async execution (not Celery/Redis for MVP). The pattern:

```python
from fastapi import BackgroundTasks

@router.post("/backtest/preview")
async def start_backtest_preview(
    request: BacktestPreviewRequest,
    background_tasks: BackgroundTasks
):
    run_id = uuid.uuid4()
    background_tasks.add_task(run_backtest_preview, run_id, request)
    return BacktestPreviewResponse(
        backtest_run_id=run_id,
        status="queued",
        estimated_duration_seconds=120
    )
```

**Decimal Precision:**
[Source: docs/architecture/15-coding-standards.md#15.1]

- Backend: Use Python `Decimal` for all financial calculations (win_rate, avg_r_multiple, etc.)
- Frontend: Convert decimal strings to `Big` from `big.js` library for calculations
- Never use `float` (Python) or `number` (TypeScript) for financial values

**Database Schema:**
[Source: docs/architecture/9-database-schema.md]

Backtest results are stored in the `backtest_results` table. For preview mode, add a `is_preview` boolean flag to distinguish preview runs from full backtests. This allows filtering and cleanup of temporary preview data.

```sql
ALTER TABLE backtest_results ADD COLUMN is_preview BOOLEAN DEFAULT FALSE;
CREATE INDEX idx_backtest_preview ON backtest_results(is_preview, created_at DESC);
```

### Security and Error Handling

**Error Handling:**
[Source: docs/architecture/16-error-handling-strategy.md]

Follow standard error format for API responses:

```json
{
  "error": {
    "code": "BACKTEST_TIMEOUT",
    "message": "Backtest exceeded 5-minute limit - showing partial results",
    "details": {
      "bars_analyzed": 1500,
      "total_bars": 2268,
      "percent_complete": 66
    },
    "timestamp": "2024-03-13T13:05:00Z",
    "request_id": "uuid"
  }
}
```

Frontend error handling via `useApi` composable with toast notifications for user feedback.

**Logging and Observability:**
[Source: docs/architecture/2-high-level-architecture.md#2.5]

Use structured logging (structlog) with correlation IDs:

```python
logger.info(
    "Backtest preview started",
    extra={
        "backtest_run_id": str(run_id),
        "config": proposed_config,
        "days": request.days,
        "correlation_id": correlation_id
    }
)
```

### Testing

**Backend Testing:**
[Source: docs/architecture/12-testing-strategy.md]

- **Unit Tests (pytest)**: Test backtest engine with mock bar data, verify metric calculations, test timeout logic
- **Integration Tests (pytest)**: Test full preview flow with real database and 90-day labeled dataset from `backend/tests/fixtures/labeled_patterns.json`
- **WebSocket Tests (pytest + websockets library)**: Test progress message emission

**Frontend Testing:**
[Source: docs/architecture/12-testing-strategy.md]

- **Component Tests (Vitest + Vue Testing Library)**: Test BacktestPreview.vue button interactions, progress display, results table rendering
- **Store Tests (Vitest)**: Test useBacktestStore actions and state updates
- **Chart Tests (Vitest)**: Test EquityCurveChart.vue rendering with mock data

**E2E Testing:**
[Source: docs/architecture/12-testing-strategy.md]

- **Playwright Tests**: Full user flow from configuration change → click "Save & Backtest" → progress updates → results display → equity curve visualization
- Test WebSocket reconnection during backtest execution
- Test timeout scenario with partial results

### Testing Standards

**Test File Locations:**
[Source: docs/architecture/12-testing-strategy.md]

- Backend unit tests: `backend/tests/unit/test_backtest_preview.py`
- Backend integration tests: `backend/tests/integration/test_backtest_integration.py`
- Frontend component tests: `frontend/tests/components/BacktestPreview.spec.ts`
- E2E tests: `tests/e2e/backtest-preview.spec.ts` (Playwright)

**Testing Frameworks:**
[Source: docs/architecture/3-tech-stack.md]

- Backend: pytest 8.0+ with pytest-mock, factory-boy for fixtures, async test support
- Frontend: Vitest 1.2+ for component tests, Vue Testing Library integration
- E2E: Playwright 1.41+ with WebSocket testing support, screenshot/video recording

**Test Coverage Requirements:**

- Unit tests: 80% coverage minimum for new code
- Integration tests: Cover happy path and error scenarios (timeout, invalid config, WebSocket failures)
- E2E tests: Cover complete user journey from Epic 11.1 config changes through backtest preview

**Mock Data Requirements:**

- Use existing `backend/tests/fixtures/labeled_patterns.json` for historical bar data
- Create factory for BacktestPreviewRequest with valid config variations
- Mock Polygon.io API responses for historical data fetching (not needed if using fixture data)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-20 | 1.0 | Initial story draft created with comprehensive technical details from PRD and Architecture docs | Scrum Master (Bob) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent during implementation_

### Debug Log References
_To be populated by dev agent during implementation_

### Completion Notes List
_To be populated by dev agent during implementation_

### File List
_To be populated by dev agent during implementation_

## QA Results
_To be populated by QA agent during testing_
