# Story 11.9: Pattern Performance Dashboard - Production Implementation

**Epic:** 11 - Frontend Development & User Experience
**Dependencies:** Story 11.3 (Pattern Performance Dashboard - MVP)
**Story Points:** 13
**Priority:** High
**Status:** Not Started

---

## Overview

Implement production-ready features for the Pattern Performance Dashboard, including real database queries, Redis caching infrastructure, Wyckoff enhancement detection algorithms, and comprehensive testing. This story converts the MVP implementation from Story 11.3 into a fully functional, production-grade analytics system.

---

## Business Value

**User Need:**
- Traders need accurate, real-time pattern performance metrics based on actual trade data
- System requires caching infrastructure to handle analytics queries efficiently
- Wyckoff methodology enhancements (VSA, RS, preliminary events) are critical for identifying high-probability setups
- Production system must be thoroughly tested to ensure reliability

**Value Delivered:**
- Real analytics based on actual signal/pattern database
- Sub-100ms response times via Redis caching (90%+ cache hit rate)
- Advanced Wyckoff insights (test quality, VSA events, relative strength)
- Confidence in system reliability through comprehensive testing

---

## Acceptance Criteria

### Must Have (MVP)
- [ ] **Database Queries:** Replace all placeholder data with real SQL aggregations
- [ ] **Redis Infrastructure:** Docker Compose configuration with health checks
- [ ] **Database Indexes:** Performance indexes on signals/patterns tables
- [ ] **Test Quality Tracking:** Separate win rates for test-confirmed vs non-confirmed trades
- [ ] **Integration Tests:** Full request/response cycle with PostgreSQL + Redis
- [ ] **Performance:** <500ms response time for pattern performance endpoint

### Should Have
- [ ] **VSA Metrics Detection:** No Demand, No Supply, Stopping Volume algorithms
- [ ] **Relative Strength Calculation:** RS score vs SPY and sector benchmarks
- [ ] **Preliminary Events Tracking:** PS/SC/AR/ST before Springs
- [ ] **Frontend Component Tests:** Vitest tests for 5 Vue components
- [ ] **E2E Tests:** Playwright tests for critical user flows

### Nice to Have
- [ ] **Materialized Views:** Pre-aggregated analytics for faster queries
- [ ] **Query Optimization:** Query plan analysis and tuning
- [ ] **Cache Monitoring:** Redis metrics dashboard
- [ ] **Error Tracking:** Sentry integration for production errors

---

## Tasks Breakdown

### Task 1: Database Query Implementation (8 hours)

**Description:** Replace placeholder queries in `AnalyticsRepository` with real SQL aggregations.

**Subtasks:**
1. Implement `_query_pattern_metrics()` with SQLAlchemy async queries
   - JOIN signals, patterns tables
   - GROUP BY pattern_type, detection_phase
   - Aggregate: win_rate, avg(r_multiple), profit_factor, COUNT(*)
   - Filter by time period (cutoff_date)
   - Handle NULL exit_date (exclude open trades)

2. Implement sector breakdown query
   - JOIN signals → patterns → sector_mapping
   - GROUP BY sector_name
   - Calculate per-sector metrics
   - Sort by win_rate DESC

3. Implement trend data query for `get_win_rate_trend()`
   - GROUP BY DATE(generated_at), pattern_type
   - Calculate daily win rates
   - ORDER BY date ASC

4. Implement trade details query for `get_trade_details()`
   - SELECT with pagination (LIMIT, OFFSET)
   - Filter by pattern_type, time period
   - Include all TradeDetail fields

5. Add error handling for empty result sets
   - Return empty arrays, not errors
   - Log warnings for suspicious metrics (e.g., 0 trades)

**Files Modified:**
- `backend/src/repositories/analytics_repository.py`

**Testing:**
- Test with realistic data volumes (1000+ signals)
- Verify aggregations match manual calculations
- Test edge cases (no trades, all losses, etc.)

---

### Task 2: Database Indexes for Performance (2 hours)

**Description:** Create database indexes to optimize analytics queries.

**Subtasks:**
1. Create Alembic migration `012_analytics_indexes.py`

2. Add indexes:
   - `CREATE INDEX idx_signals_status_generated_at ON signals(status, generated_at);`
   - `CREATE INDEX idx_patterns_pattern_type_phase ON patterns(pattern_type, detection_phase);`
   - `CREATE INDEX idx_signals_symbol_generated_at ON signals(symbol, generated_at);` (for sector queries)

3. Add composite index for common filter combinations:
   - `CREATE INDEX idx_signals_analytics ON signals(status, generated_at, pattern_id) WHERE status IN ('CLOSED_WIN', 'CLOSED_LOSS');`

4. Run `EXPLAIN ANALYZE` on queries to verify index usage

5. Document expected query performance in migration file

**Files Created:**
- `backend/alembic/versions/012_analytics_indexes.py`

**Testing:**
- Compare query times before/after indexes
- Target: <100ms for pattern performance query

---

### Task 3: Sector Mapping Implementation (3 hours)

**Description:** Create sector mapping table/view for sector breakdown analytics.

**Subtasks:**
1. Create Alembic migration `013_sector_mapping.py`

2. Create `sector_mapping` table:
   ```sql
   CREATE TABLE sector_mapping (
       symbol VARCHAR(10) PRIMARY KEY,
       sector_name VARCHAR(50) NOT NULL,
       industry VARCHAR(100),
       last_updated TIMESTAMP DEFAULT NOW()
   );
   ```

3. Add sector mapping model:
   - `backend/src/models/sector_mapping.py`
   - Fields: symbol, sector_name, industry, last_updated

4. Create seed data script for common symbols:
   - Technology: AAPL, MSFT, NVDA, etc.
   - Financials: JPM, BAC, GS, etc.
   - Healthcare: UNH, JNJ, PFE, etc.
   - 9 other sectors (GICS classification)

5. Add foreign key to signals table (optional, for data integrity)

**Files Created:**
- `backend/alembic/versions/013_sector_mapping.py`
- `backend/src/models/sector_mapping.py`
- `backend/scripts/seed_sector_mapping.py`

**Testing:**
- Verify all test symbols have sector mappings
- Test sector breakdown query with real data

---

### Task 4: Redis Infrastructure Setup (2 hours)

**Description:** Add Redis to Docker Compose and configure backend integration.

**Subtasks:**
1. Update `docker-compose.yml`:
   ```yaml
   services:
     redis:
       image: redis:7-alpine
       container_name: bmad_redis
       ports:
         - "6379:6379"
       volumes:
         - redis_data:/data
       command: redis-server --appendonly yes
       healthcheck:
         test: ["CMD", "redis-cli", "ping"]
         interval: 10s
         timeout: 3s
         retries: 3

   volumes:
     redis_data:
   ```

2. Add Redis URL to backend config:
   - `backend/src/config.py`: Add `REDIS_URL` setting
   - Default: `redis://localhost:6379/0`
   - Environment variable: `REDIS_URL`

3. Update `.env.example` with Redis configuration

4. Add Redis health check to `/api/v1/health` endpoint

5. Document Redis setup in README.md

**Files Modified:**
- `docker-compose.yml`
- `backend/src/config.py`
- `backend/.env.example`
- `backend/src/api/main.py` (health check)
- `README.md`

**Testing:**
- Start Docker Compose stack
- Verify Redis container healthy
- Test cache set/get operations
- Verify graceful fallback when Redis down

---

### Task 5: Test Quality Tracking Implementation (4 hours)

**Description:** Implement separate win rate tracking for test-confirmed vs non-confirmed trades.

**Subtasks:**
1. Update `_query_pattern_metrics()` to include test quality:
   ```sql
   SELECT
     COUNT(*) FILTER (WHERE test_confirmed = true) as test_confirmed_count,
     AVG(r_multiple) FILTER (WHERE test_confirmed = true AND r_multiple >= 1.0) as test_confirmed_win_rate,
     AVG(r_multiple) FILTER (WHERE test_confirmed = false AND r_multiple >= 1.0) as non_test_confirmed_win_rate
   FROM signals
   JOIN patterns ON signals.pattern_id = patterns.id
   GROUP BY pattern_type
   ```

2. Ensure `test_confirmed` field exists in signals/patterns table
   - Add migration if needed

3. Update PatternPerformanceMetrics population logic

4. Add validation: test_confirmed_count <= trade_count

5. Test with mixed data (some confirmed, some not)

**Files Modified:**
- `backend/src/repositories/analytics_repository.py`
- Possibly: `backend/alembic/versions/014_test_confirmed_field.py` (if field missing)

**Testing:**
- Create test data with known test_confirmed values
- Verify win rates calculated correctly
- Test edge case: all confirmed, none confirmed

---

### Task 6: VSA Metrics Detection (6 hours)

**Description:** Implement Volume Spread Analysis detection algorithms.

**Subtasks:**
1. Create VSA detection service:
   - `backend/src/services/vsa_detector.py`

2. Implement No Demand detection:
   - High volume (>1.5x average)
   - Narrow spread (<0.5x average)
   - Down close (close < open)
   - Context: Uptrend resistance

3. Implement No Supply detection:
   - High volume (>1.5x average)
   - Narrow spread (<0.5x average)
   - Up close (close > open)
   - Context: Downtrend support

4. Implement Stopping Volume detection:
   - Climactic volume (>2.5x average)
   - Wide spread initially, narrow on next bar
   - Reversal signal (down bar followed by up bar, or vice versa)
   - Context: Support/resistance level

5. Add `vsa_events` JSONB column to patterns table:
   ```sql
   ALTER TABLE patterns ADD COLUMN vsa_events JSONB;
   -- Example: {"no_demand": 3, "no_supply": 1, "stopping_volume": 2}
   ```

6. Update `get_vsa_metrics()` to query vsa_events column

**Files Created:**
- `backend/src/services/vsa_detector.py`
- `backend/alembic/versions/015_vsa_events_column.py`
- `backend/tests/unit/services/test_vsa_detector.py`

**Testing:**
- Unit tests with known VSA scenarios
- Test with historical data (manual verification)
- Validate event counts match expectations

---

### Task 7: Relative Strength Calculation (5 hours)

**Description:** Calculate RS score vs SPY and sector ETFs.

**Subtasks:**
1. Create RS calculation service:
   - `backend/src/services/relative_strength_calculator.py`

2. Implement RS score formula:
   ```python
   rs_score = (stock_return - benchmark_return) * 100
   # Example: Stock +10%, SPY +5% → RS = 5.0
   ```

3. Fetch benchmark prices (SPY, sector ETFs):
   - Use existing market data adapter
   - Cache benchmark prices (1-day TTL)

4. Calculate RS for each symbol in sector breakdown:
   - Compare to SPY (market benchmark)
   - Compare to sector ETF (e.g., XLK for Technology)

5. Identify sector leaders:
   - Top 20% RS within sector
   - Set `is_sector_leader = true` in SectorBreakdown

6. Add `rs_score` and `is_sector_leader` to sector_mapping table

7. Update `_query_pattern_metrics()` to include RS data

**Files Created:**
- `backend/src/services/relative_strength_calculator.py`
- `backend/alembic/versions/016_rs_fields.py`
- `backend/tests/unit/services/test_relative_strength_calculator.py`

**Testing:**
- Test with known stock/benchmark returns
- Verify sector leader identification
- Test edge case: benchmark outperforms all stocks

---

### Task 8: Preliminary Events Tracking (4 hours)

**Description:** Track PS/SC/AR/ST events before Spring patterns.

**Subtasks:**
1. Query patterns table for preliminary events:
   ```sql
   SELECT pattern_type, COUNT(*)
   FROM patterns
   WHERE symbol = :spring_symbol
     AND generated_at BETWEEN :spring_date - INTERVAL '30 days' AND :spring_date
     AND pattern_type IN ('PS', 'SC', 'AR', 'ST')
   GROUP BY pattern_type
   ```

2. Implement `get_preliminary_events()`:
   - Accept pattern_id (Spring/UTAD)
   - Look back 30 days
   - Count PS, SC, AR, ST events
   - Return PreliminaryEvents model

3. Add preliminary event counts to PatternPerformanceMetrics
   - Only for SPRING and UTAD patterns

4. Update PDF report to show preliminary events

**Files Modified:**
- `backend/src/repositories/analytics_repository.py`
- `backend/src/services/pdf_export_service.py`

**Testing:**
- Create test data with known preliminary events
- Verify counts match expectations
- Test with no preliminary events

---

### Task 9: Backend Integration Tests (6 hours)

**Description:** Write integration tests with real PostgreSQL and Redis.

**Subtasks:**
1. Create integration test fixtures:
   - `backend/tests/integration/conftest.py`
   - Fixtures: `test_db`, `test_redis`, `test_client`

2. Create test data factory:
   - `backend/tests/fixtures/analytics_data_factory.py`
   - Generate realistic signals, patterns, sector mappings
   - Known metrics for validation

3. Write integration tests:
   - `backend/tests/integration/test_analytics_api.py`
   - Test GET /pattern-performance with real DB
   - Test cache hit/miss scenarios
   - Test trend endpoint with date range
   - Test trade details pagination
   - Test PDF export with multi-page content

4. Test error scenarios:
   - Database connection failure
   - Redis connection failure
   - Empty database
   - Invalid query parameters

5. Add CI/CD configuration:
   - GitHub Actions workflow
   - Spin up PostgreSQL + Redis containers
   - Run integration tests

**Files Created:**
- `backend/tests/integration/test_analytics_api.py` (15 tests)
- `backend/tests/fixtures/analytics_data_factory.py`
- `.github/workflows/integration-tests.yml` (if not exists)

**Testing:**
- All 15 integration tests passing
- CI/CD pipeline green

---

### Task 10: Frontend Component Tests (5 hours)

**Description:** Write Vitest tests for Vue components.

**Subtasks:**
1. Create test utilities:
   - `frontend/tests/utils/test-helpers.ts`
   - Mock Pinia stores
   - Mock PrimeVue components

2. Test PatternPerformanceCard.vue (5 tests):
   - Renders metrics correctly
   - Color coding based on win rate
   - Emits drill-down event on click
   - Shows/hides test quality section
   - Shows/hides phase distribution

3. Test SectorBreakdown.vue (4 tests):
   - Renders sector table
   - Sorts by win rate
   - Shows empty state when no data
   - Calculates summary stats correctly

4. Test WinRateTrendChart.vue (4 tests):
   - Initializes Lightweight Charts
   - Updates on pattern change
   - Calculates trend direction
   - Handles loading/error states

5. Test TradeDetailsModal.vue (5 tests):
   - Opens/closes modal
   - Filters trades by phase
   - Paginates correctly
   - Calculates win/loss counts
   - Emits view-trade event

6. Test analyticsStore.ts (6 tests):
   - Fetches pattern performance
   - Caches responses (1-hour TTL)
   - Invalidates cache on filter change
   - Handles API errors
   - Exports PDF
   - Manages loading state

**Files Created:**
- `frontend/tests/components/analytics/PatternPerformanceCard.spec.ts`
- `frontend/tests/components/analytics/SectorBreakdown.spec.ts`
- `frontend/tests/components/analytics/WinRateTrendChart.spec.ts`
- `frontend/tests/components/analytics/TradeDetailsModal.spec.ts`
- `frontend/tests/stores/analyticsStore.spec.ts`
- `frontend/tests/utils/test-helpers.ts`

**Testing:**
- All 24 component tests passing
- Coverage >80% for analytics module

---

### Task 11: E2E Tests with Playwright (4 hours)

**Description:** Write end-to-end tests for critical user flows.

**Subtasks:**
1. Create E2E test fixtures:
   - `frontend/tests/e2e/fixtures/analytics.ts`
   - Seed database with test data

2. Write E2E tests:
   - `frontend/tests/e2e/pattern-performance-dashboard.spec.ts`

3. Test: Load dashboard (5 steps):
   - Navigate to /analytics/pattern-performance
   - Verify 4 pattern cards render
   - Verify sector table loads
   - Verify trend chart renders
   - Verify no console errors

4. Test: Filter by time period (4 steps):
   - Click "30 Days" button
   - Wait for API request
   - Verify data updates
   - Verify cache hit on re-select

5. Test: Filter by phase (4 steps):
   - Select "Phase C" dropdown
   - Wait for API request
   - Verify pattern cards update
   - Verify URL query params

6. Test: Drill-down modal (6 steps):
   - Click Spring pattern card
   - Verify modal opens
   - Verify trade list loads
   - Filter by test confirmed
   - Paginate to page 2
   - Close modal

7. Test: PDF export (3 steps):
   - Click "Export PDF" button
   - Wait for download
   - Verify PDF file exists

8. Test: Error handling (2 steps):
   - Mock API error
   - Verify error message displayed

**Files Created:**
- `frontend/tests/e2e/pattern-performance-dashboard.spec.ts`
- `frontend/tests/e2e/fixtures/analytics.ts`

**Testing:**
- All 7 E2E tests passing
- Tests run in CI/CD pipeline

---

### Task 12: Performance Optimization (3 hours)

**Description:** Optimize queries and implement caching best practices.

**Subtasks:**
1. Run query performance analysis:
   - `EXPLAIN ANALYZE` on all analytics queries
   - Identify slow queries (>100ms)

2. Add query result pagination for large datasets:
   - Limit sector breakdown to top 20 sectors
   - Add pagination to trend data (max 365 days)

3. Implement cache warming strategy:
   - Pre-cache common queries (30 days, all patterns)
   - Background job to refresh cache every 6 hours

4. Add cache monitoring:
   - Log cache hit/miss rates
   - Alert if hit rate <80%

5. Consider materialized views for complex aggregations:
   - Create `mv_pattern_performance_daily` materialized view
   - Refresh nightly via cron job

**Files Modified:**
- `backend/src/repositories/analytics_repository.py`
- `backend/src/services/cache_warmer.py` (new)
- Possibly: `backend/alembic/versions/017_materialized_views.py`

**Testing:**
- Load test with 10,000 signals
- Verify <500ms response time
- Verify >90% cache hit rate after warmup

---

### Task 13: Documentation and Deployment (2 hours)

**Description:** Update documentation and prepare for production deployment.

**Subtasks:**
1. Update API documentation:
   - Add analytics endpoints to OpenAPI spec
   - Add example requests/responses
   - Document cache headers

2. Update user documentation:
   - Pattern Performance Dashboard user guide
   - Screenshots of UI
   - Explanation of metrics (Win Rate, R-Multiple, etc.)

3. Update deployment documentation:
   - Redis setup instructions
   - Database migration steps
   - Environment variables

4. Create runbook for production issues:
   - Cache invalidation procedure
   - Query performance troubleshooting
   - Common errors and solutions

5. Add monitoring/alerting:
   - Sentry for error tracking
   - DataDog for performance metrics
   - Alert on >1s response time

**Files Modified/Created:**
- `docs/api/analytics-endpoints.md`
- `docs/user-guide/pattern-performance-dashboard.md`
- `docs/deployment/redis-setup.md`
- `docs/runbooks/analytics-troubleshooting.md`

---

## Technical Notes

### Database Query Performance

**Expected Query Times** (with indexes):
- Pattern performance (4 patterns): 50-100ms
- Win rate trend (30 days): 30-50ms
- Trade details (paginated 50): 20-30ms
- Sector breakdown (12 sectors): 40-60ms

**Optimization Strategies:**
1. Use composite indexes for common filter combinations
2. Limit result sets (pagination)
3. Consider materialized views for daily aggregations
4. Cache at both application (Redis) and client (1-hour) layers

### Redis Caching Strategy

**Cache Keys:**
- `analytics:pattern_performance:{days}:{phase}` (24-hour TTL)
- `analytics:trend:{pattern}:{days}` (24-hour TTL)
- `analytics:trades:{pattern}:{days}:{limit}:{offset}` (1-hour TTL)
- `analytics:benchmark_prices:{symbol}:{date}` (24-hour TTL)

**Cache Invalidation:**
- Automatic expiration via TTL
- Manual invalidation on data corrections (admin tool)
- Cache warming on deployment

### Wyckoff Detection Algorithms

**VSA Event Criteria:**
- Volume: Calculate 20-bar SMA, compare current bar
- Spread: (high - low) vs 20-bar average spread
- Context: Identify trend direction, support/resistance levels

**RS Calculation:**
- Fetch daily returns for stock and benchmark
- Calculate excess return over period (default: 30 days)
- RS Score = (stock_return - benchmark_return) * 100

**Preliminary Events:**
- Look-back window: 30 days before target pattern
- Valid events: PS, SC, AR, ST (from patterns table)
- Same symbol only

---

## Testing Strategy

### Unit Tests
- **Target:** 100% coverage for new services
- **Focus:** VSA detector, RS calculator, query logic
- **Tools:** pytest, pytest-asyncio

### Integration Tests
- **Target:** 15 tests covering full API flows
- **Focus:** DB queries, caching, pagination
- **Tools:** pytest, Docker (PostgreSQL + Redis)

### Component Tests
- **Target:** 24 tests for Vue components
- **Focus:** Rendering, events, computed properties
- **Tools:** Vitest, Vue Testing Library

### E2E Tests
- **Target:** 7 critical user flows
- **Focus:** Dashboard load, filtering, drill-down, PDF export
- **Tools:** Playwright

### Performance Tests
- **Target:** <500ms response time at 90th percentile
- **Focus:** Load testing with 10,000+ signals
- **Tools:** Locust, JMeter

---

## Definition of Done

- [ ] All 13 tasks completed
- [ ] All placeholder queries replaced with real SQL
- [ ] Redis running in Docker Compose
- [ ] Database indexes created and verified
- [ ] Test quality tracking functional
- [ ] VSA, RS, preliminary events implemented
- [ ] All tests passing (unit, integration, component, E2E)
- [ ] Performance benchmarks met (<500ms, >90% cache hit rate)
- [ ] Code review completed
- [ ] Documentation updated
- [ ] Production deployment successful
- [ ] User acceptance testing passed

---

## Dependencies

**Requires:**
- Story 11.3: Pattern Performance Dashboard (MVP)
- Existing signals and patterns database tables
- Market data adapter (for RS calculations)

**Enables:**
- Production analytics for traders
- Data-driven pattern selection
- Performance tracking over time

---

## Risks and Mitigation

**Risk 1: Query Performance Degradation**
- **Likelihood:** Medium
- **Impact:** High (slow dashboard)
- **Mitigation:** Comprehensive indexing, materialized views, pagination

**Risk 2: Redis Availability**
- **Likelihood:** Low
- **Impact:** Medium (slower responses, no cache)
- **Mitigation:** Graceful fallback to DB queries, monitoring/alerting

**Risk 3: VSA Detection Accuracy**
- **Likelihood:** Medium
- **Impact:** Medium (incorrect event counts)
- **Mitigation:** Manual validation with historical data, tunable thresholds

**Risk 4: Sector Mapping Completeness**
- **Likelihood:** Medium
- **Impact:** Low (missing sector data)
- **Mitigation:** Seed script for common symbols, fallback to "Unknown" sector

---

## Success Metrics

**Technical Metrics:**
- Response time: <500ms (90th percentile)
- Cache hit rate: >90%
- Test coverage: >80%
- Zero critical bugs in first week

**Business Metrics:**
- User adoption: >70% of active traders use dashboard
- Feature engagement: Avg 5+ dashboard views per user per week
- Data accuracy: <1% discrepancy vs manual calculations

---

## Story Points Justification

**Total: 13 points (Large)**

- Database queries (8h) = 3 points
- Redis setup (2h) = 1 point
- Indexes (2h) = 1 point
- Sector mapping (3h) = 1 point
- Test quality (4h) = 1 point
- VSA detection (6h) = 2 points
- RS calculation (5h) = 2 points
- Preliminary events (4h) = 1 point
- Backend integration tests (6h) = 2 points
- Frontend component tests (5h) = 2 points
- E2E tests (4h) = 1 point
- Performance optimization (3h) = 1 point
- Documentation (2h) = 1 point

**Total Estimated Hours:** 54 hours (13 points @ 4 hours/point)

---

## Notes

This story completes the Pattern Performance Dashboard by implementing all production-ready features deferred from Story 11.3. Upon completion, the dashboard will provide accurate, high-performance analytics based on real trade data with advanced Wyckoff methodology enhancements.

The implementation follows best practices for database optimization, caching strategies, and comprehensive testing. All components are designed to scale to production workloads (10,000+ signals).
